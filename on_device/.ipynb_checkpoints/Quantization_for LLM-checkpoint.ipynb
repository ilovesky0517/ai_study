{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3e175-ad79-48a1-829e-b2505f726ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Goals\n",
    "\n",
    "ë³¸ ì‹¤ìŠµì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(Large Language Model, LLM)ì— ëŒ€í•´ ì–‘ìí™”(Quantization)ì„ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ì„ ì••ì¶•í•˜ëŠ” ë°©ë²•ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "LLMì€ íŒŒë¼ë¯¸í„°ì˜ ê°œìˆ˜ê°€ ë§¤ìš° ë§ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ FP16ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ íŒŒë¼ë¯¸í„°ì˜ í¬ê¸°ê°€ ë§ì´ í¬ë©°, LLaMA-2 7Bì™€ ê°™ì´ ì‘ì€ ëª¨ë¸ì— ëŒ€í•´ì„œë„ ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ ìˆ˜í–‰í•˜ê³ ì í•˜ëŠ” ê²½ìš° FP16ì—ì„œë„ ìµœì†Œ 14GB ì´ìƒì˜ ë©”ëª¨ë¦¬ë¥¼ ìš”êµ¬í•˜ë©° ì´ëŠ” ì‹¤ì œë¡œ ëŒë¦¬ê¸°ì— ë¬´ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì–‘ìí™”ë¥¼ í†µí•´ ëª¨ë¸ì˜ weightë¥¼ ë” ë‚®ì€ precisionìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "Contents\n",
    "Weight-only Quantization\n",
    "Weightë§Œ quantizationì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "ì¥ì : ë§¤ìš° ë‚®ì€ bit-widthë¡œ weightë¥¼ ì–‘ìí™”í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¨ì¼ ë°°ì¹˜ ì¶”ë¡  í™˜ê²½ì—ì„œ ìœ ë¦¬í•©ë‹ˆë‹¤.\n",
    "ë‹¨ì : Dequantization í›„ FP16 ì—°ì‚°ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ì˜ˆì‹œ: AWQ (W3A16, W4A16)\n",
    "Weight and Activation Quantization\n",
    "Weightì™€ activation ëª¨ë‘ quantizationì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "ì¥ì : ë‚®ì€ precisionì˜ ì—°ì‚°ì„ í†µí•´ ê°€ì† ê°€ëŠ¥í•˜ë©°, ëŒ€ê·œëª¨ ë°°ì¹˜ ì¶”ë¡  í™˜ê²½ì—ì„œ ìœ ë¦¬í•©ë‹ˆë‹¤.\n",
    "ë‹¨ì : weightì˜ bit-widthë¥¼ ë‚®ì¶”ê¸°ì— í•œê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "ì˜ˆì‹œ: SmoothQuant (W8A8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53db658-d9ca-4855-bb47-24ba505be68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up environment...\")\n",
    "\n",
    "# íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "# !pip install -q torch transformers==4.31.0 accelerate==0.21.0 sentencepiece==0.1.99 tokenizers==0.13.3 datasets==2.15.0 tqdm zstandard huggingface-hub==0.27.0\n",
    "\n",
    "# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "!mkdir -p /content/data\n",
    "!curl -sL https://huggingface.co/datasets/mit-han-lab/pile-val-backup/resolve/main/val.jsonl.zst -o /content/data/val.jsonl.zst\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_HOME\"] = \"/content/hf_cache\"\n",
    "datapath = \"/content/data/val.jsonl.zst\"\n",
    "\n",
    "weights_path = \"opt_125m_weights.pth\"\n",
    "weights_url = \"https://www.dropbox.com/scl/fi/mumd3abnbhyz03v0bopn8/opt_125m_weights.pth?rlkey=lo8gaw3axn8o9i8sw0kivxfwx&st=onxynbq2&dl=1\"\n",
    "\n",
    "if not os.path.exists(weights_path):\n",
    "    print(f\"Downloading weights from {weights_url}...\")\n",
    "    # curl -L -o [ì €ì¥íŒŒì¼ëª…] [URL] ('dl=1'ë¡œ ë³€ê²½í•˜ì—¬ ì§ì ‘ ë‹¤ìš´ë¡œë“œ)\n",
    "    os.system(f\"curl -L -o {weights_path} '{weights_url}'\")\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Weights file already exists.\")\n",
    "\n",
    "print(\"Environment ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595c2f3-d558-4df3-95f1-74cede75c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from matplotlib.colors import Normalize\n",
    "from transformers.models.opt.modeling_opt import OPTDecoderLayer\n",
    "from transformers.models.bloom.modeling_bloom import BloomBlock\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaRMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03834eee-1980-4ff9-9e47-1261df81d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMModel:\n",
    "    \"\"\"\n",
    "    ëŒ€í˜•ì–¸ì–´ëª¨ë¸(LLM)ì˜ í‰ê°€ ë° ê´€ë¦¬ í´ë˜ìŠ¤.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, weights_path=None):\n",
    "        self.model_name = model_name\n",
    "        self.weights_path = weights_path\n",
    "\n",
    "        if (self.weights_path != None):\n",
    "            self._load_model_from_weights()\n",
    "        else:\n",
    "            # ì‚¬ì „í•™ìŠµëœ ì–¸ì–´ëª¨ë¸ ë¡œë“œ\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16,\n",
    "                use_safetensors=True\n",
    "            )\n",
    "            self.model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì • (dropout ë¹„í™œì„±í™”)\n",
    "\n",
    "        # í† í¬ë‚˜ì´ì € ë¡œë“œ (í† í¬ë‚˜ì´ì €ëŠ” ì•„í‚¤í…ì²˜ì™€ ì„¸íŠ¸ì´ë¯€ë¡œ from_pretrained ìœ ì§€)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "        # í‰ê°€ìš© ë°ì´í„°ì…‹ ë¡œë“œ (WikiText-2)\n",
    "        testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "        testenc = self.tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n",
    "        self.testenc = testenc.input_ids.to(self.model.device)\n",
    "\n",
    "        self.model_changed = False\n",
    "\n",
    "    def _load_model_from_weights(self):\n",
    "        print(f\"Loading model architecture: {self.model_name}\")\n",
    "        config = AutoConfig.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "        print(f\"Loading weights from: {self.weights_path}\")\n",
    "        checkpoint = torch.load(self.weights_path, map_location=\"cpu\")\n",
    "\n",
    "        # --- [ìˆ˜ì •] state_dict í‚¤ í™•ì¸ ë° ì¶”ì¶œ ë¡œì§ ì¶”ê°€ ---\n",
    "        if \"state_dict\" in checkpoint:\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "\n",
    "        try:\n",
    "            self.model.load_state_dict(state_dict)\n",
    "        except RuntimeError as e:\n",
    "            print(\"Error loading state_dict directly. Attempting strict=False...\")\n",
    "            print(e)\n",
    "            self.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        self.model.to(dtype=torch.float16)\n",
    "        self.model.cuda()\n",
    "        self.model.eval()\n",
    "\n",
    "    def _evaluate(self):\n",
    "        nsamples = 100\n",
    "        nlls = []\n",
    "\n",
    "        for i in tqdm(range(nsamples), desc=\"evaluating...\"):\n",
    "            batch = self.testenc[:, (i * 2048):((i + 1) * 2048)].to(self.model.device)\n",
    "            with torch.no_grad():\n",
    "                lm_logits = self.model(batch).logits\n",
    "\n",
    "            shift_logits = lm_logits[:, :-1, :].contiguous().float()\n",
    "            shift_labels = self.testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "            neg_log_likelihood = loss.float() * 2048\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n",
    "\n",
    "    def get_model_size(self, data_width=16, group_size=-1):\n",
    "        if group_size != -1:\n",
    "            data_width += (16 + 4) / group_size\n",
    "        num_elements = sum(param.numel() for param in self.model.parameters())\n",
    "        return num_elements * data_width\n",
    "\n",
    "    def model_delete(self):\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def model_evaluate(self, data_width, group_size):\n",
    "        model_perplexity = self._evaluate()\n",
    "        model_size = self.get_model_size(data_width=data_width, group_size=group_size)\n",
    "\n",
    "        print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n",
    "        print(f\"model size: {model_size / 1024 / 1024 / 8:.2f} MiB\")\n",
    "        return model_perplexity\n",
    "\n",
    "    def model_reset(self):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ì´ ë³€ê²½ëœ ê²½ìš° ì´ˆê¸° ê°€ì¤‘ì¹˜ íŒŒì¼ë¡œ ì¬ì„¤ì •\n",
    "        \"\"\"\n",
    "        if self.model_changed:\n",
    "            self.model_delete()\n",
    "            # ì´ˆê¸° ë¡œë“œ ë¡œì§ ì¬ì‚¬ìš©\n",
    "            if (self.weights_path != None):\n",
    "                self._load_model_from_weights()\n",
    "            else:\n",
    "                # ì‚¬ì „í•™ìŠµëœ ì–¸ì–´ëª¨ë¸ ë¡œë“œ\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16,\n",
    "                    use_safetensors=True\n",
    "                )\n",
    "                self.model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì • (dropout ë¹„í™œì„±í™”)\n",
    "            self.model_changed = False\n",
    "\n",
    "    def model_change(self, model: nn.Module):\n",
    "        self.model_delete()\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model_changed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537dfff8-e9b1-4d30-880e-004fef063b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"facebook/opt-125m\"\n",
    "# LLMModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œë„ í•¨ê»˜ ì „ë‹¬ (ìœ„ì—ì„œ ì •ì˜í•œ weights_path)\n",
    "llm_model = LLMModel(model_path, weights_path)\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=32, group_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38ff49-152a-4ba9-9b1c-1ea8fc0c134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.1. Weight-only Quantization (AWQ)\n",
    "AWQ (activation aware weight only quantization)\n",
    "\n",
    "ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆì§€ë§Œ, ì—„ì²­ë‚œ ëª¨ë¸ í¬ê¸°ë¡œ ì¸í•´ í•˜ë“œì›¨ì–´ì  ì¥ë²½(ë©”ëª¨ë¦¬ í¬ê¸°)ì´ ë†’ì•„ì§€ê³ , í† í° ìƒì„± ì†ë„ê°€ ëŠë ¤ì§‘ë‹ˆë‹¤(ë©”ëª¨ë¦¬ ëŒ€ì—­í­). LLMì˜ í¬ê¸°ì™€ ê³„ì‚°ëŸ‰ì€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ê³  ìˆëŠ” ë°˜ë©´, ë©”ëª¨ë¦¬ ëŒ€ì—­í­ì€ ëŠë¦¬ê²Œ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ê²©ì°¨ëŠ” LLM ì„±ëŠ¥ì—ì„œ ì¤‘ìš”í•œ ë³‘ëª© í˜„ìƒì…ë‹ˆë‹¤. ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” **ìƒˆë¡œìš´ ì–‘ìí™” ì•Œê³ ë¦¬ì¦˜(AWQ)**ì„ ì‚¬ìš©í•˜ì—¬ LLMì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ ê°€ì†í™”í•˜ëŠ” ë°©ë²•ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "AWQ\n",
    "Uniform quantization ì€ ì‹¤ìˆ˜ ê°’ì„ range  [ğ›½,ğ›¼]ì—ì„œ  [0,2ğ‘âˆ’1]ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "Notation:\n",
    "Quantized Weight:  ğ‘¤ğ‘\n",
    "Scale factor:  ğ‘ ğ‘\n",
    "Zero Point:  ğ‘§\n",
    " \n",
    "ğ‘ ğ‘=ğ›¼âˆ’ğ›½2ğ‘âˆ’1,(1)\n",
    "ğ‘§=âˆ’Round(ğ›½âˆ—ğ‘ ğ‘ğ‘ğ‘™ğ‘’)(2)\n",
    "ğ‘¤ğ‘=Clamp(Round(ğ‘¤ğ‘ ğ‘)+ğ‘§),(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d39003-9ccd-4fa9-802e-af9b5aadacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•µì‹¬ ì–‘ìí™” í•¨ìˆ˜ (simulated quantization, ì¦‰ ì‹¤ì œ ë¹„íŠ¸ ì¶•ì†Œ ì—†ì´ íš¨ê³¼ë§Œ ëª¨ì‚¬)\n",
    "def pseudo_quantize_tensor(w, n_bit=4, q_group_size=-1):\n",
    "    org_w_shape = w.shape\n",
    "    # ê·¸ë£¹ ë‹¨ìœ„ ì–‘ìí™” ì ìš© ì‹œ weight í–‰ë ¬ì„ ê·¸ë£¹ í¬ê¸°ë¡œ ì¬êµ¬ì„±\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    # ê° í–‰(ë˜ëŠ” ê·¸ë£¹)ë³„ ìµœëŒ€ê°’(Î±)ê³¼ ìµœì†Œê°’(Î²) ê³„ì‚°\n",
    "    max_val = w.amax(dim=1, keepdim=True)\n",
    "    min_val = w.amin(dim=1, keepdim=True)\n",
    "\n",
    "    # ìŠ¤ì¼€ì¼(scale)ê³¼ ì œë¡œí¬ì¸íŠ¸(zero point) ê³„ì‚°\n",
    "    # scale = (Î± - Î²) / (2^n_bit - 1)\n",
    "    # zero = round(-Î² / scale)\n",
    "    max_int = 2 ** n_bit - 1\n",
    "    scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "    zeros = (-torch.round(min_val / scales)).clamp_(0, max_int)\n",
    "\n",
    "    # NaN ê²€ì¦\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    # ì–‘ìí™”(Quantization) ë‹¨ê³„: ì‹¤ìˆ˜ê°’ì„ ì •ìˆ˜ ë²”ìœ„ [0, 2^n_bit - 1]ë¡œ ë§¤í•‘\n",
    "    w = torch.clamp(torch.round(w / scales) + zeros, 0, max_int)\n",
    "\n",
    "    # ë³µì› ë‹¨ê³„(Dequantization): ì •ìˆ˜ê°’ì„ ë‹¤ì‹œ ì‹¤ìˆ˜ ë²”ìœ„ë¡œ ë³€í™˜ (pseudo quantization)\n",
    "    w = (w - zeros) * scales\n",
    "\n",
    "    # NaN ê²€ì¦\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    # ì›ë˜ í…ì„œ í˜•íƒœë¡œ ë³µì›\n",
    "    w = w.reshape(org_w_shape)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight(model, w_bit, q_group_size):\n",
    "    # ëª¨ë¸ ë‚´ Linear ê³„ì¸µì˜ weightë§Œ ì„ íƒì ìœ¼ë¡œ pseudo quantization ì ìš©\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data = pseudo_quantize_tensor(\n",
    "                m.weight.data, n_bit=w_bit, q_group_size=q_group_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd75ffa-f943-4fa7-a798-0ea69f949d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model.model_reset()\n",
    "pseudo_quantize_model_weight(llm_model.model, w_bit=3, q_group_size=128)\n",
    "llm_model.model_changed = True\n",
    "\n",
    "# Evaluate the model\n",
    "llm_model.model_evaluate(data_width=3, group_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900e237-335d-4217-89a9-c10e1b2fe58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(62.8900, device='cuda:0')\n",
    "ëª¨ë¸ í¬ê¸°ê°€ ì¤„ì–´ë“  ê²ƒì€ í™•ì¸í•  ìˆ˜ ìˆì§€ë§Œ, í˜¼ë€ë„(perplexity)ëŠ” ìƒë‹¹íˆ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.\n",
    "ë…¼ë¬¸ì—ì„œì˜ ê´€ì°°ì— ë”°ë¥´ë©´ LLMì˜ í™œì„±í™”(activations)ì—ì„œ ì¼ë¶€ ì±„ë„ì— **ì•„ì›ƒë¼ì´ì–´(outliers)**ê°€ ì†ŒëŸ‰ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹ì • ì±„ë„ì— ì•„ì›ƒë¼ì´ì–´ê°€ ìˆëŠ” ê²½ìš°, ì´ëŠ” ëª¨ë“  í† í°ì—ì„œ ì§€ì†ì ìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n",
    "ì£¼ì–´ì§„ í† í°ì— ëŒ€í•œ ì±„ë„ ê°„ì˜ ë¶„ì‚°(variance)ì€ í¬ì§€ë§Œ(ì¼ë¶€ ì±„ë„ì˜ í™œì„±í™”ëŠ” ë§¤ìš° í¬ê³ , ëŒ€ë¶€ë¶„ì€ ì‘ìŠµë‹ˆë‹¤), íŠ¹ì • ì±„ë„ì˜ í¬ê¸°(magnitude)ê°€ í† í° ê°„ì— ê°€ì§€ëŠ” ë¶„ì‚°ì€ ì‘ìŠµë‹ˆë‹¤(ì•„ì›ƒë¼ì´ì–´ ì±„ë„ì€ ì§€ì†ì ìœ¼ë¡œ í½ë‹ˆë‹¤).\n",
    "AWQ(Activation Aware Weight Quantization)ì˜ ê¸°ë²•ì— ë”°ë¥´ë©´, í™œì„±í™”(activation) ì•„ì›ƒë¼ì´ì–´ì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜ ì±„ë„ì€ ë” ë‘ë“œëŸ¬ì§€ë©°, ì´ëŸ¬í•œ ë‘ë“œëŸ¬ì§„ ê°€ì¤‘ì¹˜ë¥¼ ë³´ì¡´í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ í–¥ìƒìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ, ë‘ë“œëŸ¬ì§„ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ê³  ì›ë˜ ê°’ìœ¼ë¡œ ìœ ì§€í•˜ì—¬ í˜¼ë€ë„(perplexity)ì˜ ë³€í™”ë¥¼ ê´€ì°°í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "ì•„ë˜ ì½”ë“œëŠ” calibration ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ í™œì„±í™” ì•„ì›ƒë¼ì´ì–´ë¥¼ ì–»ê³  ë‘ë“œëŸ¬ì§„ ê°€ì¤‘ì¹˜ë¥¼ ì‹ë³„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f504ed5-05e0-4b57-b9f8-b59571707742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n",
    "    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "    samples = []\n",
    "    n_run = 0\n",
    "\n",
    "    # n_samples ë§Œí¼ ë¬¸ì¥ì„ í† í¬ë‚˜ì´ì¦ˆí•˜ì—¬ ìƒ˜í”Œ ìƒì„±\n",
    "    for data in dataset:\n",
    "        line = data[\"text\"].strip()\n",
    "        line_encoded = tokenizer.encode(line)\n",
    "\n",
    "        # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ì œì™¸\n",
    "        if len(line_encoded) > block_size:\n",
    "            continue\n",
    "\n",
    "        sample = torch.tensor([line_encoded])\n",
    "        if sample.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        samples.append(sample)\n",
    "        n_run += 1\n",
    "\n",
    "        # ì„¤ì •ëœ ê°œìˆ˜ë§Œí¼ ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ ì‹œ ì¢…ë£Œ\n",
    "        if n_run == n_samples:\n",
    "            break\n",
    "\n",
    "    # ëª¨ë“  ìƒ˜í”Œì„ ì—°ê²° í›„ block_size ë‹¨ìœ„ë¡œ ë‚˜ëˆ”\n",
    "    cat_samples = torch.cat(samples, dim=1)\n",
    "    n_split = cat_samples.shape[1] // block_size\n",
    "    print(f\" * Split into {n_split} blocks\")\n",
    "\n",
    "    # block_size ë‹¨ìœ„ë¡œ ë¶„í• ëœ í…ì„œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "    return [cat_samples[:, i * block_size:(i + 1) * block_size] for i in range(n_split)]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_calib_feat(model, tokenizer):\n",
    "    # ê° Linear layerì˜ ì…ë ¥ í™œì„±ê°’(activation) í†µê³„ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•œ í›… ë“±ë¡ìš© dict\n",
    "    input_dict = dict()\n",
    "\n",
    "    # forward hook í•¨ìˆ˜: layer ì…ë ¥ì˜ í‰ê·  ì ˆëŒ“ê°’(activation scale)ì„ ê³„ì‚°\n",
    "    def stat_input_max_hook(m, x, y, name):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        # ì…ë ¥ í…ì„œë¥¼ (batch*seq_len, hidden_dim) í˜•íƒœë¡œ reshape í•œ ë’¤,\n",
    "        # hidden_dim ë°©í–¥ìœ¼ë¡œ í‰ê·  ì ˆëŒ“ê°’ ê³„ì‚° â†’ activation scale ì¶”ì •\n",
    "        x_max = x.view(-1, x.shape[-1]).abs().mean(dim=0).cpu().detach()\n",
    "\n",
    "        # ë ˆì´ì–´ë³„ë¡œ í†µê³„ ëˆ„ì \n",
    "        if name not in input_dict:\n",
    "            input_dict[name] = [x_max]\n",
    "        else:\n",
    "            input_dict[name] += [x_max]\n",
    "\n",
    "    hooks = []\n",
    "    # ëª¨ë“  Linear layerì— ëŒ€í•´ forward hook ë“±ë¡\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            hooks.append(\n",
    "                m.register_forward_hook(\n",
    "                    partial(stat_input_max_hook, name=name)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\"Collecting activation scales...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # calibrationìš© ì…ë ¥ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    samples = get_calib_dataset(tokenizer)\n",
    "    pbar = tqdm(samples)\n",
    "\n",
    "    # ê° ìƒ˜í”Œì„ ëª¨ë¸ì— í†µê³¼ì‹œí‚¤ë©° hookìœ¼ë¡œ ì…ë ¥ í†µê³„ ìˆ˜ì§‘\n",
    "    for input_ids in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        model(input_ids)\n",
    "\n",
    "    # hook í•´ì œ\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # ë ˆì´ì–´ë³„ ì…ë ¥ í†µê³„(activation scale) ë°˜í™˜\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7607dd-ee5f-4e7b-bd0f-aa0d4e4de3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model.model_reset()\n",
    "input_feat = get_calib_feat(llm_model.model, llm_model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2561098-5cd7-4fd7-a9f6-abd5fc328708",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_feat['model.decoder.layers.0.self_attn.q_proj']))\n",
    "print(input_feat['model.decoder.layers.0.self_attn.q_proj'][0].shape)\n",
    "\n",
    "plt.figure(figsize=(20, 8), dpi=150)\n",
    "\n",
    "for i, layer_idx in enumerate(range(12)):\n",
    "  plt.subplot(3, 4, i+1)\n",
    "  plt.plot(input_feat[f'model.decoder.layers.{layer_idx}.self_attn.q_proj'][0])\n",
    "  plt.ylabel(\"Activation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b331a1d-8c7f-4613-985b-5b6373e7a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ì‹¤ìŠµ 1] Scale 1% salient channels\n",
    "1%ì˜ ê°€ì¤‘ì¹˜ë¥¼ FP16ìœ¼ë¡œ ìœ ì§€í•˜ë©´ ëª¨ë¸ í¬ê¸°(ì´ ë¹„íŠ¸ ìˆ˜ë¡œ ì¸¡ì •)ë¥¼ í¬ê²Œ ëŠ˜ë¦¬ì§€ ì•Šê³ ë„ ì–‘ìí™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ, ì´ëŸ¬í•œ í˜¼í•© ì •ë°€ë„ ë°ì´í„° ìœ í˜•ì€ ì‹œìŠ¤í…œ êµ¬í˜„ì„ ì–´ë µê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ì‹¤ì œë¡œ FP16ìœ¼ë¡œ ìœ ì§€í•˜ì§€ ì•Šê³  ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ë³´í˜¸í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì—¬ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì„ ìŠ¤ì¼€ì¼ë§í•˜ê³ , ì–‘ìí™” í•œ ë‹¤ìŒ, ë‹¤ì‹œ ìŠ¤ì¼€ì¼ì„ ì¤„ì¸ í›„ í˜¼ë€ë„(perplexity)ì˜ ë³€í™”ë¥¼ ê´€ì°°í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ce7d9-b4c8-4e2b-9457-bbf5496b2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight_scaleup(\n",
    "    model, w_bit, q_group_size, input_feat, scale_factor\n",
    "):\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            importance = sum(input_feat[n]).float()\n",
    "            # 1í¼ì„¼íŠ¸ ì±„ë„ì˜ ê°œìˆ˜\n",
    "            num_samples = int(len(importance) * 0.01)\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            # Step 1: importanceë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1%ì˜ ì¤‘ìš”í•œ ì±„ë„ì„ ì°¾ìœ¼ì„¸ìš”  (hint: use torch.topk())\n",
    "            # hint : torch.topk() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. torch.topk() í•¨ìˆ˜ëŠ” PyTorchì—ì„œ í…ì„œì˜ ê°’ ì¤‘ ìƒìœ„ kê°œì˜ ê°’ê³¼ ê·¸ë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. torch.topk()[0]ëŠ” ê°’ì„, torch.topk()[1]ì€ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "            outlier_mask = torch.topk(importance, int(len(importance) * 0.01))[1]\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################\n",
    "            assert outlier_mask.dim() == 1\n",
    "\n",
    "            # ìŠ¤ì¼€ì¼ íŒ©í„°ë¥¼ ì ìš©í•˜ëŠ” ê²ƒì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´, ì–‘ìí™” ì „ì— ìŠ¤ì¼€ì¼ íŒ©í„°ë¥¼ ê³±í•˜ê³ , ì–‘ìí™” í›„ì— ìŠ¤ì¼€ì¼ íŒ©í„°ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "            # scale_factorë¥¼ ì´ìš©í•´ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì˜ ê°’ì„ í™•ëŒ€í•©ë‹ˆë‹¤.\n",
    "            m.weight.data[:, outlier_mask] *= scale_factor\n",
    "\n",
    "            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n",
    "\n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "\n",
    "            # Step 2: scale_factorë¥¼ ì´ìš©í•´ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì˜ ê°’ì„ ë‹¤ì‹œ ì¶•ì†Œí•˜ì„¸ìš”.\n",
    "            m.weight.data[:, outlier_mask] /= scale_factor\n",
    "\n",
    "            ############### YOUR CODE ENDS HERE #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99674be2-5769-4274-b402-d82a119d4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scale_factor in [1, 1.5, 2, 2.5, 3]:\n",
    "    print(f\"=========== Scale_factor : {scale_factor} ===========\")\n",
    "    llm_model.model_reset()\n",
    "    pseudo_quantize_model_weight_scaleup(llm_model.model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=scale_factor)\n",
    "    llm_model.model_changed = True\n",
    "\n",
    "    # Evaluate the model\n",
    "    llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1a5c7-2247-42d5-91db-9530597a6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ì‹¤ìŠµ 2] Scale factor search\n",
    "ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ìŠ¤ì¼€ì¼ë§ íŒ©í„° ğ‘ \n",
    " ë¥¼ ì§ì ‘ ì •ì˜í•´ ì£¼ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ Fine-tuningì˜ ë¶ˆì•ˆì •ì„± ë•Œë¬¸ì—, ë¯¸ë¦¬ ì •ì˜ëœ ê²€ìƒ‰ ê³µê°„ ë‚´ì—ì„œ ìµœì ì˜  ğ‘ \n",
    " ë¥¼ ì°¾ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„ íƒì´ ë  ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ë³´í˜¸í•˜ë©´ì„œ ë‹¤ë¥¸ ê°’ì„ ê³ ë ¤í•˜ê¸° ìœ„í•´ ê²€ìƒ‰ ê³µê°„ ë‚´ì—ì„œ ìµœì ì˜ ìŠ¤ì¼€ì¼ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œë¡œ, ë…¼ë¬¸ì—ì„œëŠ” í™œì„±í™”ë§Œ ê³ ë ¤í•˜ëŠ” ê²ƒìœ¼ë¡œë„ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ëŠ” ìŠ¤ì¼€ì¼ë§ íŒ©í„°  ğ‘ \n",
    " ë¥¼ í™œì„±í™”ì˜ L1-norm (ì¦‰, acviation matrixì˜ ì ˆëŒ“ê°’ë“¤ì˜ í‰ê· )ì˜  ğ›¼\n",
    " ì œê³±ìœ¼ë¡œ ì„¤ì •í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ğ›¼\n",
    " ì˜ ê°’ì€ grid searchë¥¼ í†µí•´ ì ì ˆí•œ ê°’ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê²€ìƒ‰ì„ ìœ„í•œ ì½”ë“œë¥¼ ì¶”ê°€í•˜ê³  ì‹¤í–‰í•˜ì—¬ í˜¼ë€ë„(perplexity)ë¥¼ ê´€ì°°í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8919c67b-eb96-4645-a609-5aba44aea166",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def scale_ln_fcs(ln, fcs, scales):\n",
    "    \"\"\"\n",
    "    LayerNorm(LN)ê³¼ ê·¸ ë‹¤ìŒì— ì—°ê²°ëœ Linear layer(fc)ë“¤ì˜ íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ì„ ì¡°ì •\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(fcs, list):\n",
    "        fcs = [fcs]\n",
    "\n",
    "    # scale í…ì„œë¥¼ LNì˜ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "    scales = scales.to(ln.weight.device)\n",
    "    \n",
    "    ln.weight.div_(scales)\n",
    "    if hasattr(ln, 'bias') and ln.bias is not None:\n",
    "        ln.bias.div_(scales)\n",
    "        \n",
    "    # í›„ì† Linear layerì˜ ì…ë ¥ ìŠ¤ì¼€ì¼ì„ ë³´ì •\n",
    "    # LN ì¶œë ¥ì´ ì‘ì•„ì§€ë©´, fc ì…ë ¥ ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼ ë¹„ìœ¨ë¡œ í‚¤ì›Œì„œ feature scale ê· í˜• ìœ ì§€\n",
    "    \n",
    "    for fc in fcs:\n",
    "        fc.weight.mul_(scales.view(1, -1))\n",
    "    \n",
    "    # NaN ê²€ì¦ (ì•ˆì •ì„± ì²´í¬)\n",
    "    for p in ln.parameters():\n",
    "        assert torch.isnan(p).sum() == 0\n",
    "    for fc in fcs:\n",
    "        for p in fc.parameters():\n",
    "            assert torch.isnan(p).sum() == 0\n",
    "            \n",
    "@torch.no_grad()\n",
    "def scale_fc_fc(fc1, fc2, scales):\n",
    "    \"\"\"\n",
    "    ë‘ ê°œì˜ ì—°ì†ëœ Linear layer(fc1 â†’ fc2) ê°„ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(fc1, nn.Linear)\n",
    "    assert isinstance(fc2, nn.Linear)\n",
    "\n",
    "    scales = scales.to(fc1.weight.device)\n",
    "    \n",
    "    # fc1ì˜ ì¶œë ¥ ì±„ë„ ë°©í–¥ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ë³´ì •\n",
    "    # ì£¼ì„ëœ ë¼ì¸ì€ ì „ì²´ weightì— ì ìš©í•˜ëŠ” ë°©ì‹ (í˜„ì¬ëŠ” ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ì ìš©)\n",
    "    fc1.weight[-scales.size(0):].div_(scales.view(-1, 1))\n",
    "    \n",
    "    # biasê°€ ì¡´ì¬í•  ê²½ìš° ë™ì¼í•˜ê²Œ ë³´ì •\n",
    "    if fc1.bias is not None:\n",
    "        fc1.bias.div_(scales.view(-1))\n",
    "        \n",
    "    # fc2ì˜ ì…ë ¥ ì±„ë„ ë°©í–¥ìœ¼ë¡œ ìŠ¤ì¼€ì¼ì„ ê³±í•´ normalization í›„ feature scale ë³µì›\n",
    "    fc2.weight.mul_(scales.view(1, -1))\n",
    "\n",
    "    # NaN ê²€ì¦ (ì•ˆì •ì„± ì²´í¬)\n",
    "    for p in fc1.parameters():\n",
    "        assert torch.isnan(p).sum() == 0\n",
    "    for p in fc2.parameters():\n",
    "        assert torch.isnan(p).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae72bf-680f-426b-911b-2d99cf524814",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def auto_scale_block(module, name, w_bit,\n",
    "                     q_group_size,\n",
    "                     input_feat):\n",
    "\n",
    "    # find the best scale ratio\n",
    "    def _search_module_scale(block, linears2scale: list, x, kwargs={}):\n",
    "        x= x.to(next(block.parameters()).device)\n",
    "        with torch.no_grad():\n",
    "            org_out = block(x, **kwargs)\n",
    "            if isinstance(org_out, tuple):\n",
    "                org_out = org_out[0]\n",
    "                \n",
    "        s_x = x.view(-1, x.shape[-1]).abs().mean(0)\n",
    "        s_x = torch.clamp(s_x, 1e-5)  \n",
    "        \n",
    "        # Step 1: best_error, best_ratio, ë° best_scalesë¥¼ ì´ˆê¸°í™”\n",
    "        best_error = torch.inf\n",
    "        best_ratio = -1\n",
    "        best_scales = 0\n",
    "        \n",
    "        n_grid = 20\n",
    "        history = []\n",
    "        \n",
    "        org_sd = {k: v.cpu() for k, v in block.state_dict().items()}\n",
    "        for ratio in range(n_grid):\n",
    "            # ratio is the \\alpha in the formula\n",
    "            ratio = ratio * 1 / n_grid\n",
    "        \n",
    "            ############### YOUR CODE STARTS HERE ###############\n",
    "            # Step 2: ê³µì‹ì— ë”°ë¼ ìŠ¤ì¼€ì¼ ê³„ì‚°\n",
    "            scales = \n",
    "            ############### YOUR CODE ENDS HERE #################\n",
    "            assert scales.shape == s_x.shape\n",
    "\n",
    "            scales = scales / (scales.max() * scales.min()).sqrt().view(1, -1)\n",
    "\n",
    "            for fc in linears2scale:\n",
    "                scales = scales.to(fc.weight.device)\n",
    "                ############### YOUR CODE STARTS HERE ###############\n",
    "                # Step 3: scale_factorë¥¼ ì´ìš©í•´ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì˜ ê°’ì„ í™•ëŒ€í•©ë‹ˆë‹¤.\n",
    "                fc.weight.data *= \n",
    "                fc.weight.data = pseudo_quantize_tensor(fc.weight.data, w_bit, q_group_size)\n",
    "                # Step 4: scale_factorë¥¼ ì´ìš©í•´ ì¤‘ìš”í•œ ê°€ì¤‘ì¹˜ ì±„ë„ì˜ ê°’ì„ ë‹¤ì‹œ ì¶•ì†Œí•˜ì„¸ìš”.\n",
    "                fc.weight.data /= \n",
    "                ############### YOUR CODE ENDS HERE #################\n",
    "            out = block(x, **kwargs)\n",
    "            if isinstance(out, tuple):\n",
    "                out = out[0]\n",
    "\n",
    "            loss = (org_out - out).float().pow(2).mean().item()  # float prevents overflow\n",
    "            history.append(loss)\n",
    "            is_best = loss < best_error\n",
    "            if is_best:\n",
    "                best_error = loss\n",
    "                best_ratio = ratio\n",
    "                best_scales = scales\n",
    "            block.load_state_dict(org_sd)\n",
    "        if best_ratio == -1:\n",
    "            print(history)\n",
    "            raise Exception\n",
    "        \n",
    "        best_scales = best_scales.view(-1)\n",
    "\n",
    "        assert torch.isnan(best_scales).sum() == 0, best_scales\n",
    "        return best_scales.detach()\n",
    "\n",
    "    ### ì´ê² ê³¼ì—° ë‚˜ì˜¬ê¹Œ? ë‹¨ìˆœ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ì¸ë“¯ ###\n",
    "    # (1) Self-Attention ì…ë ¥ ë¶€ë¶„ (Q, K, V projection)\n",
    "    inp = input_feat[name + '.self_attn.out_proj']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0).unsqueeze(0)\n",
    "    qkv = [module.self_attn.q_proj, module.self_attn.k_proj, module.self_attn.v_proj]\n",
    "    final_scales = _search_module_scale(module.self_attn, qkv, inp)\n",
    "    scale_ln_fcs(module.self_attn_layer_norm, qkv, final_scales)\n",
    "\n",
    "    # (2) Self-Attention ì¶œë ¥ ë¶€ë¶„ (out_proj)\n",
    "    inp = input_feat[name + '.self_attn.out_proj']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n",
    "    final_scales = _search_module_scale(module.self_attn.out_proj, [module.self_attn.out_proj], inp)\n",
    "    scale_fc_fc(module.self_attn.v_proj, module.self_attn.out_proj, final_scales)\n",
    "\n",
    "    # (3) Feed-Forward Network ì²« ë²ˆì§¸ FC (fc1)\n",
    "    inp = input_feat[name + '.fc1']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n",
    "    final_scales = _search_module_scale(module.fc1, [module.fc1], inp)\n",
    "    scale_ln_fcs(module.final_layer_norm, module.fc1, final_scales)\n",
    "\n",
    "    # (4) Feed-Forward Network ë‘ ë²ˆì§¸ FC (fc2)\n",
    "    inp = input_feat[name + '.fc2']\n",
    "    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n",
    "    final_scales = _search_module_scale(module.fc2, [module.fc2], inp)\n",
    "    scale_fc_fc(module.fc1, module.fc2, final_scales)\n",
    "    ### ì´ê² ê³¼ì—° ë‚˜ì˜¬ê¹Œ? ë‹¨ìˆœ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ì¸ë“¯ ###\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_quantize_model_weight_auto_scale(model, w_bit, q_group_size, input_feat):\n",
    "    \"\"\"\n",
    "    OPT ê³„ì—´ ëª¨ë¸ ì „ì²´ì— ëŒ€í•´ ìë™ ìŠ¤ì¼€ì¼ ì¡°ì •(auto-scale) ê¸°ë°˜ pseudo quantization ìˆ˜í–‰.\n",
    "    ê° Decoder Layer ë‹¨ìœ„ë¡œ auto_scale_blockì„ ì ìš©.\n",
    "    \"\"\"\n",
    "    from transformers.models.opt.modeling_opt import OPTDecoderLayer\n",
    "\n",
    "    # ê° OPTDecoderLayer(Transformer ë¸”ë¡)ì— ëŒ€í•´ auto scaling ìˆ˜í–‰\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, OPTDecoderLayer):\n",
    "            auto_scale_block(module, name, w_bit, q_group_size, input_feat)\n",
    "\n",
    "    # ëª¨ë“  Linear layerì— pseudo quantization ìµœì¢… ì ìš©\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data = pseudo_quantize_tensor(\n",
    "                m.weight.data, n_bit=w_bit, q_group_size=q_group_size\n",
    "            )\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b85c5-8e8f-4b58-b358-7b957b4fc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model.model_reset()\n",
    "pseudo_quantize_model_weight_auto_scale(llm_model.model, w_bit=3, q_group_size=128, input_feat=input_feat)\n",
    "llm_model.model_changed = True\n",
    "\n",
    "# Evaluate and delete the model\n",
    "llm_model.model_evaluate(data_width=3, group_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d69d4-1366-4600-8c79-b2fe8a5db7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Weight and Activation Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85dd70a-4791-4c4d-a999-bca0973c0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A8Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    8-bit Weight(A) Ã— 8-bit Activation(A) ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ìš© Linear Layer.\n",
    "    ì‹¤ì œ ì—°ì‚°ì€ FP16ìœ¼ë¡œ ìˆ˜í–‰ë˜ì§€ë§Œ, ì—°ì‚° ì „í›„ì— quantize/dequantize ë‹¨ê³„ë¥¼ í†µí•´\n",
    "    8-bit ì •ë°€ë„ì˜ íš¨ê³¼ë¥¼ ëª¨ì‚¬\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        act_quant=\"per_token\",\n",
    "        quantize_output=False,\n",
    "        quantize_bits=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Weight ì´ˆê¸°í™” (FP16)\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Bias ì´ˆê¸°í™” (FP16)\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        # ì…ë ¥ activation quantization ë°©ì‹ ì„ íƒ\n",
    "        # per_token: í† í°ë³„ë¡œ ìŠ¤ì¼€ì¼ ê³„ì‚°\n",
    "        # per_tensor: ì „ì²´ í…ì„œ ë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ ê³„ì‚°\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=8)\n",
    "        elif act_quant == \"per_tensor\":\n",
    "            self.act_quant_name = \"per_tensor\"\n",
    "            self.act_quant = partial(quantize_activation_per_tensor_absmax, n_bits=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "        # ì¶œë ¥ activation quantization ì˜µì…˜ (ì˜ˆ: BMM ì…ë ¥ìš©)\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x  # ì¶œë ¥ quantization ë¯¸ì ìš©\n",
    "\n",
    "        self.quantize_bits = quantize_bits\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        # .to() í˜¸ì¶œ ì‹œ weightì™€ biasë„ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "        super(W8A8Linear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        ìˆœì „íŒŒ (forward pass)\n",
    "        1. ì…ë ¥ activation quantization\n",
    "        2. Linear ì—°ì‚°\n",
    "        3. ì¶œë ¥ quantization\n",
    "        \"\"\"\n",
    "        q_x = self.act_quant(x)\n",
    "        y = torch.functional.F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        module, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False, quantize_bits=8\n",
    "    ):\n",
    "        \"\"\"\n",
    "        FP16 Linear ëª¨ë“ˆì„ ë°›ì•„, W8A8Linearë¡œ ë³€í™˜\n",
    "        Weightë¥¼ 8-bitë¡œ quantizeí•˜ê³ , activation quantization í•¨ìˆ˜ë¥¼ ì—°ê²°í•¨.\n",
    "        \"\"\"\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = W8A8Linear(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "\n",
    "        # Weight quantization ë°©ì‹ ì„ íƒ\n",
    "        if weight_quant == \"per_channel\":\n",
    "            # ì¶œë ¥ ì±„ë„ë³„ ìŠ¤ì¼€ì¼ ê³„ì‚°\n",
    "            new_module.weight = quantize_weight_per_channel_absmax(\n",
    "                module.weight, n_bits=new_module.quantize_bits\n",
    "            )\n",
    "        elif weight_quant == \"per_tensor\":\n",
    "            # ì „ì²´ í…ì„œ ë‹¨ìœ„ ìŠ¤ì¼€ì¼ ê³„ì‚°\n",
    "            new_module.weight = quantize_weight_per_tensor_absmax(\n",
    "                module.weight, n_bits=new_module.quantize_bits\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "\n",
    "        # bias ë³µì‚¬\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"W8A8Linear({self.in_features}, {self.out_features}, \"\n",
    "            f\"bias={self.bias is not None}, \"\n",
    "            f\"weight_quant={self.weight_quant_name}, \"\n",
    "            f\"act_quant={self.act_quant_name}, \"\n",
    "            f\"output_quant={self.output_quant_name})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc8098-3333-48f6-b9d5-c82cdf1fc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax(w, n_bits=8):\n",
    "    \"\"\"\n",
    "    weight per-channel quantization (ì¶œë ¥ ì±„ë„ë³„ ìµœëŒ€ê°’ ìŠ¤ì¼€ì¼)\n",
    "    \"\"\"\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_tensor_absmax(w, n_bits=8):\n",
    "    \"\"\"\n",
    "    weight per-tensor quantization (ì „ì²´ í…ì„œì˜ absmax ê¸°ì¤€)\n",
    "    \"\"\"\n",
    "    scales = w.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "    \"\"\"\n",
    "    activation per-token quantization:\n",
    "    ê° í† í°(=ì‹œí€€ìŠ¤ ë‹¨ìœ„)ë§ˆë‹¤ absmax ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§.\n",
    "    \"\"\"\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_tensor_absmax(t, n_bits=8):\n",
    "    \"\"\"\n",
    "    activation per-tensor quantization:\n",
    "    ì „ì²´ í…ì„œì˜ absmaxë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§.\n",
    "    \"\"\"\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "def quantize_opt(\n",
    "    model,\n",
    "    weight_quant=\"per_tensor\",\n",
    "    act_quant=\"per_tensor\",\n",
    "    quantize_bmm_input=True,\n",
    "    quantize_bits=8,\n",
    "):\n",
    "    \"\"\"\n",
    "    OPT ëª¨ë¸ì— W8A8 quantizationì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜.\n",
    "    Linear ë° Attention projection ê³„ì¸µì„ W8A8Linearë¡œ êµì²´í•¨.\n",
    "\n",
    "    - weight_quant: 'per_channel' ë˜ëŠ” 'per_tensor'\n",
    "    - act_quant: 'per_token' ë˜ëŠ” 'per_tensor'\n",
    "    - quantize_bmm_input: Trueì´ë©´ q_proj, k_proj, v_proj ì¶œë ¥ë„ quantizeí•˜ì—¬ BMM ì…ë ¥ ì‹œë®¬ë ˆì´ì…˜\n",
    "    \"\"\"\n",
    "    from transformers.models.opt.modeling_opt import OPTAttention, OPTDecoderLayer\n",
    "\n",
    "    for name, m in model.model.named_modules():\n",
    "        if isinstance(m, OPTDecoderLayer):\n",
    "            # Feed-Forward ê³„ì¸µ (fc1, fc2) quantization\n",
    "            m.fc1 = W8A8Linear.from_float(\n",
    "                m.fc1,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_bits=quantize_bits,\n",
    "            )\n",
    "            m.fc2 = W8A8Linear.from_float(\n",
    "                m.fc2,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_bits=quantize_bits,\n",
    "            )\n",
    "\n",
    "        elif isinstance(m, OPTAttention):\n",
    "            # Attention ë‚´ë¶€ q, k, v, out projection quantization\n",
    "            # quantize_bmm_input=Trueì´ë©´ BMM ì…ë ¥ìš©ìœ¼ë¡œ q/k/v ì¶œë ¥ë„ quantize ì‹œë®¬ë ˆì´ì…˜\n",
    "            m.q_proj = W8A8Linear.from_float(\n",
    "                m.q_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "                quantize_bits=quantize_bits,\n",
    "            )\n",
    "            m.k_proj = W8A8Linear.from_float(\n",
    "                m.k_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "                quantize_bits=quantize_bits,\n",
    "            )\n",
    "            m.v_proj = W8A8Linear.from_float(\n",
    "                m.v_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_output=quantize_bmm_input,\n",
    "                quantize_bits=quantize_bits,\n",
    "            )\n",
    "            m.out_proj = W8A8Linear.from_float(\n",
    "                m.out_proj,\n",
    "                weight_quant=weight_quant,\n",
    "                act_quant=act_quant,\n",
    "                quantize_bits=quantize_bits,\n",
    "            )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e33a7e-0ced-4710-9f14-766116aca5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610cf63-7f52-4151-970b-d7a4e7021326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71bb68-c7a1-4037-8553-e1f248410100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
