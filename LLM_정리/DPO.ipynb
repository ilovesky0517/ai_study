{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0204bd-603e-4d6b-a785-cbc9000bae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. ë°ì´í„°ì…‹ ë° Collate í•¨ìˆ˜ (DPOìš©)\n",
    "# DPOëŠ” (í”„ë¡¬í”„íŠ¸, ì„ íƒëœ ë‹µë³€, ê±°ë¶€ëœ ë‹µë³€)ì˜ ìŒì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "# ==========================================\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        \n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        \n",
    "        # ë°ì´í„°ë¥¼ ë¯¸ë¦¬ í† í°í™”(Tokenization)í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        for entry in data:\n",
    "            prompt = format_input(entry)\n",
    "            rejected_response = entry[\"rejected\"] # ğŸ‘ ëœ ì„ í˜¸ë˜ëŠ” ë‹µë³€\n",
    "            chosen_response = entry[\"chosen\"]     # ğŸ‘ ë” ì„ í˜¸ë˜ëŠ” ë‹µë³€\n",
    "\n",
    "            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ë§Œ ë”°ë¡œ ì¸ì½”ë”© (ë‚˜ì¤‘ì— ë§ˆìŠ¤í‚¹í•˜ê¸° ìœ„í•´ í•„ìš”)\n",
    "            prompt_tokens = tokenizer.encode(prompt)\n",
    "            \n",
    "            # í”„ë¡¬í”„íŠ¸ + ë‹µë³€ í˜•íƒœë¡œ ì „ì²´ ë¬¸ì¥ì„ ì¸ì½”ë”©\n",
    "            chosen_full_tokens = tokenizer.encode(f\"{prompt}\\n\\n### Response:\\n{chosen_response}\")\n",
    "            rejected_full_tokens = tokenizer.encode(f\"{prompt}\\n\\n### Response:\\n{rejected_response}\")\n",
    "\n",
    "            self.encoded_texts.append({\n",
    "                \"prompt\": prompt_tokens,\n",
    "                \"chosen\": chosen_full_tokens,\n",
    "                \"rejected\": rejected_full_tokens,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2693a69-26e6-49de-a736-9b213762dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token_id=50256, allowed_max_length=None, mask_prompt_tokens=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜(Batch) ë‚´ì˜ ë°ì´í„° ê¸¸ì´ë¥¼ ë§ì¶”ê³ (Padding), ë§ˆìŠ¤í¬(Mask)ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "    batch_data = {\n",
    "        \"prompt\": [],\n",
    "        \"chosen\": [],\n",
    "        \"rejected\": [],\n",
    "        \"rejected_mask\": [], # Loss ê³„ì‚° ì‹œ ë¬´ì‹œí•  ë¶€ë¶„ (íŒ¨ë”© ë“±)\n",
    "        \"chosen_mask\": []\n",
    "    }\n",
    "    \n",
    "    # 1. ë°°ì¹˜ ë‚´ì—ì„œ ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì°¾ê¸° (íŒ¨ë”©ì„ ìœ„í•´)\n",
    "    max_length_common = 0\n",
    "    if batch:\n",
    "        for key in [\"chosen\", \"rejected\"]:\n",
    "            current_max = max(len(item[key]) + 1 for item in batch)\n",
    "            max_length_common = max(max_length_common, current_max)\n",
    "            \n",
    "            # 2. íŒ¨ë”© ì¶”ê°€ ë° ë§ˆìŠ¤í¬ ìƒì„±\n",
    "    for item in batch:\n",
    "        prompt = torch.tensor(item[\"prompt\"])\n",
    "        batch_data[\"prompt\"].append(prompt)\n",
    "\n",
    "        for key in [\"chosen\", \"rejected\"]:\n",
    "            sequence = item[key]\n",
    "            # ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° íŒ¨ë”© í† í° ì¶”ê°€\n",
    "            padded = sequence + [pad_token_id] * (max_length_common - len(sequence))\n",
    "            \n",
    "            # ê¸°ë³¸ ë§ˆìŠ¤í¬: ë°ì´í„°ê°€ ìˆëŠ” ê³³ì€ 1(True), íŒ¨ë”©ì€ 0(False)\n",
    "            mask = torch.ones(len(padded)).bool()\n",
    "            mask[len(sequence):] = False  # íŒ¨ë”© ë¶€ë¶„ ë§ˆìŠ¤í‚¹\n",
    "\n",
    "            # [ì¤‘ìš”] í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹ (ì„ íƒì )\n",
    "            # DPOëŠ” 'ë‹µë³€'ì˜ í™•ë¥  ì°¨ì´ë¥¼ í•™ìŠµí•˜ë¯€ë¡œ, ì§ˆë¬¸(Prompt) ë¶€ë¶„ì€ Loss ê³„ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.\n",
    "            if mask_prompt_tokens:\n",
    "                mask[:prompt.shape[0] + 2] = False \n",
    "\n",
    "            batch_data[key].append(torch.tensor(padded))\n",
    "            batch_data[f\"{key}_mask\"].append(mask)\n",
    "            \n",
    "    # 3. í…ì„œ ë³€í™˜ ë° ë””ë°”ì´ìŠ¤(GPU/CPU) ì´ë™\n",
    "    for key in [\"chosen\", \"rejected\", \"chosen_mask\", \"rejected_mask\"]:\n",
    "        tensor_stack = torch.stack(batch_data[key])\n",
    "        # ê¸¸ì´ ì œí•œì´ ìˆë‹¤ë©´ ìë¥´ê¸°\n",
    "        if allowed_max_length is not None:\n",
    "            tensor_stack = tensor_stack[:, :allowed_max_length]\n",
    "        batch_data[key] = tensor_stack.to(device)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5eab37-7df9-4a8c-8bb5-3933de309d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. DPO Loss ë° Log Probability ê³„ì‚° í•¨ìˆ˜\n",
    "# DPOì˜ í•µì‹¬ ìˆ˜í•™ì  ë¡œì§ì´ ë“¤ì–´ìˆëŠ” ë¶€ë¶„.\n",
    "# ==========================================\n",
    "def compute_logprobs(logits, labels, selection_mask=None):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì˜ ì¶œë ¥(logits)ê³¼ ì •ë‹µ(labels)ì„ ë°›ì•„ í•´ë‹¹ ì •ë‹µ í† í°ì˜ ë¡œê·¸ í™•ë¥ ì„ ê³„ì‚°.\n",
    "    \"\"\"\n",
    "    # Auto-regressive ëª¨ë¸ íŠ¹ì„±ìƒ, ì…ë ¥ [A, B, C]ì— ëŒ€í•´ ì˜ˆì¸¡ì€ [B, C, D]ê°€ ë˜ë¯€ë¡œ ì‹œí”„íŠ¸(Shift)\n",
    "    labels = labels[:, 1:].clone()\n",
    "    logits = logits[:, :-1, :]\n",
    "    \n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # ì‹¤ì œ ì •ë‹µ ë ˆì´ë¸”ì— í•´ë‹¹í•˜ëŠ” í™•ë¥ ê°’ë§Œ ì¶”ì¶œ (gather ì‚¬ìš©)\n",
    "    selected_log_probs = torch.gather(\n",
    "        input=log_probs,\n",
    "        dim=-1,\n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    \n",
    "    if selection_mask is not None:\n",
    "        # ë§ˆìŠ¤í¬ë„ ì‹œí”„íŠ¸í•˜ì—¬ ì ìš© (íŒ¨ë”©ì´ë‚˜ í”„ë¡¬í”„íŠ¸ ì˜ì—­ ë¬´ì‹œ)\n",
    "        mask = selection_mask[:, 1:].clone()\n",
    "        selected_log_probs = selected_log_probs * mask\n",
    "        \n",
    "        # ìœ íš¨í•œ í† í°ë“¤ì˜ ë¡œê·¸ í™•ë¥  í‰ê·  ê³„ì‚°\n",
    "        avg_log_prob = selected_log_probs.sum(-1) / mask.sum(-1)\n",
    "        return avg_log_prob\n",
    "    else:\n",
    "        return selected_log_probs.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb4462-837f-4ce9-a8c3-a3d9575d828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss(model_chosen_logprobs, model_rejected_logprobs, \n",
    "                     reference_chosen_logprobs, reference_rejected_logprobs, beta=0.1):\n",
    "    \"\"\"\n",
    "    DPO ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°:\n",
    "    Policy ëª¨ë¸ì´ Reference ëª¨ë¸ë³´ë‹¤ 'chosen' ë‹µë³€ì„ ë” ì„ í˜¸í•˜ê³ , 'rejected' ë‹µë³€ì„ ëœ ì„ í˜¸í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    beta: Reference ëª¨ë¸ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚ ì§€ ì œì–´í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë³´í†µ 0.1~0.5)\n",
    "    \"\"\"\n",
    "    # ëª¨ë¸ì˜ (Chosen - Rejected) ë¡œê·¸ í™•ë¥  ì°¨ì´\n",
    "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
    "    # ê¸°ì¤€ ëª¨ë¸ì˜ (Chosen - Rejected) ë¡œê·¸ í™•ë¥  ì°¨ì´\n",
    "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
    "    \n",
    "    # ë‘ ë¹„ìœ¨ì˜ ì°¨ì´ (Policyê°€ Referenceë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ì˜ êµ¬ë¶„í–ˆëŠ”ê°€)\n",
    "    logits = model_logratios - reference_logratios\n",
    "    \n",
    "    # Sigmoid í›„ ìŒìˆ˜ ë¡œê·¸ (Cross Entropyì™€ ìœ ì‚¬) -> ì´ ê°’ì„ ìµœì†Œí™”í•˜ë©´ ì„ í˜¸ë„ ì°¨ì´ê°€ ê·¹ëŒ€í™”ë¨\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "    \n",
    "    # í•™ìŠµ ì¶”ì ìš© ë³´ìƒ(Reward) ê³„ì‚° (ì‹¤ì œ í•™ìŠµì—” ì•ˆ ì“°ì´ê³  ë¡œê¹…ìš©)\n",
    "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
    "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
    "    \n",
    "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0dc98f-ddb9-4051-9878-83cabe7f291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss_batch(batch, policy_model, reference_model, beta):\n",
    "    \"\"\"\n",
    "    í•˜ë‚˜ì˜ ë°°ì¹˜ì— ëŒ€í•´ ì „ì²´ DPO ê³¼ì •ì„ ìˆ˜í–‰í•˜ëŠ” í—¬í¼ í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    # 1. í•™ìŠµ ì¤‘ì¸ ëª¨ë¸(Policy Model)ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚° (ê¸°ìš¸ê¸° ê³„ì‚° O)\n",
    "    policy_chosen_log_probas = compute_logprobs(\n",
    "        logits=policy_model(batch[\"chosen\"]),\n",
    "        labels=batch[\"chosen\"],\n",
    "        selection_mask=batch[\"chosen_mask\"]\n",
    "    )\n",
    "    policy_rejected_log_probas = compute_logprobs(\n",
    "        logits=policy_model(batch[\"rejected\"]),\n",
    "        labels=batch[\"rejected\"],\n",
    "        selection_mask=batch[\"rejected_mask\"]\n",
    "    )\n",
    "    \n",
    "    # 2. ê¸°ì¤€ ëª¨ë¸(Reference Model)ì˜ ë¡œê·¸ í™•ë¥  ê³„ì‚° (ê¸°ìš¸ê¸° ê³„ì‚° X -> ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    with torch.no_grad():\n",
    "        ref_chosen_log_probas = compute_logprobs(\n",
    "            logits=reference_model(batch[\"chosen\"]),\n",
    "            labels=batch[\"chosen\"],\n",
    "            selection_mask=batch[\"chosen_mask\"]\n",
    "        )\n",
    "        ref_rejected_log_probas = compute_logprobs(\n",
    "            logits=reference_model(batch[\"rejected\"]),\n",
    "            labels=batch[\"rejected\"],\n",
    "            selection_mask=batch[\"rejected_mask\"]\n",
    "        )\n",
    "\n",
    "    # 3. ìµœì¢… Loss ê³„ì‚°\n",
    "    loss, chosen_rewards, rejected_rewards = compute_dpo_loss(\n",
    "        model_chosen_logprobs=policy_chosen_log_probas,\n",
    "        model_rejected_logprobs=policy_rejected_log_probas,\n",
    "        reference_chosen_logprobs=ref_chosen_log_probas,\n",
    "        reference_rejected_logprobs=ref_rejected_log_probas,\n",
    "        beta=beta\n",
    "    )\n",
    "    return loss, chosen_rewards, rejected_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362b3ce-66ab-4c8c-b461-96142b6e57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. í‰ê°€ ë° í•™ìŠµ ë£¨í”„\n",
    "# ì‹¤ì œ ëª¨ë¸ í•™ìŠµì„ ëŒë¦¬ëŠ” ë©”ì¸ ë£¨í”„ì…ë‹ˆë‹¤.\n",
    "# ==========================================\n",
    "def evaluate_dpo_loss_loader(policy_model, reference_model, train_loader, val_loader, beta, eval_iter):\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ì¤‘ê°„ì— ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€(Validation)í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    policy_model.eval() # í‰ê°€ ëª¨ë“œ ì „í™˜ (Dropout ë“± ë¹„í™œì„±í™”)\n",
    "    with torch.no_grad():\n",
    "        # ë¡œë”ë¥¼ ìˆœíšŒí•˜ë©° í‰ê·  Loss ê³„ì‚°í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜\n",
    "        def compute_loader_metric(loader):\n",
    "            total_loss, total_chosen, total_rejected = 0., 0., 0.\n",
    "            num_batches = min(eval_iter, len(loader))\n",
    "            if num_batches == 0: return float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "            \n",
    "            for i, batch in enumerate(loader):\n",
    "                if i >= num_batches: break\n",
    "                loss, chosen, rejected = compute_dpo_loss_batch(batch, policy_model, reference_model, beta)\n",
    "                total_loss += loss.item()\n",
    "                total_chosen += chosen.item()\n",
    "                total_rejected += rejected.item()\n",
    "            return total_loss/num_batches, total_chosen/num_batches, total_rejected/num_batches\n",
    "\n",
    "        train_loss, train_chosen, train_rejected = compute_loader_metric(train_loader)\n",
    "        val_loss, val_chosen, val_rejected = compute_loader_metric(val_loader)\n",
    "\n",
    "    policy_model.train() # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
    "    return {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_chosen_reward\": train_chosen,\n",
    "        \"train_rejected_reward\": train_rejected,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_chosen_reward\": val_chosen,\n",
    "        \"val_rejected_reward\": val_rejected\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824daf63-832e-498f-81f9-21f32703e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_dpo_simple(policy_model, reference_model, train_loader, val_loader, \n",
    "                           optimizer, num_epochs, beta, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \"\"\"\n",
    "    ì „ì²´ í•™ìŠµ ê³¼ì •ì„ ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    tracking = {\"train_losses\": [], \"val_losses\": [], \"tokens_seen\": []}\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        policy_model.train() # í•™ìŠµ ëª¨ë“œ ì‹œì‘\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad() # ì´ì „ ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
    "            \n",
    "            # Loss ê³„ì‚°\n",
    "            loss, chosen_rewards, rejected_rewards = compute_dpo_loss_batch(\n",
    "                batch=batch, policy_model=policy_model, reference_model=reference_model, beta=beta\n",
    "            )\n",
    "            \n",
    "            loss.backward() # ì—­ì „íŒŒ (ê¸°ìš¸ê¸° ê³„ì‚°)\n",
    "            optimizer.step() # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "            \n",
    "            tokens_seen += batch[\"chosen\"].numel()\n",
    "            global_step += 1\n",
    "          \n",
    "\n",
    "        # ì—í¬í¬ê°€ ëë‚  ë•Œë§ˆë‹¤ ìƒ˜í”Œ ìƒì„±í•˜ì—¬ ìœ¡ì•ˆìœ¼ë¡œ í™•ì¸\n",
    "        generate_and_print_sample(model=policy_model, tokenizer=tokenizer, device=loss.device, start_context=start_context)\n",
    "\n",
    "    return tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a452ff8d-bfe2-44d0-91b9-fda26d0ca0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
