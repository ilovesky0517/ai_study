{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb8b64-07d4-43e8-aa3a-a09a2bb9b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###중요: LoRA 구현 부분\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. LoRA 클래스 및 함수 정의\n",
    "# -----------------------------------------------------------------------------\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        \"\"\"\n",
    "        LoRA(Low-Rank Adaptation) 레이어 초기화\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.A = nn.Parameter(torch.empty(in_dim, rank))\n",
    "        # 입력을 저차원 공간(rank)으로 투영하는 역할.\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        # kaiming_uniform_ 초기화로 학습 안정성을 확보. 그래서 앞에 empty로 하는듯?\n",
    "        \n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        # 저차원 표현을 다시 원래 출력 차원으로 확장하는 역할.\n",
    "        # 학습 초기에 원래 모델의 출력에 영향을 주지 않도록 설계.\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "    def forward(self, x):\n",
    "        x = (self.alpha / self.rank) * (x @ self.A @ self.B)\n",
    "        # LoRA 논문에서 제안된 안정화 기법입니다. scaling factor = self.alpha / self.rank\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e2ce6-1b03-4fb2-bef8-e1a02e3db288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    기존의 Linear 레이어를 감싸서 LoRA 어댑터를 추가한 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 기존 Linear와 LoRA 출력 합산 x @ W + (self.alpha / self.rank) * (x @ A @ B) 가 되도록\n",
    "        return self.linear(x) + self.lora(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6da122-d72b-49f1-a068-a345badcabd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    \"\"\"\n",
    "    모델 내의 모든 Linear 레이어를 찾아 LoRA가 적용된 레이어로 교체\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # 기존 Linear 레이어를 LinearWithLoRA로 교체\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # 재귀 호출\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
