{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112aade6-17a9-4486-8bd6-f163a1455b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Attention > MultiHeadAttention > Transformer > GPT 순으로 확장해가면서 개념 파악하면 도움될 듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a173abc-fb0f-4ff6-bf2b-6d56a7370204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# GPT 아키텍처 구성 요소\n",
    "#####################################\n",
    "class LayerNorm(nn.Module):  # 층 정규화 (Layer Normalization): 학습 안정성을 높임\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) # 학습 가능한 스케일 파라미터 (Gamma)\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # 학습 가능한 시프트 파라미터 (Beta)\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82b205-7914-490b-b5fe-65051624d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "GPT 아키텍처 조립\n",
    "#####################################\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    표준 트랜스포머 블록 (Decoder Block)\n",
    "    구조: LayerNorm -> Attention -> Add(Residual) -> LayerNorm -> FeedForward -> Add(Residual)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self, x):\n",
    "        # region [어텐션 블록 (Residual Connection 적용)]\n",
    "        shortcut = x\n",
    "        x = self.norm1(x) # Pre-LayerNorm 방식\n",
    "        x = self.att(x)   \n",
    "        x = self.drop_shortcut(x)\n",
    "        ###################################################\n",
    "        x = x + shortcut  # 원본 입력을 더해줌 (기울기 소실 방지)\n",
    "        ###################################################\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        ###################################################\n",
    "        x = x + shortcut   # 원본 입력을 더해줌 (기울기 소실 방지)\n",
    "        ###################################################\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfc15d-6dc0-4828-88d1-9e45063ccd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    전체 GPT 모델 구조 정의\n",
    "    Embedding -> Transformer Blocks -> Final Norm -> Output Head\n",
    "    \n",
    "    GPTModel\n",
    "        __init__\n",
    "            # 토큰 임베딩 (단어 -> 벡터) 하나의 vocab 당 emb_dim으로 표현되니까 총 크기는 vocab_size * emb_dim       \n",
    "            # 위치 임베딩 (위치 정보 -> 벡터)\n",
    "            # 트랜스포머 블록 쌓기 (n_layers 만큼) *를 통해 리스트 언패킹\n",
    "            # 최종 정규화 및 출력 헤드\n",
    "            # 각 토큰 위치마다 vocab_size 크기의 벡터가 나온다.        \n",
    "        __forward__\n",
    "            # 1. tok, pos 임베딩 생성   \n",
    "            # 2. 토큰 임베딩과 위치 임베딩 합산\n",
    "            # 3. dropout\n",
    "            # 4. 트랜스포머 블록 통과\n",
    "            # 5.최종 출력 계산            \n",
    "    \"\"\"\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 토큰 임베딩 (단어 -> 벡터) 하나의 vocab 당 emb_dim으로 표현되니까 총 크기는 vocab_size * emb_dim\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        # 위치 임베딩 (위치 정보 -> 벡터)\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # 트랜스포머 블록 쌓기 (n_layers 만큼)\n",
    "        # *를 통해 리스트 언패킹 해준다. nn.Sequential은 리스트 입력 안받고 모듈들을 직접 나열해야 하기 때문에\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "       \n",
    "\n",
    "        # 최종 정규화 및 출력 헤드\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"]) # 마지막 임베딩 차원(emb_dim)에 대해 Layer Normalization을 적용\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        # 각 토큰 위치마다 vocab_size 크기의 벡터가 나온다. 이 벡터는 다음에 올 단어 후보들에 대한 로짓(logits)을 담는다.\n",
    "\n",
    "     def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "         # 1. tok, pos 임베딩 생성\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        \n",
    "        # 2. 토큰 임베딩과 위치 임베딩 합산\n",
    "        x = tok_embeds + pos_embeds \n",
    "        \n",
    "        # 3. dropout\n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        # 4. 트랜스포머 블록 통과\n",
    "        x = self.trf_blocks(x)    \n",
    "\n",
    "        # 5.최종 출력 계산\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x) # 각 단어에 대한 예측 점수 (Logits)\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be563bd9-e4be-4ca6-860e-d4cd59def63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model의 예측 결과를 logits = logits[:, -1, :] 로 뽑아낸다는 것을 알아야 할 듯.\n",
    "\"\"\"\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \"\"\"\n",
    "    간단한 텍스트 생성 루프\n",
    "    현재 문맥을 넣어 다음 토큰을 예측하고, 이를 다시 문맥에 추가하여 반복함\n",
    "    \"\"\"\n",
    "    # idx: 현재 문맥의 토큰 인덱스들 (Batch, Time)\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 모델이 지원하는 최대 길이(context_size)를 넘지 않도록 자름\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "       \n",
    "        with torch.no_grad():  # 기울기 계산 불필요할 때, model.eval()등과 같이 쓰이고, @torch.no_grad()와 유사함.(적용 범위는 달라질 수 있음)\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # 마지막 타임스텝의 예측값만 가져옴 (다음 단어 예측이므로)\n",
    "        logits = logits[:, -1, :] # (batch, n_token, vocab_size) -> (batch, vocab_size)\n",
    "\n",
    "        # vocab_size 차원에서 가장 확률(로짓값)이 높은 인덱스 선택\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True) \n",
    "        \n",
    "        # 종료 조건 확인 (EOS 토큰이 나오면 중단) 엄청 중요한건 아닐듯?\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        # 예측된 토큰을 현재 시퀀스 뒤에 이어 붙임\n",
    "        idx = torch.cat((idx, idx_next), dim=1)     \n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff0940-0747-4c87-bd70-a97fd022740b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fb6a6-0d46-4a60-b766-eaa03ea0779e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc6fe0d-c21f-4513-8a63-0183fcc33578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8756c-6a8a-4839-8593-722e823d41fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db301c-dd74-4e14-9a0a-d07aaf8105b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42012a51-4925-4288-b283-db966385fc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd041787-c1c9-430b-8afa-275e45ebd3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27494296-27fc-4c75-a42c-5ead9056967e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
