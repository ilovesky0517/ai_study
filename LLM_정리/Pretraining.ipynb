{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83379e5-bc6d-4578-81d8-00d1a27746f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 중요: lOSS를 계산 ###\n",
    "# 손실 계산 (CrossEntropyLoss)\n",
    "# PyTorch의 CrossEntropyLoss는 입력을 (N, C) 형태로 받기를 원한다.\n",
    "# N: 전체 샘플 수 (여기서는 Batch_size * Sequence_length)\n",
    "# C: 클래스 수 (여기서는 Vocab_size)\n",
    "# 따라서 3차원 텐서를 2차원으로 평탄화(flatten) 해야 한다.\n",
    "# logits shape: (batch_size, sequence_length, vocab_size) 3차원을\n",
    "# logits.flatten(0, 1) -> (batch_size * sequence_length, vocab_size) 으로 2차원으로 변환\n",
    "# target_batch.flatten() -> (batch_size * sequence_length)\n",
    "loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0, 1), \n",
    "    target_batch.flatten()\n",
    ")\n",
    "# CrossEntropyLoss는 (N, C) 형태의 입력과 (N,) 형태의 정답 레이블을 요구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03291b5-9f3e-4f6e-934a-252e414e1086",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 모델 학습 과정 (반드시 외우기)\n",
    "    # model.train() 반드시 필요 // evaluation이나 test에서는 model.eval()\n",
    "    # 1. 기울기 초기화 > .zero_grad()\n",
    "    # 2. 데이터를 GPU로 이동 > .to(device)\n",
    "    # 3. 모델의 순전파 > model(...)\n",
    "    # 4. 손실 계산 > flatten(...)\n",
    "    # 5. 역전파 > .backward()\n",
    "    # 6. 가중치 업데이트 > step()\n",
    "\"\"\"\n",
    "    \n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "    \n",
    "     for epoch in range(num_epochs):\n",
    "        model.train()  # 훈련 시작 전 반드시 train 모드 설정\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # 1. 이전 배치에서 계산된 기울기 초기화(필수)\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # 2. 데이터를 GPU로 이동\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            \n",
    "            # 3. 모델의 순전파(Forward Pass)\n",
    "            logits = model(input_batch)\n",
    "            \n",
    "            # 4. 손실 계산\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                input_batch.flatten(0,1),\n",
    "                target_batch.flatten(),\n",
    "            )\n",
    "            \n",
    "            # 5. 역전파 (Backpropagation): 각 파라미터별 기울기(Gradient) 계산\n",
    "            loss.backward()\n",
    "            \n",
    "            # 6. 가중치 업데이트: 계산된 기울기를 이용해 파라미터 수정\n",
    "            optimizer.step()\n",
    "            \n",
    "            tokens_seen += input_batch.numel() # 처리한 토큰 수 카운트\n",
    "            global_step += 1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64ff65-fc03-44f5-858e-41a5d6837d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단계별 포인트\n",
    "\n",
    "def main(gpt_config, settings):\n",
    "    ##############################\n",
    "    # 1. 데이터 준비\n",
    "    ##############################\n",
    "    \n",
    "    ##############################\n",
    "    # 2. 모델 및 옵티마이저 초기화\n",
    "    ##############################\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=settings[\"learning_rate\"], weight_decay=settings[\"weight_decay\"]\n",
    "    ) # 이걸 다 외워야 할건 아닌거 같고 그냥 이런식으로 optimizer 선언한다고 숙지하면 될 듯.\n",
    "    \n",
    "    ##############################\n",
    "    # 3. 데이터 로더 구축 : 어떤식으로 데이터 나누는지 확인\n",
    "    ##############################\n",
    "    train_ratio = 0.90\n",
    "    split_idx = int(train_ratio * len(text_data))\n",
    "    \n",
    "    # 훈련 데이터 로더: 순서를 섞음(Shuffle=True)\n",
    "    train_loader = create_dataloader_v1(\n",
    "        text_data[:split_idx], # 0~ 전체 0.9에 대항하는 데이터까지\n",
    "        ...\n",
    "        drop_last=True, # 뒤에 자투리 남으면 버리자.\n",
    "        shuffle=True, # 데이터 섞어라. 학습에는 중요\n",
    "        ...\n",
    "    )\n",
    "    \n",
    "    # 검증 데이터 로더: 순서를 섞지 않음(Shuffle=False) -> 평가는 일관되게\n",
    "    val_loader = create_dataloader_v1(\n",
    "        \n",
    "        text_data[split_idx:], # 0.9 이후의 것들로 테스트\n",
    "        ...\n",
    "        drop_last=False, # 버리지 마라\n",
    "        shuffle=False,   # 섞지마!!\n",
    "        ...\n",
    "    )\n",
    "    ##############################\n",
    "    # 4. 훈련 시작\n",
    "    ##############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
