{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36176e7",
   "metadata": {
    "id": "e36176e7"
   },
   "source": [
    "# Assignment 3. Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c802d3",
   "metadata": {
    "id": "32c802d3"
   },
   "source": [
    "## Goals\n",
    "\n",
    "이 실습의 목적은 **Knowledge Distillation**을 활용하여, 작은 모델(Student)이 큰 모델(Teacher)의 지식을 효과적으로 학습하는 방법을 이해하고 실험을 통해 비교하는 것입니다.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. **Baseline 학습 (Cross-Entropy Loss)**\n",
    "    - Teacher 모델과 Student 모델을 각각 Cross-Entropy Loss만으로 학습시켜 정확도를 비교합니다.\n",
    "2. **Knowledge Distillation (Soft Targets)**\n",
    "    - Teacher의 softmax 출력을 활용한 Knowledge Distillation을 적용하고, temperature 및 loss weight에 따른 영향을 분석합니다.\n",
    "3. **Cosine Loss Minimization (Cosine Loss)**  \n",
    "    - Teacher와 Student의 convolutional feature를 추출하여, CosineEmbeddingLoss를 적용해 내부 표현 유사도를 증가시키는 방식으로 학습합니다.\n",
    "4. **Intermediate Regressor (Regressor + MSE)**\n",
    "    - Teacher의 feature map과 Student의 regressed feature map을 MSE로 정렬하며, 중간 표현을 직접 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532bbe95",
   "metadata": {
    "id": "532bbe95"
   },
   "source": [
    "# Environment Setup\n",
    "\n",
    "본 실습에서는 PyTorch와 Torchvision을 활용하여 Knowledge Distillation을 구현합니다. 먼저 필요한 라이브러리를 import하고, 실행 환경(GPU/CPU)을 설정합니다.\n",
    "\n",
    "## Import Modules\n",
    "- `torch`, `torch.nn`, `torch.optim`: PyTorch의 핵심 기능 및 신경망, 최적화 알고리즘\n",
    "- `torchvision.transforms`, `torchvision.datasets`: CIFAR-10 데이터셋 로딩 및 전처리를 위한 모듈\n",
    "- `collections.OrderedDict`: 이후에 모델 구조 정의 시 순서를 보장하기 위한 dict 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0908f919",
   "metadata": {
    "id": "0908f919"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daaf15f",
   "metadata": {
    "id": "5daaf15f"
   },
   "source": [
    "## Data Loading: CIFAR-10\n",
    "\n",
    "본 실습에서는 CIFAR-10 데이터셋을 사용하여 Knowledge Distillation의 효과를 검증합니다. CIFAR-10은 10개의 클래스로 구성된 32x32 크기의 컬러 이미지 데이터셋입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b49d9351",
   "metadata": {
    "id": "b49d9351",
    "outputId": "94e15a89-39a3-468e-ad78-bbd966f7c3a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to D:\\data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 170M/170M [00:11<00:00, 15.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\data\\cifar-10-python.tar.gz to D:\\data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Below we are preprocessing data for CIFAR-10. We use an arbitrary batch size of 128.\n",
    "transforms_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Loading the CIFAR-10 dataset:\n",
    "train_dataset = datasets.CIFAR10(root='D:\\\\data', train=True, download=True, transform=transforms_cifar)\n",
    "test_dataset = datasets.CIFAR10(root='D:\\\\data', train=False, download=True, transform=transforms_cifar)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764544a4",
   "metadata": {
    "id": "764544a4"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def set_seed(seed=44):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# 시드 고정\n",
    "def get_train_loader(train_dataset, seed=44):\n",
    "    set_seed(seed)\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                                               num_workers=0, generator=g, worker_init_fn=seed_worker)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde73335",
   "metadata": {
    "id": "dde73335"
   },
   "source": [
    "## Load Pretrained Model Weights (VGG on CIFAR-10)\n",
    "\n",
    "Knowledge Distillation에서 중요한 전제는 **강력한 성능을 가진 Teacher 모델**이 존재한다는 것입니다. 본 코드에서는 사전에 학습된 VGG 모델의 가중치를 불러와 Teacher 모델로 사용할 준비를 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9eb51a",
   "metadata": {
    "id": "1c9eb51a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/SKKU-ESLAB/pytorch-models/releases/download/samsung/vgg.cifar.pretrained.pth\" to D:\\data\\hub\\checkpoints\\vgg.cifar.pretrained.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 35.2M/35.2M [00:04<00:00, 8.88MB/s]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TORCH_HOME\"] = \"D:\\\\data\"\n",
    "state_dict_url = \"https://github.com/SKKU-ESLAB/pytorch-models/releases/download/samsung/vgg.cifar.pretrained.pth\"\n",
    "state_dict = torch.hub.load_state_dict_from_url(state_dict_url, progress=True)\n",
    "state_dict = state_dict[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115710f5",
   "metadata": {
    "id": "115710f5"
   },
   "source": [
    "## Define Teacher and Student Models\n",
    "\n",
    "Knowledge Distillation 실험을 위해 두 개의 모델 구조를 정의합니다. 두 모델은 VGG 스타일의 CNN 구조를 기반으로 하며, **Teacher (VGGCifar9)** 모델은 더 깊고 복잡한 구조, **Student (VGGCifar5)** 모델은 간단한 구조로 설계되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68281adf",
   "metadata": {
    "id": "68281adf"
   },
   "outputs": [],
   "source": [
    "class VGGCifar9(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, 64, 3, padding=1, bias=False)),\n",
    "            ('bn0', nn.BatchNorm2d(64)),\n",
    "            ('relu0', nn.ReLU(True)),\n",
    "            ('conv1', nn.Conv2d(64, 128, 3, padding=1, bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(128)),\n",
    "            ('relu1', nn.ReLU(True)),\n",
    "            ('pool0', nn.MaxPool2d(2)),\n",
    "            ('conv2', nn.Conv2d(128, 256, 3, padding=1, bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(256)),\n",
    "            ('relu2', nn.ReLU(True)),\n",
    "            ('conv3', nn.Conv2d(256, 256, 3, padding=1, bias=False)),\n",
    "            ('bn3', nn.BatchNorm2d(256)),\n",
    "            ('relu3', nn.ReLU(True)),\n",
    "            ('pool1', nn.MaxPool2d(2)),\n",
    "            ('conv4', nn.Conv2d(256, 512, 3, padding=1, bias=False)),\n",
    "            ('bn4', nn.BatchNorm2d(512)),\n",
    "            ('relu4', nn.ReLU(True)),\n",
    "            ('conv5', nn.Conv2d(512, 512, 3, padding=1, bias=False)),\n",
    "            ('bn5', nn.BatchNorm2d(512)),\n",
    "            ('relu5', nn.ReLU(True)),\n",
    "            ('pool2', nn.MaxPool2d(2)),\n",
    "            ('conv6', nn.Conv2d(512, 512, 3, padding=1, bias=False)),\n",
    "            ('bn6', nn.BatchNorm2d(512)),\n",
    "            ('relu6', nn.ReLU(True)),\n",
    "            ('conv7', nn.Conv2d(512, 512, 3, padding=1, bias=False)),\n",
    "            ('bn7', nn.BatchNorm2d(512)),\n",
    "            ('relu7', nn.ReLU(True)),\n",
    "            ('pool3', nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class VGGCifar5(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        # Generate the same scratch model\n",
    "        set_seed()\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, 64, 3, padding=1, bias=False)),\n",
    "            ('bn0', nn.BatchNorm2d(64)),\n",
    "            ('relu0', nn.ReLU(True)),\n",
    "            ('pool0', nn.MaxPool2d(2)),\n",
    "            ('conv1', nn.Conv2d(64, 128, 3, padding=1, bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(128)),\n",
    "            ('relu1', nn.ReLU(True)),\n",
    "            ('pool1', nn.MaxPool2d(2)),\n",
    "            ('conv2', nn.Conv2d(128, 256, 3, padding=1, bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(256)),\n",
    "            ('relu2', nn.ReLU(True)),\n",
    "            ('pool2', nn.MaxPool2d(2)),\n",
    "            ('conv3', nn.Conv2d(256, 256, 3, padding=1, bias=False)),\n",
    "            ('bn3', nn.BatchNorm2d(256)),\n",
    "            ('relu3', nn.ReLU(True)),\n",
    "            ('pool3', nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212de936",
   "metadata": {
    "id": "212de936"
   },
   "source": [
    "# 3.1. Baseline 학습 (Cross-Entropy Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae2925d",
   "metadata": {
    "id": "fae2925d"
   },
   "source": [
    "## Train & Test Functions\n",
    "\n",
    "학습 및 검증은 아래 두 함수로 수행되며, Student/Teacher 모델 모두 동일한 루프 구조를 따릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab177b4",
   "metadata": {
    "id": "6ab177b4"
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_loader,\n",
    "          epochs,\n",
    "          learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model,\n",
    "         test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  # for multiple outputs\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad797223",
   "metadata": {
    "id": "ad797223"
   },
   "source": [
    "## Load & Evaluate Teacher Model\n",
    "\n",
    "Knowledge Distillation의 기준이 되는 **Teacher 모델(VGGCifar9)** 을 초기화하고, 앞서 불러온 pretrained 가중치를 적용합니다. 이후, CIFAR-10 test set에 대해 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa80466f",
   "metadata": {
    "id": "aa80466f",
    "outputId": "7a3b5578-4457-4aed-aba6-c26d62a62264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.95%\n"
     ]
    }
   ],
   "source": [
    "teacher_model = VGGCifar9().cuda()\n",
    "teacher_model.load_state_dict(state_dict)\n",
    "test_accuracy_teacher = test(teacher_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863b5fe",
   "metadata": {
    "id": "9863b5fe"
   },
   "source": [
    "## 모델 초기화 일관성 확인\n",
    "\n",
    "Knowledge Distillation 실험에서 **공정한 비교**를 위해 Student 모델의 초기화 상태가 동일한지 확인하는 과정입니다. 여기서는 동일한 구조를 가진 두 개의 `VGGCifar5` 모델을 생성하고, 첫 번째 convolution layer의 weight norm을 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1591f3",
   "metadata": {
    "id": "fd1591f3",
    "outputId": "c4c65255-325e-4a30-fa02-637aa591b95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of 1st layer of student_model: 4.603283882141113\n",
      "Norm of 1st layer of student_model_2: 4.603283882141113\n"
     ]
    }
   ],
   "source": [
    "# Print the norm of the first layer of the initial lightweight model\n",
    "student_model = VGGCifar5().cuda()\n",
    "print(\"Norm of 1st layer of student_model:\", torch.norm(student_model.backbone[0].weight).item())\n",
    "\n",
    "# Print the norm of the first layer of the new lightweight model\n",
    "student_model_2 = VGGCifar5().cuda()\n",
    "print(\"Norm of 1st layer of student_model_2:\", torch.norm(student_model_2.backbone[0].weight).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be71ad1",
   "metadata": {
    "id": "2be71ad1"
   },
   "source": [
    "## 모델 파라미터 수 비교\n",
    "\n",
    "Teacher와 Student 모델 간의 **복잡도 차이**를 수치적으로 비교하기 위해 전체 파라미터 개수를 출력합니다. 이는 Knowledge Distillation의 핵심 가정인 \"*성능은 높지만 무거운 Teacher → 가볍고 빠른 Student로 지식 이전*\"을 정량적으로 뒷받침하는 자료가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905ce5f1",
   "metadata": {
    "id": "905ce5f1",
    "outputId": "872040fb-ceee-49da-d594-53a089b923b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teaher model parameters: 9,228,362\n",
      "Student model parameters: 964,170\n"
     ]
    }
   ],
   "source": [
    "total_params_teacher = \"{:,}\".format(sum(p.numel() for p in teacher_model.parameters()))\n",
    "print(f\"Teaher model parameters: {total_params_teacher}\")\n",
    "total_params_student = \"{:,}\".format(sum(p.numel() for p in student_model.parameters()))\n",
    "print(f\"Student model parameters: {total_params_student}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f73038",
   "metadata": {
    "id": "d0f73038"
   },
   "source": [
    "## Student 모델 단독 학습 (Cross-Entropy Only)\n",
    "\n",
    "본 단계에서는 Student 모델을 **Teacher의 도움 없이** 단독으로 학습시킵니다. 이 실험은 이후 Knowledge Distillation을 적용했을 때 얼마나 성능이 향상되는지를 비교하기 위한 **Baseline** 성능을 확보하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c79955",
   "metadata": {
    "id": "61c79955",
    "outputId": "0b26d76f-39dc-4b38-faf5-49ed5b0eb3b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.3603065112972503\n",
      "Epoch 2/5, Loss: 0.8317949570658262\n",
      "Epoch 3/5, Loss: 0.5934104708302052\n",
      "Epoch 4/5, Loss: 0.39868114389421994\n",
      "Epoch 5/5, Loss: 0.26499412363142616\n",
      "Test Accuracy: 82.44%\n"
     ]
    }
   ],
   "source": [
    "student_model = VGGCifar5().cuda()\n",
    "train(student_model, get_train_loader(train_dataset), epochs=5, learning_rate=0.01)\n",
    "test_accuracy_student_ce = test(student_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee805e",
   "metadata": {
    "id": "5aee805e"
   },
   "source": [
    "## 정확도 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cb5363",
   "metadata": {
    "id": "47cb5363",
    "outputId": "825c6670-b28a-4b17-95cc-a642504a51e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 92.95%\n",
      "Student accuracy without teacher: 82.44%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_teacher:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_student_ce:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1725be7",
   "metadata": {
    "id": "b1725be7"
   },
   "source": [
    "# 3.2. Knowledge Distillation (Soft Targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3100a75d",
   "metadata": {
    "id": "3100a75d"
   },
   "source": [
    "## [실습 1] Knowledge Distillation 학습 함수 정의\n",
    "\n",
    "아래 함수는 **Teacher 모델의 soft output**을 활용하여 Student 모델을 지도 학습하는 Knowledge Distillation (KD) 학습 루프입니다.  \n",
    "기존 Cross-Entropy 학습에 더해, soft target을 이용한 추가적인 loss를 도입하여 Student가 Teacher의 예측 구조까지 학습할 수 있도록 합니다.\n",
    "\n",
    "\n",
    "### KD 핵심 개념\n",
    "\n",
    "- **Soft Targets**: Teacher의 출력(logits)을 softmax로 부드럽게 만든 확률 분포\n",
    "- **Temperature (T)**: softmax 분포의 평탄함을 제어하며, 높을수록 클래스 간 정보가 더 많이 보존됨\n",
    "- **Loss 조합**:\n",
    "  - `CrossEntropyLoss`: Ground-truth label 기반 지도 손실\n",
    "  - `KL-like Loss`: Teacher의 soft target 분포와 Student 예측 분포 간 차이를 최소화하는 손실\n",
    "  - 두 손실을 weighted sum으로 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cda1b5e8",
   "metadata": {
    "id": "cda1b5e8"
   },
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher,\n",
    "                                 student,\n",
    "                                 train_loader,\n",
    "                                 epochs,\n",
    "                                 learning_rate,\n",
    "                                 T,  # temperature\n",
    "                                 soft_target_loss_weight,\n",
    "                                 ce_loss_weight):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ##################### YOUR CODE STARTS HERE #####################\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Soften the student logits by applying softmax\n",
    "            # Hint: nn.functional.softmax()\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            student_prob = nn.functional.softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - student_prob.log())) / student_prob.size(0) * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "            ##################### YOUR CODE ENDS HERE #######################\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834009c5",
   "metadata": {
    "id": "834009c5"
   },
   "source": [
    "## Knowledge Distillation 학습 수행\n",
    "\n",
    "앞서 정의한 `train_knowledge_distillation()` 함수를 이용하여, Teacher 모델의 soft prediction을 기반으로 Student 모델을 학습시킵니다. 이후, 테스트 정확도를 측정하여 **기존 CE-only 학습과의 성능 차이**를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd4addb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "fd4addb9",
    "outputId": "52d2505a-47cb-4578-f6ea-a62a689c9d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 7.115619836256022\n",
      "Epoch 2/5, Loss: 3.4852485614054647\n",
      "Epoch 3/5, Loss: 2.233771598857382\n",
      "Epoch 4/5, Loss: 1.5219025212480588\n",
      "Epoch 5/5, Loss: 1.1669407553989868\n",
      "Test Accuracy: 85.43%\n"
     ]
    }
   ],
   "source": [
    "student_model = VGGCifar5().cuda()\n",
    "train_knowledge_distillation(teacher=teacher_model,\n",
    "                             student=student_model,\n",
    "                             train_loader=get_train_loader(train_dataset),\n",
    "                             epochs=5,\n",
    "                             learning_rate=0.01,\n",
    "                             T=10,\n",
    "                             soft_target_loss_weight=0.5,\n",
    "                             ce_loss_weight=0.5)\n",
    "\n",
    "test_accuracy_student_ce_and_kd = test(student_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea07997",
   "metadata": {
    "id": "cea07997"
   },
   "source": [
    "## 정확도 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff51b0fd",
   "metadata": {
    "id": "ff51b0fd",
    "outputId": "867610ab-f4d9-4c22-fcd1-be3b259c0de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 92.95%\n",
      "Student accuracy without teacher: 82.44%\n",
      "Student accuracy with CE + KD: 85.43%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_teacher:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_student_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_student_ce_and_kd:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af957a",
   "metadata": {
    "id": "94af957a"
   },
   "source": [
    "# 3.3. Cosine Loss Minimization (Cosine Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b022162",
   "metadata": {
    "id": "5b022162"
   },
   "source": [
    "## Cosine Similarity 기반 KD 모델 정의\n",
    "\n",
    "본 실험에서는 **Teacher와 Student 모델의 내부 표현(hidden representation)** 을 정렬하여 학습 효과를 높이고자 합니다.  \n",
    "이를 위해 기존 VGG 구조를 변형하여 **flatten된 feature representation을 반환하는** 모델을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "783da0a7",
   "metadata": {
    "id": "783da0a7"
   },
   "outputs": [],
   "source": [
    "class VGGCifar9_Cosine(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, 64, 3, padding=1, bias=False)),\n",
    "            ('bn0', nn.BatchNorm2d(64)),\n",
    "            ('relu0', nn.ReLU(True)),\n",
    "            ('conv1', nn.Conv2d(64, 128, 3, padding=1, bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(128)),\n",
    "            ('relu1', nn.ReLU(True)),\n",
    "            ('pool0', nn.MaxPool2d(2)),\n",
    "            ('conv2', nn.Conv2d(128, 256, 3, padding=1, bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(256)),\n",
    "            ('relu2', nn.ReLU(True)),\n",
    "            ('conv3', nn.Conv2d(256, 256, 3, padding=1, bias=False)),\n",
    "            ('bn3', nn.BatchNorm2d(256)),\n",
    "            ('relu3', nn.ReLU(True)),\n",
    "            ('pool1', nn.MaxPool2d(2)),\n",
    "            ('conv4', nn.Conv2d(256, 512, 3, padding=1, bias=False)),\n",
    "            ('bn4', nn.BatchNorm2d(512)),\n",
    "            ('relu4', nn.ReLU(True)),\n",
    "            ('conv5', nn.Conv2d(512, 512, 3, padding=1, bias=False)),\n",
    "            ('bn5', nn.BatchNorm2d(512)),\n",
    "            ('relu5', nn.ReLU(True)),\n",
    "            ('pool2', nn.MaxPool2d(2)),\n",
    "            ('conv6', nn.Conv2d(512, 512, 3, padding=1, bias=False)),\n",
    "            ('bn6', nn.BatchNorm2d(512)),\n",
    "            ('relu6', nn.ReLU(True)),\n",
    "            ('conv7', nn.Conv2d(512, 512, 3, padding=1, bias=False)),\n",
    "            ('bn7', nn.BatchNorm2d(512)),\n",
    "            ('relu7', nn.ReLU(True)),\n",
    "            ('pool3', nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(x)\n",
    "        conv_output = torch.flatten(x, 1)\n",
    "        conv_output_after_pooling = torch.nn.functional.avg_pool1d(conv_output, 2)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x, conv_output_after_pooling\n",
    "\n",
    "class VGGCifar5_Cosine(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        # Generate the same scratch model\n",
    "        set_seed()\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, 64, 3, padding=1, bias=False)),\n",
    "            ('bn0', nn.BatchNorm2d(64)),\n",
    "            ('relu0', nn.ReLU(True)),\n",
    "            ('pool0', nn.MaxPool2d(2)),\n",
    "            ('conv1', nn.Conv2d(64, 128, 3, padding=1, bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(128)),\n",
    "            ('relu1', nn.ReLU(True)),\n",
    "            ('pool1', nn.MaxPool2d(2)),\n",
    "            ('conv2', nn.Conv2d(128, 256, 3, padding=1, bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(256)),\n",
    "            ('relu2', nn.ReLU(True)),\n",
    "            ('pool2', nn.MaxPool2d(2)),\n",
    "            ('conv3', nn.Conv2d(256, 256, 3, padding=1, bias=False)),\n",
    "            ('bn3', nn.BatchNorm2d(256)),\n",
    "            ('relu3', nn.ReLU(True)),\n",
    "            ('pool3', nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(x)\n",
    "        conv_output = torch.flatten(x, 1)\n",
    "        x = x.mean([2, 3])\n",
    "        # conv_output = x\n",
    "        x = self.classifier(x)\n",
    "        return x, conv_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fde4f",
   "metadata": {
    "id": "715fde4f"
   },
   "source": [
    "## Cosine Loss 기반 KD를 위한 모델 초기화\n",
    "\n",
    "앞서 정의한 **representation-level KD 구조**를 활용하여 Teacher와 Student 모델을 초기화합니다.  \n",
    "Teacher는 기존 `VGGCifar9`의 학습된 가중치를 그대로 활용하며, Student는 새로 초기화하여 학습을 수행할 준비를 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5d20922",
   "metadata": {
    "id": "f5d20922"
   },
   "outputs": [],
   "source": [
    "teacher_model_cosine = VGGCifar9_Cosine().cuda()\n",
    "teacher_model_cosine.load_state_dict(state_dict)\n",
    "\n",
    "student_model_cosine = VGGCifar5_Cosine().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86244997",
   "metadata": {
    "id": "86244997"
   },
   "source": [
    "## Cosine Distillation을 위한 Representation 차원 확인\n",
    "\n",
    "CosineEmbeddingLoss를 적용하기 위해서는 Teacher와 Student 모델이 반환하는 **hidden representation**이 동일한 차원을 가져야 합니다.  \n",
    "아래 코드는 임의의 입력(batch) 데이터를 각각의 모델에 통과시켜, 출력되는 **logits**과 **flatten된 convolutional feature vector**의 shape을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aee4deda",
   "metadata": {
    "id": "aee4deda",
    "outputId": "1fd13cfa-fde2-4d5a-8bf8-acc8ca6e4cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student logits shape: torch.Size([128, 10])\n",
      "Student hidden representation shape: torch.Size([128, 1024])\n",
      "Teacher logits shape: torch.Size([128, 10])\n",
      "Teacher hidden representation shape: torch.Size([128, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample input tensor\n",
    "sample_input = torch.randn(128, 3, 32, 32).cuda() # Batch size: 128, Filters: 3, Image size: 32x32\n",
    "\n",
    "# Pass the input through the student\n",
    "logits, hidden_representation = student_model_cosine(sample_input)\n",
    "\n",
    "# Print the shapes of the tensors\n",
    "print(\"Student logits shape:\", logits.shape) # batch_size x total_classes\n",
    "print(\"Student hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size\n",
    "\n",
    "# Pass the input through the teacher\n",
    "logits, hidden_representation = teacher_model_cosine(sample_input)\n",
    "\n",
    "# Print the shapes of the tensors\n",
    "print(\"Teacher logits shape:\", logits.shape) # batch_size x total_classes\n",
    "print(\"Teacher hidden representation shape:\", hidden_representation.shape) # batch_size x hidden_representation_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7eba19",
   "metadata": {
    "id": "1e7eba19"
   },
   "source": [
    "## [실습 2] Cosine Similarity 기반 KD 학습 함수 정의\n",
    "\n",
    "이 함수는 **Teacher와 Student의 내부 표현(hidden representation)** 간 유사도를 **CosineEmbeddingLoss**를 통해 극대화하는 방식으로 Student를 학습시킵니다.  \n",
    "이는 soft-label 기반 KD와 달리 **representation-level distillation**으로 분류되며, Student의 feature extractor 품질 향상에 초점을 둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8dd2085",
   "metadata": {
    "id": "c8dd2085"
   },
   "outputs": [],
   "source": [
    "def train_cosine_loss(teacher,\n",
    "                      student,\n",
    "                      train_loader,\n",
    "                      epochs,\n",
    "                      learning_rate,\n",
    "                      hidden_rep_loss_weight,\n",
    "                      ce_loss_weight):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    cosine_loss = nn.CosineEmbeddingLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ##################### YOUR CODE STARTS HERE #####################\n",
    "            # Forward pass with the teacher model and keep only the hidden representation\n",
    "            with torch.no_grad():\n",
    "                _, teacher_hidden_representation = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits, student_hidden_representation = student(inputs)\n",
    "\n",
    "            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is\n",
    "            # the case where loss minimization leads to cosine similarity increase.\n",
    "            # Hint: cosine_loss(x, y, target)에서 target은 1로 이루어진 vector이며, torch.ones(inputs.size(0)).cuda())를 사용\n",
    "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation,\n",
    "                                          target=torch.ones(inputs.size(0)).cuda())\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "            ##################### YOUR CODE ENDS HERE #######################\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b546de1",
   "metadata": {
    "id": "9b546de1"
   },
   "source": [
    "## Cosine Similarity 기반 Knowledge Distillation 실험\n",
    "\n",
    "이 실험에서는 Teacher와 Student의 **internal feature vector** 간 유사도를 기반으로 하는 **CosineEmbeddingLoss**를 적용하여 Student 모델을 학습시킵니다.  \n",
    "이 방법은 soft label이 아닌, feature-level에서의 표현력 유사성을 유도하는 방식으로 distillation을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "083ef6f2",
   "metadata": {
    "id": "083ef6f2",
    "outputId": "357b1a71-a4e4-464c-b76b-8476a220a9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8624538840235346\n",
      "Epoch 2/5, Loss: 0.5542182272200085\n",
      "Epoch 3/5, Loss: 0.4163331224028107\n",
      "Epoch 4/5, Loss: 0.31045055736208815\n",
      "Epoch 5/5, Loss: 0.2399151202129281\n",
      "Test Accuracy: 82.91%\n"
     ]
    }
   ],
   "source": [
    "# Train and test the lightweight network with cross entropy loss\n",
    "student_model_cosine = VGGCifar5_Cosine().cuda()\n",
    "train_cosine_loss(teacher=teacher_model_cosine,\n",
    "                  student=student_model_cosine,\n",
    "                  train_loader=get_train_loader(train_dataset),\n",
    "                  epochs=5,\n",
    "                  learning_rate=0.01,\n",
    "                  hidden_rep_loss_weight=0.5,\n",
    "                  ce_loss_weight=0.5)\n",
    "\n",
    "test_accuracy_student_ce_and_cosine_loss = test(student_model_cosine, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1525c",
   "metadata": {
    "id": "8ca1525c"
   },
   "source": [
    "## 정확도 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98dea22d",
   "metadata": {
    "id": "98dea22d",
    "outputId": "b942ccb4-4eee-43ce-a018-b6ed861fa523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 92.95%\n",
      "Student accuracy without teacher: 82.44%\n",
      "Student accuracy with CE + KD: 85.43%\n",
      "Student accuracy with CE + CosineLoss: 82.91%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_teacher:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_student_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_student_ce_and_kd:.2f}%\")\n",
    "print(f\"Student accuracy with CE + CosineLoss: {test_accuracy_student_ce_and_cosine_loss:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c793a66c",
   "metadata": {
    "id": "c793a66c"
   },
   "source": [
    "# 3.4. Intermediate Regressor (Regressor + MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ea2be",
   "metadata": {
    "id": "703ea2be"
   },
   "source": [
    "## Feature Map Shape 비교\n",
    "\n",
    "Hint-based Knowledge Distillation에서는 Teacher와 Student의 **중간 feature map**을 정렬(MSE 등)하기 위해,  \n",
    "서로의 **convolutional output shape**을 일치시키거나 맞춰주는 작업이 필요합니다.  \n",
    "아래 코드는 이 과정을 준비하기 위해 각 모델의 **backbone 출력 형태**를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41fb1d5a",
   "metadata": {
    "id": "41fb1d5a",
    "outputId": "0a5d29a4-dc38-4ddf-fcc9-494b4508d1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's feature extractor output shape:  torch.Size([128, 256, 2, 2])\n",
      "Teacher's feature extractor output shape:  torch.Size([128, 512, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Pass the sample input only from the convolutional feature extractor\n",
    "convolutional_fe_output_student = student_model.backbone(sample_input)\n",
    "convolutional_fe_output_teacher = teacher_model.backbone(sample_input)\n",
    "\n",
    "# Print their shapes\n",
    "print(\"Student's feature extractor output shape: \", convolutional_fe_output_student.shape)\n",
    "print(\"Teacher's feature extractor output shape: \", convolutional_fe_output_teacher.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f1ad4",
   "metadata": {
    "id": "6f0f1ad4"
   },
   "source": [
    "## Hint-based KD를 위한 Regressor 포함 모델 정의\n",
    "\n",
    "이 실험에서는 Teacher의 중간 feature map과 Student의 feature map을 직접 정렬하기 위해 **trainable regressor**를 도입합니다.  \n",
    "Teacher와 Student의 convolution output은 채널 수가 다르기 때문에, Student의 feature map을 **regressor**를 통해 변환하여 동일한 차원으로 맞춥니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8693e26a",
   "metadata": {
    "id": "8693e26a"
   },
   "outputs": [],
   "source": [
    "class VGGCifar9_Regressor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, 64, 3, padding=1, bias=False)),\n",
    "            ('bn0', nn.BatchNorm2d(64)),\n",
    "            ('relu0', nn.ReLU(True)),\n",
    "            ('conv1', nn.Conv2d(64, 128, 3, padding=1, bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(128)),\n",
    "            ('relu1', nn.ReLU(True)),\n",
    "            ('pool0', nn.MaxPool2d(2)),\n",
    "            ('conv2', nn.Conv2d(128, 256, 3, padding=1, bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(256)),\n",
    "            ('relu2', nn.ReLU(True)),\n",
    "            ('conv3', nn.Conv2d(256, 256, 3, padding=1, bias=False)),\n",
    "            ('bn3', nn.BatchNorm2d(256)),\n",
    "            ('relu3', nn.ReLU(True)),\n",
    "            ('pool1', nn.MaxPool2d(2)),\n",
    "            ('conv4', nn.Conv2d(256, 512, 3, padding=1, bias=False)),\n",
    "            ('bn4', nn.BatchNorm2d(512)),\n",
    "            ('relu4', nn.ReLU(True)),\n",
    "            ('conv5', nn.Conv2d(512, 512, 3, padding=1, bias=False)),\n",
    "            ('bn5', nn.BatchNorm2d(512)),\n",
    "            ('relu5', nn.ReLU(True)),\n",
    "            ('pool2', nn.MaxPool2d(2)),\n",
    "            ('conv6', nn.Conv2d(512, 512, 3, padding=1, bias=False)),\n",
    "            ('bn6', nn.BatchNorm2d(512)),\n",
    "            ('relu6', nn.ReLU(True)),\n",
    "            ('conv7', nn.Conv2d(512, 512, 3, padding=1, bias=False)),\n",
    "            ('bn7', nn.BatchNorm2d(512)),\n",
    "            ('relu7', nn.ReLU(True)),\n",
    "            ('pool3', nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(x)\n",
    "        conv_feature_map = x\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x, conv_feature_map\n",
    "\n",
    "class VGGCifar5_Regressor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        # Generate the same scratch model\n",
    "        set_seed()\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(3, 64, 3, padding=1, bias=False)),\n",
    "            ('bn0', nn.BatchNorm2d(64)),\n",
    "            ('relu0', nn.ReLU(True)),\n",
    "            ('pool0', nn.MaxPool2d(2)),\n",
    "            ('conv1', nn.Conv2d(64, 128, 3, padding=1, bias=False)),\n",
    "            ('bn1', nn.BatchNorm2d(128)),\n",
    "            ('relu1', nn.ReLU(True)),\n",
    "            ('pool1', nn.MaxPool2d(2)),\n",
    "            ('conv2', nn.Conv2d(128, 256, 3, padding=1, bias=False)),\n",
    "            ('bn2', nn.BatchNorm2d(256)),\n",
    "            ('relu2', nn.ReLU(True)),\n",
    "            ('pool2', nn.MaxPool2d(2)),\n",
    "            ('conv3', nn.Conv2d(256, 256, 3, padding=1, bias=False)),\n",
    "            ('bn3', nn.BatchNorm2d(256)),\n",
    "            ('relu3', nn.ReLU(True)),\n",
    "            ('pool3', nn.MaxPool2d(2)),\n",
    "        ]))\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(x)\n",
    "        regressor_output = self.regressor(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        return x, regressor_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc206cec",
   "metadata": {
    "id": "cc206cec"
   },
   "source": [
    "## Hint-based KD용 Teacher 모델 초기화 및 가중치 로딩\n",
    "\n",
    "Hint-based Knowledge Distillation에서는 Teacher 모델의 중간 feature map을 **지도 정보로 활용**합니다.  \n",
    "이를 위해 기존 학습된 `VGGCifar9`의 가중치를 기반으로 하되, **feature map 반환이 가능한 구조**로 변경한 `VGGCifar9_Regressor` 모델을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5796502d",
   "metadata": {
    "id": "5796502d",
    "outputId": "67aa9998-af67-4950-eb1f-b7e3769f1cf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model_reg = VGGCifar9_Regressor().cuda()\n",
    "teacher_model_reg.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ae213",
   "metadata": {
    "id": "d85ae213"
   },
   "source": [
    "## [실습 3] Hint-based Knowledge Distillation 학습 함수 정의 (MSE Loss 기반)\n",
    "\n",
    "이 함수는 **중간 feature map**을 기준으로 Teacher와 Student의 표현을 정렬하기 위해 **Mean Squared Error (MSE) Loss**를 사용하는 Hint-based KD 학습 방식입니다.  \n",
    "이를 통해 Student의 feature extractor가 Teacher의 중간 표현력을 모방하도록 유도합니다.\n",
    "\n",
    "\n",
    "**학습 개념 요약**\n",
    "\n",
    "| 손실 종류      | 역할                                       |\n",
    "|----------------|--------------------------------------------|\n",
    "| CrossEntropy   | 정답 라벨 기반 분류 학습                   |\n",
    "| MSE Loss       | Teacher feature map ↔ Student regressed feature map 간 오차 최소화 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23a4be06",
   "metadata": {
    "id": "23a4be06"
   },
   "outputs": [],
   "source": [
    "def train_mse_loss(teacher,\n",
    "                   student,\n",
    "                   train_loader,\n",
    "                   epochs,\n",
    "                   learning_rate,\n",
    "                   feature_map_weight,\n",
    "                   ce_loss_weight):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ##################### YOUR CODE STARTS HERE #####################\n",
    "            # Again ignore teacher logits\n",
    "            with torch.no_grad():\n",
    "                _, teacher_feature_map = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits, regressor_feature_map = student(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "            ##################### YOUR CODE ENDS HERE #######################\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5549441",
   "metadata": {
    "id": "c5549441"
   },
   "source": [
    "## Hint-based KD (Regressor + MSE Loss) 실험\n",
    "\n",
    "이 실험에서는 Student가 Teacher의 **중간 feature map**을 직접 모방하도록 유도하는 **Hint-based Knowledge Distillation**을 수행합니다.  \n",
    "이를 위해 Student에 **trainable regressor layer**를 도입하고, Teacher의 표현을 **MSE Loss**로 정렬합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef6a3172",
   "metadata": {
    "id": "ef6a3172",
    "outputId": "f5d0218b-9d31-401a-bee0-7a7f19d60290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.7331331200764307\n",
      "Epoch 2/5, Loss: 0.4394583729526881\n",
      "Epoch 3/5, Loss: 0.3140947205559982\n",
      "Epoch 4/5, Loss: 0.20931232139430084\n",
      "Epoch 5/5, Loss: 0.13904573413950708\n",
      "Test Accuracy: 83.11%\n"
     ]
    }
   ],
   "source": [
    "student_model_reg = VGGCifar5_Regressor().cuda()\n",
    "train_mse_loss(teacher=teacher_model_reg,\n",
    "               student=student_model_reg,\n",
    "               train_loader=get_train_loader(train_dataset),\n",
    "               epochs=5,\n",
    "               learning_rate=0.01,\n",
    "               feature_map_weight=0.5,\n",
    "               ce_loss_weight=0.5)\n",
    "\n",
    "test_accuracy_student_ce_and_mse_loss = test(student_model_reg, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab32e46",
   "metadata": {
    "id": "bab32e46"
   },
   "source": [
    "## 정확도 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d84db42d",
   "metadata": {
    "id": "d84db42d",
    "outputId": "0e1f07a6-1d08-4741-fb52-f7eb9c4637bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 92.95%\n",
      "Student accuracy without teacher: 82.44%\n",
      "Student accuracy with CE + KD: 85.43%\n",
      "Student accuracy with CE + CosineLoss: 82.91%\n",
      "Student accuracy with CE + RegressorMSE: 83.11%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_teacher:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_student_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_student_ce_and_kd:.2f}%\")\n",
    "print(f\"Student accuracy with CE + CosineLoss: {test_accuracy_student_ce_and_cosine_loss:.2f}%\")\n",
    "print(f\"Student accuracy with CE + RegressorMSE: {test_accuracy_student_ce_and_mse_loss:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
