{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060461b-a632-4818-8624-cf2fda5a53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LSTM \"\"\"\n",
    "input_size = X_train.shape[-1]\n",
    "num_layers = 2\n",
    "hidden_size = 64\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # (lstm): LSTM(1, 64, num_layers=2, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        # (linear): Linear(in_features=64, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.linear(out)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6429de4-94b9-4a8f-9521-c2fff4c24583",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    # llm의 모델 결과와 약간 비슷한데 다르다.\n",
    "    차이점 잘 구분할 것.\n",
    "\"\"\"\n",
    "\n",
    "def train(model, train_loader, test_loader):\n",
    "    train_hist = []\n",
    "    test_hist = []\n",
    "    num_epochs = 10\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        pred = model(batch_x)[:, -1, 0]\n",
    "        # llm은 [:, -1, :] 인데 ts는 [:, -1, 0]이다. 헷갈리지 말자.\n",
    "        loss = loss_fn(pred, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_train_loss / len(train_loader)\n",
    "    train_hist.append(avg_loss)\n",
    "\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_x, test_y in test_loader:\n",
    "            test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "            test_pred = model(test_x)[:, -1, 0]\n",
    "            test_loss = loss_fn(test_pred, test_y)\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    test_hist.append(avg_test_loss)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0491159-2e1f-42da-a9a7-ad24f9c65764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecasting(model, X_test, y_test):\n",
    "  model.eval()\n",
    "  num_forecast_steps = 30\n",
    "  input_data = X_test[-num_forecast_steps].cpu().numpy().squeeze()\n",
    "\n",
    "  forecasted_values = []\n",
    "  with torch.no_grad():\n",
    "    for i in range(2 * num_forecast_steps):\n",
    "      input_tensor = torch.as_tensor(input_data).view(1, -1, 1).to(device)\n",
    "      predicted = model(input_tensor)[0, -1, 0].item()\n",
    "\n",
    "      forecasted_values.append(predicted)\n",
    "      input_data = np.roll(input_data, shift=-1)\n",
    "\n",
    "      if i < num_forecast_steps:\n",
    "        input_data[-1] = y_test[-num_forecast_steps + i]\n",
    "      else:\n",
    "        input_data[-1] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73443987-69a2-4405-bc16-7e68ffeba1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DModel(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    super(Conv1DModel, self).__init__()\n",
    "    self.conv1d = nn.Conv1d(in_channels=input_size, \\\n",
    "                            out_channels=hidden_size, \\\n",
    "                            kernel_size=2, stride=1)\n",
    "    self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.transpose(1, 2)\n",
    "        # Conv1d는 (batch, channels, length) 형태를 기대한다.\n",
    "        # (batch, seq_len, input_size) → (batch, input_size, seq_len)로 바꿔줌.\n",
    "    x = self.conv1d(x) # 결과 shape: (batch, hidden_size, seq_len-1)\n",
    "    x = x.transpose(1, 2)\n",
    "        # 다시 (batch, seq_len-1, hidden_size)로 바꿔서 Linear에 넣기 좋게 만든다.\n",
    "    return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30667356-e5eb-4654-87bd-bf90d9a03f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    RNN도 한번 봐두지만 input_size가 X_train.shape[-1]이 되는게 다른 분야와 차이점인듯\n",
    "\"\"\"\n",
    "class RNNModel(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers):\n",
    "      super(RNNModel, self).__init__()\n",
    "      self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "      self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "      out, _ = self.rnn(x)\n",
    "      return self.fc(out)\n",
    "\n",
    "input_size = X_train.shape[-1]\n",
    "num_layers = 2\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab233c4-a6dc-4fe6-852d-3bddcc99c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enc_dec_sequences(data):\n",
    "    features, labels = [], []\n",
    "    for i in range(len(data) - sequence_length - target_len):\n",
    "        features.append(data[i:i + sequence_length])\n",
    "        labels.append(data[i + sequence_length : i + sequence_length + target_len])\n",
    "        # feature 다음것부터 target_len개만큼 가져온다.\n",
    "\n",
    "    features = np.array(features, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.float32)\n",
    "    # noarray로 한번 바꾸고 \n",
    "    # 꼭 2단계로 할 필요는 없는데 타입을 명확히 float32로 맞추고, shape을 안정적으로 정리하기 위해\n",
    "\n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    #tensor로 바꿔준다.\n",
    "    return features, labels\n",
    "\n",
    "X_train_, y_train_ = create_enc_dec_sequences(train_scaled)\n",
    "X_test_, y_test_ = create_enc_dec_sequences(test_scaled)\n",
    "\n",
    "train_loader_ = to_loader(X_train_, y_train_, batch_size, shuffle=True)\n",
    "test_loader_ = to_loader(X_test_, y_test_, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857fa4f-181b-4c88-9e2c-7ff59d5590d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers):\n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    _, h = self.rnn(x)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef4253-75f6-49e8-beea-0a8a4d32edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers):\n",
    "    super(DecoderRNN, self).__init__()\n",
    "    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "  def forward(self, x, h):\n",
    "    out, h = self.rnn(x, h)\n",
    "    out = self.fc(out)\n",
    "    return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841fbc13-96e8-4a8b-ad0f-75b6b6671022",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers):\n",
    "    super(RNNRNN, self).__init__()\n",
    "    self.encoder = EncoderRNN(input_size, hidden_size, num_layers)\n",
    "    self.decoder = DecoderRNN(input_size, hidden_size, num_layers)\n",
    "\n",
    "  def forward(self, source, target_len): # source shape:(batch, seq_len, input_size)\n",
    "    h = self.encoder(source) # h shape: (num_layers, batch, hidden_size)\n",
    "    predictions = []\n",
    "    input = source[:, -1, :].unsqueeze(1) # shape: (batch, input_size) > (batch, 1, input_size)\n",
    "    for t in range(target_len):\n",
    "        out, h = self.decoder(input, h) # out: (batch, 1, input_size) / h: (num_layers, batch, hidden_size)\n",
    "        predictions.append(out.squeeze(1)) # out : (batch, 1, input_size) → (batch, input_size)\n",
    "        input = out # (batch, input_size)\n",
    "    outputs = torch.stack(predictions, dim=1) # (batch, target_len, input_size)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8aeece-ba55-4227-84c3-6fafb0772f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_(model, train_loader, test_loader):\n",
    "    train_hist = []\n",
    "    test_hist = []\n",
    "    num_epochs = 10\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_test_loss = 0.0\n",
    "\n",
    "        # train\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            pred = model(batch_x, batch_y.shape[1]) # batch_y.shape[1] = target_len\n",
    "            loss = loss_fn(pred, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_train_loss / len(train_loader)\n",
    "        train_hist.append(avg_loss)\n",
    "\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for test_x, test_y in test_loader:\n",
    "            test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "            test_pred = model(test_x, target_len) # tesy_y.shape[1]로 해야 하는거 아닌지?\n",
    "            test_loss = loss_fn(test_pred, test_y)\n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "        test_hist.append(avg_test_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1:2d}/{num_epochs} - Training Loss: {avg_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "train_(model_, train_loader_, test_loader_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be0b92-f62f-47a7-b775-c8fad082ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecasting(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    num_forecast_steps = 30\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, min(num_forecast_steps, len(X_test)) + 1):\n",
    "            input_tensor = X_test[-i].unsqueeze(0).to(device)\n",
    "            predicted = model(input_tensor, target_len) # (batch, target_len, input_size)\n",
    "            forecasted_values = predicted.squeeze(0).cpu().numpy() # : (target_len, input_size)\n",
    "            forecasted_values = scaler.inverse_transform( # 학습 전에 MinMaxScaler나 StandardScaler로 정규화했던 값을 원래 스케일로 \n",
    "                forecasted_values.reshape(-1, 1) # (target_len, input_size) → (target_len * input_size, 1)\n",
    "            ).flatten()\n",
    "            idx = test_data.index[-i - target_len + 1 : -i + 1 if i > 1 else None]\n",
    "            plt.plot(idx, forecasted_values, color=\"red\", alpha=0.6,\n",
    "                     label=\"forecasted values\" if i == 1 else \"\")\n",
    "\n",
    "    plt.plot(test_data.index[-100:-39], test_data[-100:-39],\n",
    "             label=\"test_data\", color=\"b\")\n",
    "    plt.plot(test_data.index[-39:], test_data.iloc[-39:],\n",
    "             label=\"actual values\", color=\"green\")\n",
    "\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.title('Time Series Forecasting')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_forecasting(model_, X_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f67e8-d715-4ce5-83ad-c85e7641f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test = X_test.to(device)\n",
    "        y_hat = model(X_test, target_len)\n",
    "        test_predictions = y_hat[:, :, 0]\n",
    "\n",
    "    test_predictions = test_predictions.cpu().numpy()\n",
    "    y_test = y_test.cpu().numpy().reshape(-1, target_len)\n",
    "\n",
    "    rmse = root_mean_squared_error(y_test, test_predictions)\n",
    "    mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "\n",
    "    return rmse, mape\n",
    "\n",
    "rmse, mape = test_(model_, X_test_, y_test_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
