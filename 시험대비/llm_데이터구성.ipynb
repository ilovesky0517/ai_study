{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3e1eb-1950-4ee9-a2a2-52133699580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "데이터 구성 원리 이해\n",
    "    -모델과학습또는평가의도에맞게데이터를구성하는방법을이해하고있나\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964802c1-7ead-42f8-a5ec-cb66914b49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 요약 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9156aa0-a85f-4a61-9fa7-d7cfc49475d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# LLM은 input x와 label y, x를 통해 그 다음에 올 토큰을 추론하는거라 1씩 shift 된다.\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c0cfb-ed8c-4e38-90f0-80a3133635ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(token_ids) - max_length, stride):\n",
    "    # 입력 청크: 현재 위치(i)부터 max_length만큼 가져온다. 인덱스 잘 볼것.\n",
    "    input_chunk = token_ids[i : i + max_length]\n",
    "                \n",
    "    # 타겟 청크: 입력보다 1칸 뒤의 위치(i+1)부터 가져온다. llm이니까\n",
    "    # GPT는 '다음 단어'를 맞추는 모델이므로, 정답은 입력보다 한 칸씩 뒤로 밀려있어야 한다.\n",
    "    target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "    \n",
    "    self.input_ids.append(torch.tensor(input_chunk))\n",
    "    self.target_ids.append(torch.tensor(target_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eeafb7-0068-4c50-a1de-a205200d2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    \"\"\"\n",
    "    데이터셋을 학습(Train), 검증(Validation), 테스트(Test) 셋으로 분할하는 함수\n",
    "    \"\"\"\n",
    "    # 전체 데이터 섞기\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # 분할 지점(인덱스) 계산\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # 데이터 분할\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e043b13-1278-46a1-a60c-37d363779107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 모델이 지원하는 최대 길이(context_size)를 넘지 않도록 자름\n",
    "        idx_cond = idx[:, -context_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b57a3-3810-45aa-a2c2-ac83d4fa24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn( batch,\n",
    "    pad_token_id=50256, ignore_index=-100, allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    데이터 로더에서 배치를 만들 때 사용하는 커스텀 함수.\n",
    "    가변 길이의 시퀀스를 배치의 최대 길이에 맞춰 패딩(padding)하고, \n",
    "    정답(target) 데이터에서 패딩 부분은 손실(loss) 계산에서 제외하도록 처리합니다.\n",
    "    \"\"\"\n",
    "    # 배치 내에서 가장 긴 시퀀스 길이 계산 (패딩을 위해 +1 여유)\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    \n",
    "    # 입력(inputs)과 정답(targets) 리스트 준비\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    \n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # 문장 끝 토큰 <|endoftext|> 추가\n",
    "        new_item += [pad_token_id]\n",
    "        \n",
    "        # 가장 긴 길이에 맞춰 패딩 추가\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        \n",
    "        # 입력은 마지막 토큰 제외 (0 ~ n-1)\n",
    "        inputs = torch.tensor(padded[:-1]) \n",
    "        # 정답은 첫 번째 토큰 제외 (1 ~ n) -> 다음 토큰 예측 문제이므로\n",
    "        targets = torch.tensor(padded[1:]) \n",
    "\n",
    "        # 중요: 패딩 부분 마스킹 처리\n",
    "        # 타겟에서 패딩 토큰인 부분은 ignore_index(-100)로 바꿔서 CrossEntropyLoss 계산 시 무시되게 함\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        \n",
    "        # 패딩이 시작되는 지점 이후의 모든 타겟 값을 -100으로 설정\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        # indices[1:] → 첫 번째 pad 위치를 제외한 나머지 pad 위치 인덱스 \n",
    "        # 위치의 targets 값을 ignore_index로 바꿉니다.\n",
    "        \n",
    "        # 선택적으로 최대 시퀀스 길이 제한 (메모리 관리 등 목적)\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    \n",
    "    # 텐서로 변환 및 디바이스(GPU/CPU)로 이동\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
