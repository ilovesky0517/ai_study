{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f47be-e309-4abb-af12-f2cd93e94bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    모델의원리이해 :\n",
    "        모델의주요layer의의미, 구성방법, 동작원리, 활용방법에대해이해를하고있나\n",
    "        모델의 결과와 정답을 이용하여 학습으로 연결시킬 수 있나\n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33ebad-5cdb-4008-b9ea-ca1cfe7e34a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"Vision Transformer (ViT)\n",
    "    - 이미지를 패치로 쪼개 토큰 시퀀스를 만들고\n",
    "    - CLS 토큰 + 위치 임베딩을 더한 뒤\n",
    "    - Transformer Encoder로 전역(Self-Attention) 관계를 학습하여\n",
    "    - CLS(또는 mean pool) 표현으로 분류합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: ViTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) 입력 해상도/패치 크기를 (H,W) 튜플로 정규화\n",
    "        image_height, image_width = pair(cfg.image_size)\n",
    "        patch_height, patch_width = pair(cfg.patch_size)\n",
    "\n",
    "        # 2) 패치가 이미지에 딱 나누어 떨어져야 (h, w) 그리드가 정확히 형성됨\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        # 3) 패치 개수(=토큰 개수)와 한 패치의 펼친 차원 계산\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = cfg.channels * patch_height * patch_width\n",
    "\n",
    "        # 4) 풀링 방식 검증: CLS 토큰을 쓸지 mean pool을 쓸지 선택\n",
    "        assert cfg.pool in {\"cls\", \"mean\"}, \"pool must be 'cls' or 'mean'\"\n",
    "\n",
    "        # 5) 패치 토큰화: (B,C,H,W) -> (B, N, patch_dim) -> (B, N, dim)\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            # 패치 그리드로 자른 뒤, 각 패치를 1D 벡터로 펼쳐 토큰 시퀀스를 만듦\n",
    "            Rearrange(\n",
    "                \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n",
    "                p1=patch_height,\n",
    "                p2=patch_width,\n",
    "            ),\n",
    "            # 펼친 패치 벡터를 Transformer hidden size(dim)로 선형 투영\n",
    "            nn.Linear(patch_dim, cfg.dim),\n",
    "        )\n",
    "\n",
    "        # 6) CLS 토큰 + 위치 임베딩(학습 파라미터)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, cfg.dim))\n",
    "        # LLM GPT의 token embedding 대신 cls_token으로 분류하는 방식인듯?\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, cfg.dim))\n",
    "        self.dropout = nn.Dropout(cfg.emb_dropout)\n",
    "\n",
    "        # 7) Transformer Encoder 스택(Attention + FFN + Residual/PreNorm)\n",
    "        self.transformer = Transformer(\n",
    "            dim=cfg.dim,\n",
    "            depth=cfg.depth,\n",
    "            heads=cfg.heads,\n",
    "            dim_head=cfg.dim_head,\n",
    "            mlp_dim=cfg.mlp_dim,\n",
    "            dropout=cfg.dropout,\n",
    "        )\n",
    "\n",
    "        self.pool = cfg.pool\n",
    "\n",
    "        # 8) 분류 헤드: (CLS/mean) 표현 -> LayerNorm -> Linear(logits)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(cfg.dim),\n",
    "            nn.Linear(cfg.dim, cfg.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # A) 패치 임베딩으로 토큰 시퀀스 생성: (B,C,H,W) -> (B,N,dim)\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # B) CLS 토큰을 배치만큼 복제해 시퀀스 맨 앞에 붙임: (B,1,dim)\n",
    "        cls_tokens = repeat(self.cls_token, \"1 1 d -> b 1 d\", b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, dim)\n",
    "\n",
    "        # C) 위치 임베딩을 더해 토큰 순서(공간 위치) 정보를 주입\n",
    "        x = x + self.pos_embedding[:, : (n + 1)]   # (B, N+1, dim) + (1, N+1, dim) -> Broadcasting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # D) Encoder를 통과하며 전역 의존성(Self-Attention) 학습\n",
    "        x = self.transformer(x)  # (B, N+1, dim)\n",
    "\n",
    "        # E) 이미지 표현 벡터 선택: CLS 토큰(0번) 또는 mean pooling\n",
    "        x = x[:, 0] if self.pool == \"cls\" else x.mean(dim=1)   # (B, dim)\n",
    "\n",
    "        # F) 최종 logits 출력 (softmax는 CrossEntropyLoss 내부에서 처리)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6971d-50b0-4bfe-9e67-9cbfff48ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) U-Net 모델 정의\n",
    "# =========================\n",
    "# 이 섹션의 목표:\n",
    "# - U-Net의 \"인코더(Down) → 보틀넥 → 디코더(Up) + Skip Connection\" 구조를\n",
    "#   PyTorch 코드로 직접 따라가며 이해합니다.\n",
    "#\n",
    "# 핵심 아이디어(한 줄):\n",
    "# - Down에서 공간 해상도(H,W)는 줄이고 채널(C)은 늘리면서 특징을 추출하고,\n",
    "#   Up에서 해상도를 복원하면서 Down 단계의 특징맵을 Skip으로 concat하여\n",
    "#   localization(위치 정보)을 되살립니다.\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"U-Net 기본 블록: (Conv → ReLU) × 2\n",
    "\n",
    "    - 첫 번째 Conv가 채널을 '중간 채널(mid_channels)'로 바꾸고,\n",
    "      두 번째 Conv가 '출력 채널(out_channels)'로 맞춥니다.\n",
    "    - 기본값(mid_channels=None)일 때는 mid_channels=out_channels로 두어,\n",
    "      (C_in → C_out → C_out) 형태가 됩니다.\n",
    "\n",
    "    입력/출력 텐서 형태:\n",
    "      - 입력:  (B, C_in, H, W)\n",
    "      - 출력:  (B, C_out, H, W)  # padding=1 이라 H,W 유지\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, mid_channels: int | None = None):\n",
    "        super().__init__()\n",
    "\n",
    "        # U-Net에서 업샘플 후 concat을 하면 채널이 2배가 되므로,\n",
    "        # bilinear 업샘플링을 쓸 때는 mid_channels=in_channels//2 처럼\n",
    "        # '중간 채널'을 줄여주는 방식이 흔히 사용됩니다.\n",
    "        if mid_channels is None:\n",
    "            mid_channels = out_channels\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # 1) (C_in → C_mid)\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # 2) (C_mid → C_out)\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc441dd0-8749-40b6-b008-bded86673e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    \"\"\"Downsampling 블록: MaxPool(2)로 해상도 1/2 → DoubleConv\n",
    "\n",
    "    입력/출력 텐서 형태:\n",
    "      - 입력:  (B, C_in,  H,  W)\n",
    "      - 출력:  (B, C_out, H/2, W/2)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 1) 해상도 축소\n",
    "        x = self.pool(x)           # (B, C_in, H/2, W/2)\n",
    "        # 2) 채널 확장 + 특징 추출\n",
    "        x = self.conv(x)           # (B, C_out, H/2, W/2)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b18375-e35b-48d7-b771-65b5abecfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upsampling 블록: 업샘플링 → (Skip concat) → DoubleConv\n",
    "\n",
    "    구현 관점(중요):\n",
    "    - in_channels 는 concat 이후 채널 수를 의미합니다.\n",
    "      예) x1(디코더) 채널=512, x2(skip) 채널=512 → concat 채널=1024 → in_channels=1024\n",
    "\n",
    "    - bilinear=True:\n",
    "        1) 업샘플은 파라미터 없는 bilinear interpolation으로 수행\n",
    "        2) concat 후 DoubleConv에서 mid_channels를 in_channels//2 로 두어\n",
    "           채널을 자연스럽게 '절반'으로 줄이는 방식(원 논문/레퍼런스 구현과 동일 계열)\n",
    "\n",
    "    - bilinear=False:\n",
    "        ConvTranspose2d로 업샘플 자체를 학습(파라미터 증가)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, bilinear: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            # (B, C, H/2, W/2) → (B, C, H, W)\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "            # bilinear일 때는 업샘플 후 채널 수(C)가 그대로 유지됩니다.\n",
    "            # concat 결과(in_channels)를 DoubleConv로 처리하되,\n",
    "            # 첫 conv의 출력(mid_channels)을 in_channels//2로 두어 채널을 줄입니다.\n",
    "            self.conv = DoubleConv(in_channels, out_channels, mid_channels=in_channels // 2)\n",
    "        else:\n",
    "            # (B, C, H/2, W/2) → (B, C/2, H, W)  (deconv가 채널도 절반으로 줄여줌)\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        # x1: (B, C_dec, H/2, W/2)  / x2: (B, C_skip, H, W)\n",
    "\n",
    "        # 1) 업샘플링: 해상도를 skip과 맞추기\n",
    "        x1 = self.up(x1)  # bilinear: 채널 유지 / deconv: 채널이 절반으로 감소\n",
    "\n",
    "        # 2) (필요 시) 패딩으로 크기 정렬\n",
    "        #    - 홀수 크기 입력 등으로 인해 x1과 x2의 H/W가 1~2 픽셀 정도 다를 수 있습니다.\n",
    "        diff_y = x2.size(2) - x1.size(2)\n",
    "        diff_x = x2.size(3) - x1.size(3)\n",
    "        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,  # left, right\n",
    "                        diff_y // 2, diff_y - diff_y // 2]) # top, bottom\n",
    "\n",
    "        # 3) 채널 방향 concat (skip 연결)\n",
    "        #    cat dim=1 은 채널(C) 축\n",
    "        x = torch.cat([x2, x1], dim=1)  # (B, C_skip + C_up, H, W) == (B, in_channels, H, W)\n",
    "\n",
    "        # 4) conv 블록으로 특징 정제 + 채널 축소\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57904cd-4d82-4f63-8a5e-c747b3ce6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    \"\"\"마지막 1x1 conv: 채널을 클래스 수로 매핑\n",
    "\n",
    "    예)\n",
    "      - binary segmentation: n_classes=1 (logits 1채널)\n",
    "      - multi-class:         n_classes=K (logits K채널)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca045b34-23ce-43f0-b111-a611091f22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net 전체 모델\n",
    "\n",
    "    전형적인 채널 구성 예:\n",
    "      1 → 64 → 128 → 256 → 512 → 1024 (down)\n",
    "      1024 → 512 → 256 → 128 → 64 (up)\n",
    "      숫자 외워야 할 듯.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels: int, n_classes: int, bilinear: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------------------------\n",
    "        # Encoder (Contracting path)\n",
    "        # -------------------------\n",
    "        self.inc = DoubleConv(n_channels, 64)   # (B, n_channels, H, W) → (B, 64, H, W)\n",
    "        self.down1 = Down(64, 128)              # → (B, 128, H/2, W/2)\n",
    "        self.down2 = Down(128, 256)             # → (B, 256, H/4, W/4)\n",
    "        self.down3 = Down(256, 512)             # → (B, 512, H/8, W/8)\n",
    "\n",
    "        # bilinear 업샘플링이면 파라미터/연산을 줄이기 위해 bottleneck 채널을 1024 대신 512로 줄이는 경우가 많음\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)  # → (B, 1024/f, H/16, W/16)\n",
    "\n",
    "        # -------------------------\n",
    "        # Decoder (Expanding path)\n",
    "        # -------------------------\n",
    "        # Up 블록의 in_channels는 \"concat 후 채널\" 기준으로 설계되어야 합니다.\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)  # (skip=512)와 concat을 고려\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "\n",
    "        # -------------------------\n",
    "        # Output head\n",
    "        # -------------------------\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # -------------------------\n",
    "        # Encoder: skip을 위해 각 단계 출력 저장\n",
    "        # -------------------------\n",
    "        x1 = self.inc(x)       # (B, 64, H, W)\n",
    "        x2 = self.down1(x1)    # (B, 128, H/2, W/2)\n",
    "        x3 = self.down2(x2)    # (B, 256, H/4, W/4)\n",
    "        x4 = self.down3(x3)    # (B, 512, H/8, W/8)\n",
    "        x5 = self.down4(x4)    # (B, 1024/f, H/16, W/16)\n",
    "\n",
    "        # -------------------------\n",
    "        # Decoder: 업샘플 + skip concat\n",
    "        # -------------------------\n",
    "        x = self.up1(x5, x4)   # (B, 512/f, H/8, W/8)\n",
    "        x = self.up2(x, x3)    # (B, 256/f, H/4, W/4)\n",
    "        x = self.up3(x, x2)    # (B, 128/f, H/2, W/2)\n",
    "        x = self.up4(x, x1)    # (B, 64, H, W)\n",
    "\n",
    "        # -------------------------\n",
    "        # 최종 logits 출력\n",
    "        # -------------------------\n",
    "        logits = self.outc(x)  # (B, n_classes, H, W)\n",
    "        return logits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
