{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ecba8-e54c-46b8-96c4-8413bfe19231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Distillation : 지식증류의원리를이해하고\n",
    "                    Teacher 모델의예측값과정답값을활용해서Student모델학습에연결시킬수있나\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f0767-d59c-4c75-bc1a-ba9fbae7341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 요약 \"\"\"\n",
    "def train_knowledge_distillation()\n",
    "    soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "    student_prob = nn.functional.softmax(student_logits / T, dim=-1)\n",
    "\n",
    "    soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - student_prob.log())) / student_prob.size(0) * (T**2)\n",
    "\n",
    "    label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "    loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "    \n",
    "def train_cosine_loss ()\n",
    "    _, teacher_hidden_representation = teacher(inputs)\n",
    "    hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation,\n",
    "                                          target=torch.ones(inputs.size(0)).cuda())\n",
    "    label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "    loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "    \n",
    "def train_mse_loss()\n",
    "    _, teacher_feature_map = teacher(inputs)\n",
    "\n",
    "    hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n",
    "\n",
    "    label_loss = ce_loss(student_logits, labels)\n",
    "    \n",
    "    loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a7a58-8451-4740-b42f-604208878e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher,\n",
    "                                 student,\n",
    "                                 train_loader,\n",
    "                                 epochs,\n",
    "                                 learning_rate,\n",
    "                                 T,  # temperature\n",
    "                                 soft_target_loss_weight,\n",
    "                                 ce_loss_weight):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ##################### YOUR CODE STARTS HERE #####################\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "            # Soften the student logits by applying softmax\n",
    "            # Hint: nn.functional.softmax()\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            student_prob = nn.functional.softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = torch.sum(soft_targets * (soft_targets.log() - student_prob.log())) / student_prob.size(0) * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "            ##################### YOUR CODE ENDS HERE #######################\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f69657-f14c-4854-8b29-00bf5d10a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cosine_loss(teacher,\n",
    "                      student,\n",
    "                      train_loader,\n",
    "                      epochs,\n",
    "                      learning_rate,\n",
    "                      hidden_rep_loss_weight,\n",
    "                      ce_loss_weight):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    cosine_loss = nn.CosineEmbeddingLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ##################### YOUR CODE STARTS HERE #####################\n",
    "            # Forward pass with the teacher model and keep only the hidden representation\n",
    "            with torch.no_grad():\n",
    "                _, teacher_hidden_representation = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits, student_hidden_representation = student(inputs)\n",
    "\n",
    "            # Calculate the cosine loss. Target is a vector of ones. From the loss formula above we can see that is\n",
    "            # the case where loss minimization leads to cosine similarity increase.\n",
    "            # Hint: cosine_loss(x, y, target)에서 target은 1로 이루어진 vector이며, torch.ones(inputs.size(0)).cuda())를 사용\n",
    "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation,\n",
    "                                          target=torch.ones(inputs.size(0)).cuda())\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "            ##################### YOUR CODE ENDS HERE #######################\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f31cee-a299-467c-9c4f-4cb5741e26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mse_loss(teacher,\n",
    "                   student,\n",
    "                   train_loader,\n",
    "                   epochs,\n",
    "                   learning_rate,\n",
    "                   feature_map_weight,\n",
    "                   ce_loss_weight):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ##################### YOUR CODE STARTS HERE #####################\n",
    "            # Again ignore teacher logits\n",
    "            with torch.no_grad():\n",
    "                _, teacher_feature_map = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits, regressor_feature_map = student(inputs)\n",
    "\n",
    "            # Calculate the loss\n",
    "            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = feature_map_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "            ##################### YOUR CODE ENDS HERE #######################\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e711d-6b88-439d-97f6-0ac51dff4374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
