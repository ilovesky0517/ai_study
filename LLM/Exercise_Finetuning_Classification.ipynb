{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27ab30-4a2e-490a-942c-77dcda09e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
    "# Source for \"Build a Large Language Model From Scratch\"\n",
    "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
    "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
    "\n",
    "# 6장의 주요 내용을 요약한 파일: 분류(Classification)를 위한 GPT 미세 조정\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 이전 챕터나 별도 모듈에서 정의된 GPT 모델 관련 함수들 임포트\n",
    "from previous_chapters import GPTModel, load_gpt2_model\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 데이터 준비 유틸리티 함수들\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    \"\"\"\n",
    "    스팸 데이터셋(SMS Spam Collection)을 다운로드하고 압축을 해제하는 함수\n",
    "    \"\"\"\n",
    "    if data_file_path.exist():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "    return\n",
    "\n",
    "    # 파일 다운로드 (스트리밍 방식)\n",
    "    response=  requests.get(url, stream=True, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    with open(zip_path, \"wb\") as out_file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                out_file.write(chunk)\n",
    "     \n",
    "    # 압축 해제\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "        \n",
    "    # 압축 해제된 파일에 .tsv 확장자 추가 (Pandas로 읽기 편하게)\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as \"{data_file_path}\")\n",
    "\n",
    "def create_balanced_dataset(df):\n",
    "    \"\"\"\n",
    "    데이터 불균형 해결을 위한 함수.\n",
    "    스팸(spam) 데이터 수에 맞춰 햄(ham, 정상 메일) 데이터를 언더샘플링합니다.\n",
    "    \"\"\"\n",
    "    # \"spam\" 라벨의 개수 계산\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "          \n",
    "    # \"ham\" 데이터 중에서 \"spam\" 개수만큼만 무작위 추출\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # 두 데이터셋 병합\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "          \n",
    "    return balanced_df\n",
    "\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    \"\"\"\n",
    "    데이터셋을 학습(Train), 검증(Validation), 테스트(Test) 셋으로 분할하는 함수\n",
    "    \"\"\"\n",
    "    # 전체 데이터 섞기\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "          \n",
    "    # 분할 지점(인덱스) 계산\n",
    "    train_end = int(len(df)*train_frac)\n",
    "    validataion_end = train_end + int(len(df)*validation_frac)\n",
    "          \n",
    "    #데이터 분할\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset 클래스 정의.\n",
    "    텍스트를 토큰화하고, 패딩(Padding) 처리를 수행합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # 1. 텍스트 데이터를 토큰 ID 리스트로 변환 (Tokenization)\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "        \n",
    "        # 2. 최대 길이(max_length) 설정\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length() \n",
    "          \n",
    "        \n",
    "        \n",
    "        \n",
    "          \n",
    "          \n",
    "          \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
