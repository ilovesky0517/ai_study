{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1aca0-b64d-4e04-9442-761c672efbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#####################################\n",
    "# Chapter 2: ë°ì´í„° ë¡œë”© ë° ì²˜ë¦¬\n",
    "#####################################\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    \"\"\"\n",
    "    GPT í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
    "    í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ í† í°í™”í•˜ê³ , ì…ë ¥(input)ê³¼ íƒ€ê²Ÿ(target) ìŒì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "                \n",
    "        # 1. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.\n",
    "        # <|endoftext|> ê°™ì€ íŠ¹ìˆ˜ í† í°ë„ í—ˆìš©í•˜ì—¬ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        # 2. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¡°ê°ëƒ…ë‹ˆë‹¤.\n",
    "        # strideë§Œí¼ ì´ë™í•˜ë©´ì„œ max_length ê¸¸ì´ì˜ ë©ì–´ë¦¬(chunk)ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            \n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last = True, num_workers=0):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë°›ì•„ í•™ìŠµì— ì‚¬ìš©í•  DataLoaderë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (GPT-2ìš© BPE ì¸ì½”ë”© ì‚¬ìš©)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    \n",
    "     # ë°ì´í„°ë¡œë” ìƒì„± (ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ë¬¶ì–´ì£¼ê³  ì…”í”Œë§ ìˆ˜í–‰)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last = drop_last, num_workers=num_workers)\n",
    "    \n",
    "    return dataloader\n",
    "                         \n",
    "            \n",
    "                                     \n",
    "                                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ffd99-4c3a-40fe-9762-6d204430709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 3: ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ (ëª¨ë¸ì˜ í•µì‹¬) // Exercise_Attentionì— ìˆëŠ” MultiHeadAttention(nn.Module) ê³¼ ë™ì¼\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ë©€í‹° í—¤ë“œ ì…€í”„ ì–´í…ì…˜ (Multi-Head Self-Attention) ëª¨ë“ˆì…ë‹ˆë‹¤.\n",
    "    ì…ë ¥ ë°ì´í„° ê°„ì˜ ê´€ê³„ì„±ì„ ì—¬ëŸ¬ ê´€ì (Head)ì—ì„œ ë³‘ë ¬ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"ì¶œë ¥ ì°¨ì›(d_out)ì€ í—¤ë“œ ìˆ˜(num_heads)ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # ê° í—¤ë“œê°€ ë‹´ë‹¹í•  ì°¨ì› í¬ê¸°\n",
    "\n",
    "        # Query, Key, Valueë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ì„ í˜• íˆ¬ì˜ ë ˆì´ì–´ë“¤\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        # ë©€í‹° í—¤ë“œ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹œ í›„ í†µê³¼ì‹œí‚¤ëŠ” ì¶œë ¥ ë ˆì´ì–´\n",
    "        self.out_proj = nn.Linear(d_out, d_out) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal Mask (ì¸ê³¼ì  ë§ˆìŠ¤í‚¹) ìƒì„±: ë¯¸ë˜ì˜ í† í°ì„ ë³´ì§€ ëª»í•˜ê²Œ í•¨\n",
    "        # ìƒì‚¼ê° í–‰ë ¬(ëŒ€ê°ì„  ìœ„ìª½)ì„ 1ë¡œ ì±„ì›Œì„œ ë‚˜ì¤‘ì— ë§ˆìŠ¤í‚¹ì— ì‚¬ìš©\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # b: ë°°ì¹˜ í¬ê¸°, num_tokens: ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "\n",
    "        # 1. Q, K, V ê³„ì‚°\n",
    "        keys = self.W_key(x)     # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 2. í—¤ë“œ ë‚˜ëˆ„ê¸° (Multi-head splitting)\n",
    "        # ì°¨ì›ì„ ë³€í˜•í•˜ì—¬ ì—¬ëŸ¬ í—¤ë“œê°€ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•¨\n",
    "        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 3. ì°¨ì› ìˆœì„œ ë³€ê²½ (Transpose)\n",
    "        # (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        # ì´ë ‡ê²Œ í•˜ë©´ (num_tokens, head_dim) í–‰ë ¬ì´ í—¤ë“œ ê°œìˆ˜ë§Œí¼ ë…ë¦½ì ìœ¼ë¡œ ì¡´ì¬í•˜ê²Œ ë¨\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # 4. Scaled Dot-Product Attention ê³„ì‚°\n",
    "        # Queryì™€ Keyì˜ ë‚´ì  (ìœ ì‚¬ë„ ê³„ì‚°)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # ê²°ê³¼ Shape: (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # 5. ë§ˆìŠ¤í‚¹ (Masking)\n",
    "        # í˜„ì¬ ì‹œì ë³´ë‹¤ ë¯¸ë˜ì˜ í† í° ì •ë³´ë¥¼ ì°¸ì¡°í•˜ì§€ ëª»í•˜ê²Œ -infë¡œ ê°€ë¦¼\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # 6. ì†Œí”„íŠ¸ë§¥ìŠ¤ ë° ë“œë¡­ì•„ì›ƒ\n",
    "        # ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜ (í•©ì´ 1ì´ ë˜ë„ë¡)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 7. Valueì™€ì˜ ê°€ì¤‘ì¹˜ í•© (Context Vector ê³„ì‚°)\n",
    "        # Shape: (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "\n",
    "        # 8. í—¤ë“œ ê²°í•© (Concatenation)\n",
    "        # ë‚˜ëˆ ì¡Œë˜ í—¤ë“œë“¤ì„ ë‹¤ì‹œ ì›ë˜ì˜ d_out ì°¨ì›ìœ¼ë¡œ í•©ì¹¨\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        \n",
    "        # 9. ìµœì¢… ì„ í˜• íˆ¬ì˜\n",
    "        context_vec = self.out_proj(context_vec) \n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe6e33d-c4c1-4460-abb9-028236fb3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 4: GPT ì•„í‚¤í…ì²˜ êµ¬ì„± ìš”ì†Œ\n",
    "#####################################\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    ì¸µ ì •ê·œí™” (Layer Normalization): í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì„\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    GELU í™œì„±í™” í•¨ìˆ˜: GPT ê³„ì—´ì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5* x * (1+torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * ( x + 0.044715 * torch.pow(x,3))\n",
    "        ))\n",
    "    \"\"\"\n",
    "    0.5 * x * (1 + tanh(...))  â†’ GELU ê·¼ì‚¬ ê³µì‹ì˜ í˜•íƒœ.\n",
    "    torch.sqrt(2.0 / torch.pi) â†’ ê·¼ì‚¬ì‹ì—ì„œ ë‚˜ì˜¤ëŠ” ìƒìˆ˜. ë£¨íŠ¸(2.0 / ğœ‹)\n",
    "    x + 0.044715 * x^3         â†’ ë‹¤í•­ì‹ ê·¼ì‚¬. \n",
    "    torch.tanh(...)            â†’ ëˆ„ì ë¶„í¬í•¨ìˆ˜(CDF)ë¥¼ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©.\n",
    "    \"\"\"\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (Feed-Forward Network)\n",
    "    ì–´í…ì…˜ì´ ëª¨ì€ ì •ë³´ë¥¼ ê° í† í°ë³„ë¡œ ê°œë³„ì ìœ¼ë¡œ ê°€ê³µí•˜ëŠ” ì—­í• \n",
    "    ë³´í†µ ì„ë² ë”© ì°¨ì›ì„ 4ë°°ë¡œ ëŠ˜ë ¸ë‹¤ê°€ ë‹¤ì‹œ ì¤„ì„\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU,\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7713113-bbb7-4996-b164-562051041693",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ì¤‘ìš” : ì•„ë˜ëŠ” GPT ëª¨ë¸ ì „ì²´ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤\n",
    "\n",
    "#####################################\n",
    "# Chapter 4: GPT ì•„í‚¤í…ì²˜ ì¡°ë¦½\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    í‘œì¤€ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ (Decoder Block)\n",
    "    êµ¬ì¡°: LayerNorm -> Attention -> Add(Residual) -> LayerNorm -> FeedForward -> Add(Residual)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # region [ì–´í…ì…˜ ë¸”ë¡ (Residual Connection ì ìš©)]\n",
    "        shortcut = x\n",
    "        x = self.norm1(x) # Pre-LayerNorm ë°©ì‹\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # ì›ë³¸ ì…ë ¥ì„ ë”í•´ì¤Œ (ê¸°ìš¸ê¸° ì†Œì‹¤ ë°©ì§€)\n",
    "        # endregion\n",
    "        \n",
    "        #region  [í”¼ë“œ í¬ì›Œë“œ ë¸”ë¡ (Residual Connection ì ìš©)]\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        #endregion\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    ì „ì²´ GPT ëª¨ë¸ êµ¬ì¡° ì •ì˜\n",
    "    Embedding -> Transformer Blocks -> Final Norm -> Output Head\n",
    "    \"\"\"\n",
    "    def __init__(self, cgf):\n",
    "        super().__init__()\n",
    "        # í† í° ì„ë² ë”© (ë‹¨ì–´ -> ë²¡í„°)\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        # ìœ„ì¹˜ ì„ë² ë”© (ìœ„ì¹˜ ì •ë³´ -> ë²¡í„°)\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "         # ìµœì¢… ì •ê·œí™” ë° ì¶œë ¥ í—¤ë“œ\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        # region [ì„ë² ë”© ìƒì„±]\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        # end region\n",
    "        \n",
    "        # region [í† í° ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì„ë² ë”© í•©ì‚°]\n",
    "        x = tok_embeds + pos_embeds\n",
    "        # endregion\n",
    "        \n",
    "        x = self.drop_emb(x)\n",
    "        \n",
    "        # region (íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ í†µê³¼)\n",
    "        x = self.trf_blocks(x)\n",
    "        # endregion\n",
    "        \n",
    "        # region (ìµœì¢… ì¶œë ¥ ê³„ì‚°)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        #endregion\n",
    "        \n",
    "        return logits        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e622e10-3542-491e-a825-e3f37dd2ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \"\"\"\n",
    "    ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ë£¨í”„\n",
    "    í˜„ì¬ ë¬¸ë§¥ì„ ë„£ì–´ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ê³ , ì´ë¥¼ ë‹¤ì‹œ ë¬¸ë§¥ì— ì¶”ê°€í•˜ì—¬ ë°˜ë³µí•¨\n",
    "    \"\"\"\n",
    "    # idx: í˜„ì¬ ë¬¸ë§¥ì˜ í† í° ì¸ë±ìŠ¤ë“¤ (Batch, Time)\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” ìµœëŒ€ ê¸¸ì´(context_size)ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìë¦„\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # ëª¨ë¸ ì˜ˆì¸¡ (ê¸°ìš¸ê¸° ê³„ì‚° ë¶ˆí•„ìš”)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì˜ˆì¸¡ê°’ë§Œ ê°€ì ¸ì˜´ (ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì´ë¯€ë¡œ)\n",
    "        # (batch, n_token, vocab_size) -> (batch, vocab_size)    \n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # ê°€ì¥ í™•ë¥ (ë¡œì§“ê°’)ì´ ë†’ì€ í† í° ì„ íƒ (Greedy Decoding\n",
    "        idx_next = torch.argmax(logits, dim=1, keepdim=True)\n",
    "        \n",
    "        # ì˜ˆì¸¡ëœ í† í°ì„ í˜„ì¬ ì‹œí€€ìŠ¤ ë’¤ì— ì´ì–´ ë¶™ì„\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5914a9-88b2-4887-9a78-812e5294d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # GPT-2 Small ëª¨ë¸ ì„¤ì •ê°’ (124M íŒŒë¼ë¯¸í„°)\n",
    "    GPT_CONFIG_124M = {\n",
    "        \"vocab_size\": 50257,     # ë‹¨ì–´ ì§‘í•© í¬ê¸°\n",
    "        \"context_length\": 1024,  # ìµœëŒ€ ë¬¸ë§¥ ê¸¸ì´\n",
    "        \"emb_dim\": 768,          # ì„ë² ë”© ì°¨ì›\n",
    "        \"n_heads\": 12,           # ì–´í…ì…˜ í—¤ë“œ ìˆ˜\n",
    "        \"n_layers\": 12,          # ë ˆì´ì–´ ìˆ˜\n",
    "        \"drop_rate\": 0.1,        # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    "        \"qkv_bias\": False        # Q,K,V í¸í–¥ ì‚¬ìš© ì—¬ë¶€\n",
    "    }\n",
    "    \n",
    "    torch.manual_seed(123)\n",
    "    \n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.eval()\n",
    "    \n",
    "    start_context = \"Hello, I am\"\n",
    "    \n",
    "    tokenizer = toktoken.get_encoding(\"gpt2\")\n",
    "    encoded = tokenizer.encode(start_context)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    \n",
    "    print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "    print(\"\\nInput text:\", start_context)\n",
    "    print(\"Encoded input text:\", encoded)\n",
    "    print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ìƒì„± ì‹¤í–‰\n",
    "    out = generate_text_simple(\n",
    "        model=model,\n",
    "        idx=encoded_tensor,\n",
    "        max_new_tokens=10,  # 10ê°œì˜ ìƒˆë¡œìš´ í† í° ìƒì„±\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "    \n",
    "    # ìƒì„±ëœ ê²°ê³¼ ë””ì½”ë”©\n",
    "    decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "    \n",
    "    print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "    print(\"\\nOutput:\", out)\n",
    "    print(\"Output length:\", len(out[0]))\n",
    "    print(\"Output text:\", decoded_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "\"\"\"\n",
    "==================================================\n",
    "                      IN\n",
    "==================================================\n",
    "\n",
    "Input text: Hello, I am\n",
    "Encoded input text: [15496, 11, 314, 716]\n",
    "encoded_tensor.shape: torch.Size([1, 4])\n",
    "\n",
    "\n",
    "==================================================\n",
    "                      OUT\n",
    "==================================================\n",
    "\n",
    "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
    "         49706, 43231, 47062, 34657]])\n",
    "Output length: 14\n",
    "Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4a199-d850-49e4-966d-0cf10b22b9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e881a92-87c4-4c63-84c4-3f086aa897e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f27af-eb3b-4bf9-8101-fe97628bd43f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
