{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db942af-95e6-4d33-b866-781c2f42643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import load_gpt2_model\n",
    "# ==========================================\n",
    "# 1. 데이터셋 준비 (Next Token Prediction)\n",
    "# ==========================================\n",
    "# 언어 모델 학습의 핵심은 \"입력된 단어들을 보고, 바로 다음에 올 단어를 맞추는 것\"입니다.\n",
    "\n",
    "# inputs: 모델에게 보여줄 문제 (현재 시점의 단어들)\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # 문장 1: [\"every\", \"effort\", \"moves\"]\n",
    "                       [40,    1107, 588]])   # 문장 2: [\"I\",     \"really\", \"like\"]\n",
    "\n",
    "# targets: 모델이 맞춰야 할 정답 (한 칸씩 오른쪽으로 이동된 단어들)\n",
    "# 예: \"every\"를 보여주면 -> \"effort\"를 맞춰야 함\n",
    "targets = torch.tensor([[3626, 6100, 345],    # 문장 1 정답: [\"effort\", \"moves\", \"you\"]\n",
    "                        [1107, 588, 11311]])  # 문장 2 정답: [\"really\", \"like\", \"chocolate\"]\n",
    "\n",
    "# ==========================================\n",
    "# 2. 모델 추론 (Forward Pass)\n",
    "# ==========================================\n",
    "# torch.no_grad(): 평가만 할 것이므로 불필요한 기울기(Gradient) 계산 메모리를 아낌\n",
    " # 실제 GPT-2 Small (124M 파라미터) 모델 설정 및 가중치 로드\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     \n",
    "    \"context_length\": 1024,  \n",
    "    \"drop_rate\": 0.0,        \n",
    "    \"qkv_bias\": True,\n",
    "    \"emb_dim\": 768, \n",
    "    \"n_layers\": 12, \n",
    "    \"n_heads\": 12         \n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small-124M.pth\"\n",
    "model = load_gpt2_model(model_name, BASE_CONFIG)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 모델에 입력을 넣어 예측값(Logits)을 얻습니다.\n",
    "    # logits shape: (배치 크기 2, 문장 길이 3, 단어장 크기 50257)\n",
    "    # 의미: 2개 문장의 각 3개 위치마다, 50,257개 단어 각각에 대한 점수를 출력\n",
    "    logits = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517d20d-9d5e-4816-93a9-f39c2993b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. 결과 해석 (확률 변환 및 확인)\n",
    "# ==========================================\n",
    "\n",
    "# 3-1. 로짓(점수) -> 확률(Probability) 변환\n",
    "# 로짓은 -무한대 ~ +무한대 범위의 숫자이므로, Softmax를 써서 0~1 사이 확률로 바꿉니다.\n",
    "# dim=-1: 가장 마지막 차원(단어장 50257개)에 대해 확률의 합이 1이 되게 만듦\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "print(\"확률 텐서 크기:\", probas.shape) \n",
    "# 예상 출력: torch.Size([2, 3, 50257])\n",
    "\n",
    "# 3-2. 가장 높은 확률을 가진 단어 찾기 (예측 결과)\n",
    "# argmax: 확률이 가장 높은 인덱스(단어 ID)를 반환\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"모델이 예측한 토큰 ID:\\n\", token_ids)\n",
    "\n",
    "\"\"\"\n",
    "확률 텐서 크기: torch.Size([2, 3, 50257])\n",
    "모델이 예측한 토큰 ID:\n",
    " tensor([[[ 13],\n",
    "         [  1],\n",
    "         [319]],\n",
    "\n",
    "        [[ 13],\n",
    "         [588],\n",
    "         [262]]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e448c-c1c3-4315-819e-abab65e6be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. PyTorch 함수로 손실(Loss) 계산하기 (권장 방식)\n",
    "# ==========================================\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"로짓 크기 (변경 전):\", logits.shape)   # (2, 3, 50257)\n",
    "print(\"타깃 크기 (변경 전):\", targets.shape)  # (2, 3)\n",
    "\n",
    "# 5-1. 평탄화 (Flattening)\n",
    "# CrossEntropyLoss 함수는 입력을 (N, Class) 형태로 받기를 선호합니다.\n",
    "# 즉, \"어떤 문장의 몇 번째 단어인지\"는 중요하지 않고, \"총 몇 문제를 풀었나\"로 형태를 바꿉니다.\n",
    "\n",
    "# (배치 2 * 길이 3, 단어장 50257) -> (6, 50257) : 총 6개의 단어 예측 문제로 변환\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "\n",
    "# (배치 2 * 길이 3) -> (6) : 정답지도 일렬로 6개 나열\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"펼친 로짓:\", logits_flat.shape)   # torch.Size([6, 50257])\n",
    "print(\"펼친 타깃:\", targets_flat.shape)  # torch.Size([6])\n",
    "\n",
    "# 5-2. Cross Entropy Loss 계산\n",
    "# 주의: nn.functional.cross_entropy는 입력으로 '확률(probas)'이 아니라 '로짓(logits)'을 받습니다.\n",
    "# 함수 내부적으로 Softmax -> Log -> NLLLoss 과정을 모두 수행하기 때문입니다.\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat) \n",
    "\n",
    "print(\"함수로 계산한 손실값(Loss):\", loss) \n",
    "# 이 값은 위에서 수동으로 계산한 'neg_avg_log_probas'와 거의 같아야 합니다.\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "로짓 크기 (변경 전): torch.Size([2, 3, 50257])\n",
    "타깃 크기 (변경 전): torch.Size([2, 3])\n",
    "펼친 로짓: torch.Size([6, 50257])\n",
    "펼친 타깃: torch.Size([6])\n",
    "함수로 계산한 손실값(Loss): tensor(7.0675)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d056ae4-29b8-4494-8865-ba4850688121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c0d30-fa12-4023-a12f-ff423297e402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
