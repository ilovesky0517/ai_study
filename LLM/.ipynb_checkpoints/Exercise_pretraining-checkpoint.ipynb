{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431f111-72bc-422f-9891-f960678b6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
    "# Source for \"Build a Large Language Model From Scratch\"\n",
    "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
    "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "from previous_chapters import GPTModel, create_dataloader_v1, generate_text_simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb918efa-fd03-4df3-b986-048325dac776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"\n",
    "    하나의 배치(Batch)에 대해 모델의 예측값과 실제값 사이의 오차(Loss)를 계산합니다.\n",
    "    \"\"\"\n",
    "    # 데이터를 GPU(또는 설정된 device)로 이동\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    \n",
    "    # 1. 모델의 순전파(Forward Pass)\n",
    "    # logits shape: (batch_size, sequence_length, vocab_size)\n",
    "    logits = model(input_batch)\n",
    "    \n",
    "    # 2. 손실 계산 (CrossEntropyLoss)\n",
    "    # PyTorch의 CrossEntropyLoss는 입력을 (N, C) 형태로 받기를 원합니다.\n",
    "    #   - N: 전체 샘플 수 (여기서는 Batch_size * Sequence_length)\n",
    "    #   - C: 클래스 수 (여기서는 Vocab_size)\n",
    "    # 따라서 3차원 텐서를 2차원으로 평탄화(flatten) 해야 합니다.\n",
    "    \n",
    "    # logits.flatten(0, 1) -> (batch_size * sequence_length, vocab_size)\n",
    "    # target_batch.flatten() -> (batch_size * sequence_length)\n",
    "    # region [손실 계산 (CrossEntropyLoss)]\n",
    "    loss = torch.nn.functional.cropp_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    \n",
    "    # endregion\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a8310-dba8-48c3-a0fa-b20bfa3d1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \"\"\"\n",
    "    [핵심] 모델 훈련을 담당하는 메인 루프입니다.\n",
    "    \"\"\"\n",
    "    # 로그 저장을 위한 리스트들\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "\n",
    "    # Epoch: 전체 데이터셋을 한 번 훑는 단위\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # [Step 1] 이전 배치에서 계산된 기울기 초기화 (필수)\n",
    "            # region [기울기 초기화]\n",
    "            optimizer.zero_grad()\n",
    "            # endregion\n",
    "                          \n",
    "            # [Step 2] 순전파 및 손실 계산\n",
    "            # region [손실 계산]\n",
    "            loss = calc_loss_batch(input_batch, output_batch, model, device)\n",
    "            # endregion\n",
    "\n",
    "            # [Step 3] 역전파 (Backpropagation): 각 파라미터별 기울기(Gradient) 계산\n",
    "            # region [역전파 수행]\n",
    "            loss.backward()\n",
    "            # endregion\n",
    "\n",
    "            # [Step 4] 가중치 업데이트: 계산된 기울기를 이용해 파라미터 수정\n",
    "            # region [가중치 업데이트]\n",
    "            optimizer.step()\n",
    "            # endregion\n",
    "            \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq ==0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "         # 한 에포크가 끝날 때마다 샘플 문장을 생성하여 모델이 똑똑해지고 있는지 확인\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )   \n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdba56-e87a-4096-8c5f-c0a8bc9b7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 중요: 결과를 생성합니다.\n",
    "def generate(model, idx, max_new_tokens, context_size, eos_id=None):\n",
    "    \"\"\"\n",
    "    고급 텍스트 생성 함수입니다. (Top-k 샘플링 & Temperature Scaling 포함)\n",
    "    \n",
    "    Args:\n",
    "        idx: 현재 문맥 (Shape: Batch, Time)\n",
    "        temperature: 0이면 결정적(Greedy), 높을수록 창의적/랜덤\n",
    "        top_k: 확률 상위 k개만 후보로 남김 (이상한 단어 생성 방지)\n",
    "    \"\"\"\n",
    "    \n",
    "    tor _ in range(max_new_tokens):\n",
    "        \n",
    "        # 1. 문맥 자르기 (Context Cropping)\n",
    "        # 모델이 처리할 수 있는 최대 길이(context_size)를 넘지 않도록 뒤쪽만 남김\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "         # 2. 모델 예측\n",
    "        # region [모델 예측]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)            \n",
    "            \n",
    "        # 3. 다음 단어 예측을 위해 '마지막 시점'의 로짓만 추출\n",
    "        # logits shape: (batch, seq_len, vocab_size) -> (batch, vocab_size)\n",
    "        logit = logits[:, -1, :]\n",
    "        \n",
    "        # 가장 높은 확률을 가진 토큰 선택 (Greedy Decoding)\n",
    "        # region [다음 토큰 선택 (Greedy Decoding)]\n",
    "        idx_next = torch.argmax(logits, dis=-1, keepdim=True)\n",
    "        # endregion\n",
    "        \n",
    "        # 4. 종료 조건 확인 (EOS 토큰이 나오면 중단)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        # 5. 생성된 토큰 이어붙이기\n",
    "        # 기존 문장(idx) 뒤에 새로 뽑은 토큰(idx_next)을 붙여 다음 스텝의 입력으로 씀\n",
    "        # region [생성된 토큰 이어붙이기]\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        #endregion\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34518d09-c41e-41a4-8023-133544e4fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    \"\"\"\n",
    "    사람이 읽는 텍스트 문자열을 모델이 이해하는 토큰 ID 텐서로 변환합니다.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer.encode(text)\n",
    "    \n",
    "    # 모델은 입력으로 (Batch_Size, Sequence_Length)의 2차원 텐서를 요구합니다.\n",
    "    # 하지만 여기서는 문장 1개이므로 Batch_Size가 없습니다.\n",
    "    # 따라서 .unsqueeze(0)를 사용하여 가짜 배치 차원을 추가합니다.\n",
    "    # 예: [12, 34, 56] -> [[12, 34, 56]] (Shape: [1, seq_len])\n",
    "    encoded_tensor = torch.tensor(encoded).unzqueeze(0)\n",
    "    return encoded_tensor\n",
    "    \n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    \"\"\"\n",
    "    모델이 뱉어낸 토큰 ID 텐서를 사람이 읽을 수 있는 텍스트로 변환합니다.\n",
    "    \"\"\"\n",
    "    # 디코딩을 위해 불필요한 배치 차원을 제거합니다.\n",
    "    # 예: [[12, 34, 56]] -> [12, 34, 56]\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    데이터 로더 전체(또는 일부)를 돌면서 평균 손실을 계산합니다.\n",
    "    훈련에는 관여하지 않고, 오직 '평가(Evaluation)' 목적으로만 쓰입니다.\n",
    "    \"\"\"\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 평가 시간을 단축하기 위해 전체 데이터를 다 보지 않고 일부만 볼 수 있게 설정\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i< num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evalueate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    \"\"\"\n",
    "    현재 모델의 성능을 훈련 세트와 검증 세트 각각에 대해 평가합니다.\n",
    "    \"\"\"\n",
    "    model.eval() # [중요] 평가 모드 전환: Dropout이나 BatchNorm 등의 동작이 변경됨\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    \"\"\"\n",
    "    훈련 중간중간 모델이 문장을 어떻게 생성하는지 눈으로 확인하기 위한 함수입니다.\n",
    "    \"\"\"\n",
    "    model.eval() # 평가 모드\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model = model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        \n",
    "         # 출력이 너무 길어지면 보기 힘드므로 줄바꿈을 공백으로 변경\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  \n",
    "    \n",
    "    model.train() # 훈련 모드 복귀\n",
    "        \n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    훈련 진행 상황(Loss 변화)을 시각화하는 함수입니다.\n",
    "    X축을 'Epoch'와 '처리한 토큰 수' 두 가지 기준으로 보여줍니다.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # 기본 X축: Epoch 기준\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    # 보조 X축 (상단): 처리한 토큰 수(Tokens seen) 기준\n",
    "    ax2 = ax1.twiny() \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0) # 투명한 그래프로 축만 생성\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout() \n",
    "    # plt.show() # 주피터 노트북 환경이면 주석 해제\n",
    "\n",
    "def main(gpt_config, settings):\n",
    "    \n",
    "    torch.manual_seed(123)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    ##############################\n",
    "    # 1. 데이터 준비\n",
    "    ##############################\n",
    "    file_path = \"datas/the-verdict.txt\"\n",
    "    url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        text_data = response.text\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    \n",
    "    ##############################\n",
    "    # 2. 모델 및 옵티마이저 초기화\n",
    "    ##############################\n",
    "    \n",
    "    model = GPTModel(gpt_config)\n",
    "    model.to(device)\n",
    "    \n",
    "    # AdamW: 가중치 감쇠(Weight Decay)가 적용된 Adam 옵티마이저\n",
    "    # Weight Decay는 모델이 너무 복잡해지지 않도록 규제(Regularization)하는 역할\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=settings[\"learning_rate\"], weight_decay=settings[\"weight_decay\"]\n",
    "    )\n",
    "    \n",
    "    ##############################\n",
    "    # 3. 데이터 로더 구축\n",
    "    ##############################\n",
    "    # 전체 텍스트를 9:1 비율로 훈련용과 검증용으로 나눔\n",
    "    train_ratio = 0.9\n",
    "    \n",
    "    split_idx = int(train_ratio* len(text_data))\n",
    "    \n",
    "    # 훈련 데이터 로더: 순서를 섞음(Shuffle=True)\n",
    "    train_loader = create_dataloader_v1(\n",
    "        text_data[:split_idx],\n",
    "        batch_size=settings[\"batch_size\"],\n",
    "        max_length=gpt_config[\"context_length\"],\n",
    "        stride=gpt_config[\"context_length\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "     # 검증 데이터 로더: 순서를 섞지 않음(Shuffle=False) -> 평가는 일관되게\n",
    "    val_loader = create_dataloader_v1(\n",
    "        text_data[split_idx:],\n",
    "        batch_size=settings[\"batch_size\"],\n",
    "        max_length=gpt_config[\"context_length\"],\n",
    "        stride=gpt_config[\"context_length\"],\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    ##############################\n",
    "    # 4. 훈련 시작\n",
    "    ##############################\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    train_loss, val_loss, tokens_seen = train_model_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=settings[\"num_epochs\"], eval_freq=5, eval_iter=1,\n",
    "        start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    return train_losses, val_losses, tokens_seen, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a5b2a-fefd-4272-a308-0ae7054573c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 Small (124M) 모델 설정\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # 어휘 크기\n",
    "    \"context_length\": 256,  # 훈련 속도를 위해 원본(1024)보다 줄임\n",
    "    \"emb_dim\": 768,         # 임베딩 벡터 차원\n",
    "    \"n_heads\": 12,          # 어텐션 헤드 개수\n",
    "    \"n_layers\": 12,         # 레이어 깊이\n",
    "    \"drop_rate\": 0.1,       # 과적합 방지 드롭아웃\n",
    "    \"qkv_bias\": False       \n",
    "}\n",
    "\n",
    "# 학습 하이퍼파라미터\n",
    "OTHER_SETTINGS = {\n",
    "    \"learning_rate\": 5e-4, \n",
    "    \"num_epochs\": 10,       \n",
    "    \"batch_size\": 2,        \n",
    "    \"weight_decay\": 0.1     \n",
    "}\n",
    "\n",
    "###########################\n",
    "# 메인 실행: 훈련\n",
    "###########################\n",
    "train_losses, val_losses, tokens_seen, model = main(GPT_CONFIG_124M, OTHER_SETTINGS)\n",
    "\n",
    "###########################\n",
    "# 훈련 결과 시각화 및 저장\n",
    "###########################\n",
    "\n",
    "# 에포크 축 생성 (0부터 num_epochs까지)\n",
    "epochs_tensor = torch.linspace(0, OTHER_SETTINGS[\"num_epochs\"], len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "plt.savefig(\"outputs/loss.pdf\")\n",
    "\n",
    "# 모델 저장 및 로드 테스트\n",
    "torch.save(model.state_dict(),  \"outputs/model.pth\")\n",
    "\n",
    "# 디바이스 설정 (모델 로드 전에 정의 필요)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 다시 불러오기 테스트\n",
    "model = GPTModel(GPT_CONFIG_124M)  # 1. 빈 모델 생성 (현재 CPU에 있음)\n",
    "model.load_state.dict(torch.load(\"outputs/model.pth\", weight_only = True)) # 2. 가중치 로드\n",
    "model.to(device)  # <--- [핵심 수정] 3. 모델을 GPU로 이동시켜야 함\n",
    "\n",
    "torch.manual_seed(123)\n",
    "###########################\n",
    "# 생성 테스트 (Inference)\n",
    "###########################\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 훈련된 모델로 텍스트 생성 시도\n",
    "# Temperature=1.4로 설정하여 다소 창의적/다양한 문장 생성 유도\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e7f47-2b55-4937-9f34-39ff1d962be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58437149-ccc1-4011-8e05-a5eb7f9c730a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3146648-e66a-4383-bf0e-c98ffc5430a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86bc4bf-32f6-410b-9fed-582fa091d007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72c925-8e6b-49b2-b58f-f7392ab8a028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
