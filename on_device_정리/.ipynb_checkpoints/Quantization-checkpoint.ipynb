{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f24b0-8802-42d5-bf97-4b1a532423da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Quantization : 모델을layer 단위에서양자화/역양자화하는과정을이해하고있나 \n",
    "        > (내생각) 실제 코드에서 scale이라던지 zero point 연산 같은것들 위주로 봐야 할 듯\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a9c03-1b11-4fec-893d-b9f395bfd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [실습2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72babf-5cd3-404e-98d1-6bf7dcda5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -> torch.Tensor:\n",
    "   \n",
    "    assert(fp_tensor.dtype == torch.float)\n",
    "    assert(isinstance(scale, float) or\n",
    "           (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n",
    "    assert(isinstance(zero_point, int) or\n",
    "           (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # Step 1: fp_tensor를 scale 하세요.\n",
    "    scaled_tensor = fp_tensor/scale\n",
    "    # Step 2: 부동 소수점 값을 정수 값으로 rounding 하세요.\n",
    "    rounded_tensor = torch.round(scaled_tensor)\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    rounded_tensor = rounded_tensor.to(dtype)\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # Step 3: rounded_tensor를 zero_point 만큼 shift하여 영점을 조정합니다.\n",
    "    shifted_tensor = rounded_tensor + zero_point\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    # Step 4: shifted_tensor를 bitwidth 범위에 있도록 절사(clamp)합니다.\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n",
    "    return quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5700d27-5f8b-4b7c-a11c-c4c7d63aee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_scale_and_zero_point(fp_tensor, bitwidth):\n",
    "\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    fp_max = fp_tensor.max().item()\n",
    "    fp_min = fp_tensor.min().item()\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # hint: quantized_max - quantized_min = 2 ** bitwith - 1\n",
    "    scale = (fp_max - fp_min) / (quantized_max - quantized_min)\n",
    "    zero_point = round(quantized_min - fp_min/scale)\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    # clip the zero_point to fall in [quantized_min, quantized_max]\n",
    "    if zero_point < quantized_min:\n",
    "        zero_point = quantized_min\n",
    "    elif zero_point > quantized_max:\n",
    "        zero_point = quantized_max\n",
    "    else: # convert from float to int using round()\n",
    "        zero_point = round(zero_point)\n",
    "    return scale, int(zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d1887-f565-41e0-a552-fcf090a229fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "* [실습3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac788689-00c3-4752-ad23-1d902f972082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_linear(input, weight, bias, feature_bitwidth, weight_bitwidth,\n",
    "                     input_zero_point, output_zero_point,\n",
    "                     input_scale, weight_scale, output_scale):\n",
    "    assert(input.dtype == torch.int8)\n",
    "    assert(weight.dtype == input.dtype)\n",
    "    assert(bias is None or bias.dtype == torch.int32)\n",
    "    assert(isinstance(input_zero_point, int))\n",
    "    assert(isinstance(output_zero_point, int))\n",
    "    assert(isinstance(input_scale, float))\n",
    "    assert(isinstance(output_scale, float))\n",
    "    assert(weight_scale.dtype == torch.float)\n",
    "\n",
    "    # Step 1: integer-based fully-connected (8-bit multiplication with 32-bit accumulation)\n",
    "    if 'cpu' in input.device.type:\n",
    "        # use 32-b MAC for simplicity\n",
    "        output = torch.nn.functional.linear(input.to(torch.int32), weight.to(torch.int32), bias)\n",
    "    else:\n",
    "        # 현재 버전의 PyTorch는 GPU에서 정수 기반의 linear() 연산을 아직 지원하지 않습니다.\n",
    "        output = torch.nn.functional.linear(input.float(), weight.float(), bias.float())\n",
    "\n",
    "    # weight scale의 shape 가[oc, 1, 1, 1] 인 반면, output의 shape는 [batch_size, oc]이므로 shape를 조정합니다.\n",
    "    weight_scale = weight_scale.flatten().view(1, -1)\n",
    "\n",
    "    # scale이 부동 소수점이므로, 출력도 부동 소수점으로 변환해야 합니다.\n",
    "    output = output.float()\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # Step 2: output 텐서를 scale합니다.\n",
    "    output = output*(input_scale * weight_scale / output_scale)\n",
    "\n",
    "    # Step 3: output을 out_zero_point 만큼 이동하여 영점을 조정합니다.\n",
    "    output = output + output_zero_point\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    # 모든 값이 지정된 비트폭 범위 내에 있도록 clamp합니다.\n",
    "    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254af75c-7a7f-4ccf-a249-1e7fcb58b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_conv2d(input, weight, bias, feature_bitwidth, weight_bitwidth,\n",
    "                     input_zero_point, output_zero_point,\n",
    "                     input_scale, weight_scale, output_scale,\n",
    "                     stride, padding, dilation, groups):\n",
    "    assert(len(padding) == 4)\n",
    "    assert(input.dtype == torch.int8)\n",
    "    assert(weight.dtype == input.dtype)\n",
    "    assert(bias is None or bias.dtype == torch.int32)\n",
    "    assert(isinstance(input_zero_point, int))\n",
    "    assert(isinstance(output_zero_point, int))\n",
    "    assert(isinstance(input_scale, float))\n",
    "    assert(isinstance(output_scale, float))\n",
    "    assert(weight_scale.dtype == torch.float)\n",
    "\n",
    "    # Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)\n",
    "    input = torch.nn.functional.pad(input, padding, 'constant', input_zero_point)\n",
    "    if 'cpu' in input.device.type:\n",
    "        # use 32-b MAC for simplicity\n",
    "        output = torch.nn.functional.conv2d(input.to(torch.int32), weight.to(torch.int32), None, stride, 0, dilation, groups)\n",
    "    else:\n",
    "        # 현재 버전의 PyTorch는 GPU에서 정수 기반의 conv2d() 연산을 아직 지원하지 않습니다.\n",
    "        output = torch.nn.functional.conv2d(input.float(), weight.float(), None, stride, 0, dilation, groups)\n",
    "        output = output.round().to(torch.int32)\n",
    "    if bias is not None:\n",
    "        output = output + bias.view(1, -1, 1, 1)\n",
    "\n",
    "    # weight scale의 shape가 [oc, 1, 1, 1]인 반면, output의 shape는 [batch_size, oc, height, width]이므로 shape를 조정합니다.\n",
    "    weight_scale = weight_scale.flatten().view(1, -1, 1, 1)\n",
    "\n",
    "    # scale이 부동 소수점이므로, 출력도 부동 소수점으로 변환해야 합니다.\n",
    "    output = output.float()\n",
    "\n",
    "    ############### YOUR CODE STARTS HERE ###############\n",
    "    # hint: 이번 코드 블록은 quantized_linear()와 매우 유사합니다.\n",
    "\n",
    "    # Step 2: output 텐서를 scale합니다.\n",
    "    output = output*(input_scale * weight_scale / output_scale)\n",
    "\n",
    "    # Step 3: output을 out_zero_point 만큼 이동하여 영점을 조정합니다.\n",
    "    output = output + output_zero_point\n",
    "    ############### YOUR CODE ENDS HERE #################\n",
    "\n",
    "    # 모든 값이 지정된 비트폭 범위 내에 있도록 clamp 합니다.\n",
    "    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0dd571-9a66-451e-96f3-8a7a02cb00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 강의자료에서 양자화/역양자화가 같이 나온 유일한 경우인데 이렇게 쉽게 나올까?\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class CIFAR10Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10Classifier, self).__init__()\n",
    "\n",
    "        # 양자화 스텁 추가\n",
    "        self.quant = torch.quantization.QuantStub()  # 입력을 양자화\n",
    "        self.dequant = torch.quantization.DeQuantStub()  # 출력을 역양자화\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "        # Pooling and Dropout\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x) # 맨 처음에 양자화 하고~!!!\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.contiguous().view(-1, 128 * 4 * 4) # 혹시모르니 외워놓을까>?\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        x = self.dequant(x) # 맨 마지막에 역양자화 하자!!!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f761cb-ee73-4b68-bac1-5bd1e6d2de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  LLM  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f91b2-61aa-4843-8d45-61d5f0a334fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 핵심 양자화 함수 (simulated quantization, 즉 실제 비트 축소 없이 효과만 모사)\\\n",
    "# 레이어 단위는 아니긴 한데 함수 내에 양자화/역양자화가 모두 있어서 참고로 외워두는게 좋을 듯.\n",
    "def pseudo_quantize_tensor(w, n_bit=4, q_group_size=-1):\n",
    "    org_w_shape = w.shape\n",
    "    # 그룹 단위 양자화 적용 시 weight 행렬을 그룹 크기로 재구성\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    # 각 행(또는 그룹)별 최대값(α)과 최소값(β) 계산\n",
    "    max_val = w.amax(dim=1, keepdim=True)\n",
    "    min_val = w.amin(dim=1, keepdim=True)\n",
    "\n",
    "    # 스케일(scale)과 제로포인트(zero point) 계산\n",
    "    # scale = (α - β) / (2^n_bit - 1)\n",
    "    # zero = round(-β / scale)\n",
    "    max_int = 2 ** n_bit - 1\n",
    "    scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "    zeros = (-torch.round(min_val / scales)).clamp_(0, max_int)\n",
    "\n",
    "    # NaN 검증\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    # 양자화(Quantization) 단계: 실수값을 정수 범위 [0, 2^n_bit - 1]로 매핑\n",
    "    w = torch.clamp(torch.round(w / scales) + zeros, 0, max_int)\n",
    "\n",
    "    # 복원 단계(Dequantization): 정수값을 다시 실수 범위로 변환 (pseudo quantization)\n",
    "    w = (w - zeros) * scales\n",
    "\n",
    "    # NaN 검증\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    # 원래 텐서 형태로 복원\n",
    "    w = w.reshape(org_w_shape)\n",
    "    return w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
