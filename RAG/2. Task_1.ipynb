{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXSKbyvh2QaU"
   },
   "source": [
    "#üìì TASK #1: WEB-BASED RETRIEVAL SUMMARIZATION\n",
    "\n",
    "In this section, we will tackle the first task of the KDD Cup: Web-based Retrieval Summarization. Since the KDD Cup CRAG benchmark fundamentally focuses on RAG (Retrieval-Augmented Generation), our approach will also be based on the RAG framework.\n",
    "\n",
    "Before building the RAG system, let‚Äôs first clarify what problem we need to solve. At first, participants receive 5 web pages per question, potentially containing relevant information. And the objective is to measure the systems' capability to identify and condense this information into accurate answers.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"https://i.imgur.com/jlNdBmD.png\">\n",
    "\n",
    "By looking at the diagram above, you will get an idea of what problem we need to solve. Additionally, since you have already reviewed the input data in previous sessions, you are well aware of the types of data you will be working with.\n",
    "\n",
    "As you may recall, the CRAG dataset contains many challenging questions, and as we observed earlier, it is difficult for the LLM alone to solve these problems effectively. Therefore, we will explore how these types of problems can be addressed using the RAG framework.\n",
    "\n",
    "Specifically, This practice class will be comprised of four sections.  \n",
    "  \n",
    "### I. Implementing a Retriever\n",
    "### II. Implementing a Reader\n",
    "### III. Implementing a RAG\n",
    "### IV. Error case analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGvoJusEMYuq"
   },
   "source": [
    "## I. Implementing a Retriever\n",
    "\n",
    "Before building the RAG system, the first essential component we need is the **Retriever**. As you are already familiar, the retriever is a crucial element for building an effective RAG. If the retriever successfully retrieves a sufficient amount of relevant information and passes it to the LLM, the probability of the LLM generating the correct answer will significantly increase.\n",
    "\n",
    "In the previous session, we only experimented with the default retriever provided by `LlamaIndex` and made minor adjustments, such as modifying the chunk size.   \n",
    "This time, however, we will define the retriever in a more low-level manner and explore its use step by step.\n",
    "\n",
    "This section is divided into the following four stages.\n",
    "\n",
    "1. Preparing Python Packages\n",
    "2. Implementing a Chunk Extractor\n",
    "3. Implementing a Retriever\n",
    "4. Implementing a Retriever with LlamaIndex\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrqPxwa4OOe8"
   },
   "source": [
    "\n",
    "\n",
    "### 1. Preparing Python Packages\n",
    "\n",
    "As always, we will start by installing and importing the necessary libraries for use.  \n",
    "\n",
    "At this point, we will also set the values for the global variables that will be needed later. The significance of these values will be explained in detail in the following steps.\n",
    "\n",
    "```Python\n",
    "!pip install openai==1.55.3 --quiet\n",
    "!pip install llama-index --quiet\n",
    "!pip install llama-index-readers-wikipedia wikipedia --quiet\n",
    "!pip install llama-index-llms-openai --quiet\n",
    "!pip install llama-index-embeddings-huggingface --quiet\n",
    "!pip install packaging==23.2 openai --quiet\n",
    "!pip install langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n",
    "\n",
    "!pip install blingfire beautifulsoup4 sentence-transformers ray --quiet\n",
    "!pip install textwrap3 --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "```\n",
    "```Python\n",
    "import numpy as np\n",
    "import ray\n",
    "import bz2\n",
    "import json\n",
    "import torch\n",
    "from blingfire import text_to_sentences_and_offsets\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #copy your api key\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, get_response_synthesizer\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "import textwrap\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Define the number of context sentences to consider for generating an answer.\n",
    "NUM_CONTEXT_SENTENCES = 20\n",
    "# Set the maximum length for each context sentence (in characters).\n",
    "MAX_CONTEXT_SENTENCE_LENGTH = 1000\n",
    "# Set the maximum context references length (in characters).\n",
    "MAX_CONTEXT_REFERENCES_LENGTH = 4000\n",
    "# Sentence Transformer Parameters\n",
    "SENTENTENCE_TRANSFORMER_BATCH_SIZE = 128 # TUNE THIS VARIABLE depending on the size of your embedding model and GPU mem available\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fTJAIMKnOfe0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-embeddings-huggingface 0.6.1 requires llama-index-core<0.15,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index 0.12.2 requires llama-index-core<0.13.0,>=0.12.2, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-llms-openai 0.3.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-multi-modal-llms-openai 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-agent-openai 0.4.8 requires llama-index-core<0.13,>=0.12.18, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-cli 0.4.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-embeddings-openai 0.3.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.8.0 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-program-openai 0.3.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-question-gen-openai 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-readers-file 0.4.11 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-readers-llama-parse 0.4.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-readers-wikipedia 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-tools-mcp 0.2.0 requires llama-index-core<0.13,>=0.12.37, but you have llama-index-core 0.14.13 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "!pip install openai==1.55.3 --quiet\n",
    "!pip install llama-index --quiet\n",
    "!pip install llama-index-readers-wikipedia wikipedia --quiet\n",
    "!pip install llama-index-llms-openai --quiet\n",
    "!pip install llama-index-embeddings-huggingface --quiet\n",
    "!pip install packaging==23.2 openai --quiet\n",
    "!pip install langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n",
    "\n",
    "!pip install blingfire beautifulsoup4 sentence-transformers ray --quiet\n",
    "!pip install textwrap3 --quiet\n",
    "!pip install scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8Ig_PSVGW_G"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import bz2\n",
    "import json\n",
    "from blingfire import text_to_sentences_and_offsets\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\" #copy your api key\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, get_response_synthesizer\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OcrU0dgdjLIE"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Define the number of context sentences to consider for generating an answer.\n",
    "NUM_CONTEXT_SENTENCES = 20\n",
    "# Set the maximum length for each context sentence (in characters).\n",
    "MAX_CONTEXT_SENTENCE_LENGTH = 1000\n",
    "# Set the maximum context references length (in characters).\n",
    "MAX_CONTEXT_REFERENCES_LENGTH = 4000\n",
    "# Sentence Transformer Parameters\n",
    "SENTENTENCE_TRANSFORMER_BATCH_SIZE = 128 # TUNE THIS VARIABLE depending on the size of your embedding model and GPU mem available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ouB4AuyGWsk"
   },
   "source": [
    "### 2. Implementing a Chunk Extractor\n",
    "\n",
    "This time, we will define and use a `Chunk Extractor`. As you observed during the first practice session, the Chunk Extractor is a function needed to split the search results into appropriately sized pieces for use.\n",
    "\n",
    "Since search results are essentially `HTML` files, we will first define the `parse_htmls` function to remove HTML tags. Then, we will define the `extract_chunks` function, which splits the text extracted from the HTML into chunks. To avoid losing information at the chunk boundaries, the text will be split at the sentence level.\n",
    "\n",
    "```Python\n",
    "def parse_htmls(search_results):\n",
    "    all_documents = []\n",
    "    \n",
    "    # Process each HTML text from the search results to extract text content.\n",
    "    for html_text in search_results:\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_text[\"page_result\"], features=\"lxml\")\n",
    "        text = soup.get_text(\" \", strip=True)  # Use space as a separator, strip whitespaces\n",
    "        all_documents.append(text)\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "def extract_chunks(all_documents):\n",
    "    # Initialize a list to hold all extracted sentences from the search results.\n",
    "    all_chunks = []\n",
    "\n",
    "    for document in all_documents:\n",
    "\n",
    "        if not document:\n",
    "            # If no document is extracted, add an empty string as a placeholder.\n",
    "            all_chunks.append(\"\")\n",
    "        else:\n",
    "\n",
    "            # Extract offsets of sentences from the document\n",
    "            _, offsets = text_to_sentences_and_offsets(document)\n",
    "\n",
    "            # Initialize a list to store sentences\n",
    "            chunks = []\n",
    "\n",
    "            # Iterate through the list of offsets and extract sentences\n",
    "            for start, end in offsets:\n",
    "                # Extract the sentence and limit its length\n",
    "                chunk = document[start:end][:MAX_CONTEXT_SENTENCE_LENGTH]\n",
    "                all_chunks.append(chunk)\n",
    "\n",
    "    return all_chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lpCFzMHMK9rV"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "def parse_htmls(search_results):\n",
    "    all_documents = []\n",
    "\n",
    "    # Process each HTML text from the search results to extract text content.\n",
    "    for html_text in search_results:\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_text[\"page_result\"], features=\"lxml\")\n",
    "        text = soup.get_text(\" \", strip=True)  # Use space as a separator, strip whitespaces\n",
    "        all_documents.append(text)\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "def extract_chunks(all_documents):\n",
    "    # Initialize a list to hold all extracted sentences from the search results.\n",
    "    all_chunks = []\n",
    "\n",
    "    for document in all_documents:\n",
    "\n",
    "        if not document:\n",
    "            # If no document is extracted, add an empty string as a placeholder.\n",
    "            all_chunks.append(\"\")\n",
    "        else:\n",
    "\n",
    "            # Extract offsets of sentences from the document\n",
    "            _, offsets = text_to_sentences_and_offsets(document)\n",
    "\n",
    "            # Initialize a list to store sentences\n",
    "            chunks = []\n",
    "\n",
    "            # Iterate through the list of offsets and extract sentences\n",
    "            for start, end in offsets:\n",
    "                # Extract the sentence and limit its length\n",
    "                chunk = document[start:end][:MAX_CONTEXT_SENTENCE_LENGTH]\n",
    "                all_chunks.append(chunk)\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onEEe6nv-J4A"
   },
   "source": [
    "Now, let‚Äôs use the two functions to process the data by loading the search results and splitting them into chunks.\n",
    "\n",
    "Run the code below to test the example:\n",
    "\n",
    "```Python\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        \n",
    "        # Get documents\n",
    "        all_documents = parse_htmls(item[\"search_results\"])\n",
    "        \n",
    "        # Get chunks\n",
    "        all_chunks = extract_chunks(all_documents)\n",
    "        \n",
    "        print(\"=========== Document ===========\")\n",
    "        print(\"# of Document Characters: \", len(all_documents[0]))\n",
    "        print()\n",
    "        print(all_documents[0])\n",
    "        print()\n",
    "        print(\"=========== Chunk ===========\")\n",
    "        print(\"# of Chunk Characters: \", len(all_chunks[0]))\n",
    "        print()\n",
    "        print(all_chunks[0])\n",
    "        print()\n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OW7a67_BkYaB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Document ===========\n",
      "# of Document Characters:  28692\n",
      "\n",
      "Steve Nash Stats, Height, Weight, Position, Draft Status and more | Basketball-Reference.com Sports¬†Reference‚ÄØ¬Æ Baseball Football (college) Basketball (college) Hockey Calcio Blog Stathead‚ÄØ¬Æ Immaculate Grid Questions or Comments? Welcome ¬∑ Your Account Logout Ad-Free Login Create Account MENU Players Teams Seasons Leaders Scores WNBA Draft Stathead Newsletter Full Site Menu Below You are here: BBR Home Page > Players > N > Steve Nash Welcome ¬∑ Your Account Logout Ad-Free Login Create Account Steve Nash Stephen John Nash ‚ñ™ Twitter : SteveNash (MVSteve, Two Time, Nashty) Position: Point Guard\n",
      "\n",
      "\n",
      "  \n",
      "  ‚ñ™ Shoots: Right 6-3 , 195lb (190cm,¬†88kg) Born: February 7 , 1974 in¬†Johannesburg, South Africa za College: Santa Clara High School: Saint Michaels University School in Victoria, Canada Draft: Phoenix Suns , 1st round (15th pick, 15th overall), 1996 NBA Draft NBA Debut: November 1, 1996 Hall of Fame: Inducted as Player in 2018 ( Full List ) Career Length: 18 years More bio, uniform, draft, salary info Hall of Fame 8x All Star 5x AST Champ 7x All-NBA 2x MVP NBA 75th Anniv. Team 13 13 13 13 10 +4 SUMMARY Career G 1217 PTS 14.3 TRB 3.0 AST 8.5 FG% 49.0 FG3% 42.8 FT% 90.4 eFG% 55.6 PER 20.0 WS 129.7 Steve Nash Overview More Nash Pages Game Logs 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Career Playoffs Splits 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Career Shooting 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Lineups 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 On/Off 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Other Steve Nash Pages Game Finder Streak Finder Span Finder Shot Finder Event Finder Quarter Finder Teammates & Opponents Compare Steve Nash to other players More Steve Nash pages at Sports Reference International Stats at Basketball-Reference.com College Basketball at Sports-Reference.com Game Logs Game-by-game stat line for the player 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Career Playoffs Splits Player stats broken down into various categories; i.e. home/away, monthly, etc... 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Career Shooting Player shooting history 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Lineups Player lineups 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 On/Off Player on/off 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 More More Steve Nash Basketball Reference pages Game Finder Streak Finder Span Finder Shot Finder Event Finder Quarter Finder Teammates & Opponents More Steve Nash pages at Sports Reference International Stats at Basketball-Reference.com College Basketball at Sports-Reference.com Compare Steve Nash to other players On this page: Per Game Totals Per 36 Minutes Per 100 Poss Advanced Adjusted Shooting Play-by-Play Shooting Game Highs Playoffs Series All-Star Games Similarity Scores College Stats Leaderboards, Awards, & Honors Transactions Salaries Frequently Asked Questions Name + \"Statistics\" Translations Full Site Menu Per Game Bold indicates league leader Per Game Bold indicates league leader Regular Season Playoffs Per Game Table Season Age Tm Lg Pos G GS MP FG FGA FG% 3P 3PA 3P% 2P 2PA 2P% eFG% FT FTA FT% ORB DRB TRB AST STL BLK TOV PF PTS Awards 1996-97 22 PHO NBA PG 65 2 10.5 1.1 2.7 .423 0.4 0.8 .418 0.8 1.8 .425 .489 0.6 0.8 .824 0.2 0.7 1.0 2.1 0.3 0.0 1.0 1.4 3.3 1997-98 23 PHO NBA PG 76 9 21.9 3.5 7.7 .459 1.1 2.6 .415 2.5 5.1 .481 .528 1.0 1.1 .860 0.4 1.7 2.1 3.4 0.8 0.1 1.3 1.9 9.1 1998-99 24 DAL NBA PG 40 40 31.7 2.9 7.9 .363 1.2 3.3 .374 1.6 4.6 .355 .441 1.0 1.2 .826 0.8 2.1 2.9 5.5 0.9 0.1 2.1 2.5 7.9 1999-00 25 DAL NBA PG 56 27 27.4 3.1 6.5 .477 1.1 2.7 .403 2.0 3.8 .528 .559 1.3 1.5 .882 0.6 1.6 2.2 4.9 0.7 0.1 1.8 2.2 8.6 2000-01 26 DAL NBA PG 70 70 34.1 5.5 11.3 .487 1.3 3.1 .406 4.2 8.2 .518 .544 3.3 3.7 .895 0.7 2.5 3.2 7.3 1.0 0.1 2.9 2.3 15.6 MIP-3 2001-02 27 DAL NBA PG 82 82 34.6 6.4 13.3 .483 1.9 4.2 .455 4.5 9.1 .495 .554 3.2 3.6 .887 0.6 2.5 3.1 7.7 0.6 0.0 2.8 2.0 17.9 MVP-14 , NBA3 , AS , MIP-3 2002-03 28 DAL NBA PG 82 82 33.1 6.3 13.6 .465 1.4 3.3 .413 5.0 10.3 .482 .515 3.8 4.1 .909 0.8 2.1 2.9 7.3 1.0 0.1 2.3 1.6 17.7 MVP-11 , NBA3 , AS , MIP-26 2003-04 29 DAL NBA PG 78 78 33.5 5.1 10.8 .470 1.3 3.3 .405 3.8 7.5 .498 .531 2.9 3.2 .916 0.8 2.2 3.0 8.8 0.9 0.1 2.7 1.8 14.5 2004-05 30 PHO NBA PG 75 75 34.3 5.7 11.4 .502 1.3 2.9 .431 4.5 8.5 .526 .557 2.8 3.2 .887 0.8 2.6 3.3 11.5 1.0 0.1 3.3 1.8 15.5 MVP-1 , NBA1 , AS 2005-06 31 PHO NBA PG 79 79 35.4 6.8 13.4 .512 1.9 4.3 .439 4.9 9.0 .548 .583 3.3 3.5 .921 0.6 3.6 4.2 10.5 0.8 0.2 3.5 1.5 18.8 MVP-1 , NBA1 , AS 2006-07 32 PHO NBA PG 76 76 35.3 6.8 12.8 .532 2.1 4.5 .455 4.8 8.3 .575 .613 2.9 3.3 .899 0.4 3.1 3.5 11.6 0.8 0.1 3.8 1.5 18.6 MVP-2 , NBA1 , AS 2007-08 33 PHO NBA PG 81 81 34.3 6.0 11.9 .504 2.2 4.7 .470 3.8 7.2 .527 .597 2.7 3.0 .906 0.3 3.1 3.5 11.1 0.7 0.1 3.6 1.4 16.9 MVP-9 , NBA2 , AS 2008-09 34 PHO NBA PG 74 74 33.6 5.8 11.5 .503 1.5 3.3 .439 4.3 8.2 .529 .566 2.6 2.8 .933 0.3 2.8 3.0 9.7 0.7 0.1 3.4 1.5 15.7 2009-10 35 PHO NBA PG 81 81 32.8 6.2 12.2 .507 1.5 3.6 .426 4.6 8.6 .540 .570 2.6 2.8 .938 0.4 2.9 3.3 11.0 0.5 0.1 3.6 1.3 16.5 MVP-8 , NBA2 , AS 2010-11 36 PHO NBA PG 75 75 33.3 5.3 10.8 .492 1.1 2.7 .395 4.2 8.1 .525 .542 3.0 3.3 .912 0.5 2.9 3.5 11.4 0.6 0.1 3.5 1.2 14.7 2011-12 37 PHO NBA PG 62 62 31.6 4.8 9.0 .532 0.9 2.3 .390 3.9 6.7 .580 .581 2.0 2.3 .894 0.4 2.6 3.0 10.7 0.6 0.1 3.7 0.9 12.5 MVP-9 , AS 2012-13 38 LAL NBA PG 50 50 32.5 4.7 9.5 .497 1.1 2.6 .438 3.6 6.9 .519 .557 2.1 2.3 .922 0.5 2.3 2.8 6.7 0.6 0.1 2.5 1.4 12.7 2013-14 39 LAL NBA PG 15 10 20.9 2.4 6.3 .383 0.5 1.6 .333 1.9 4.7 .400 .426 1.5 1.6 .917 0.3 1.7 1.9 5.7 0.5 0.1 2.1 1.2 6.8 Career NBA 1217 1053 31.3 5.2 10.6 .490 1.4 3.2 .428 3.8 7.4 .518 .556 2.5 2.8 .904 0.5 2.5 3.0 8.5 0.7 0.1 2.9 1.6 14.3 10 seasons PHO NBA 744 614 30.6 5.3 10.5 .504 1.4 3.2 .435 3.9 7.2 .535 .571 2.4 2.7 .907 0.4 2.6 3.1 9.4 0.7 0.1 3.1 1.5 14.4 6 seasons DAL NBA 408 379 32.7 5.2 11.1 .468 1.4 3.4 .416 3.8 7.7 .490 .531 2.8 3.1 .898 0.7 2.2 2.9 7.2 0.9 0.1 2.5 2.0 14.6 2 seasons LAL NBA 65 60 29.8 4.2 8.8 .478 1.0 2.4 .422 3.2 6.4 .499 .535 2.0 2.2 .921 0.5 2.2 2.6 6.4 0.6 0.1 2.4 1.4 11.4 Per Game Table Season Age Tm Lg Pos G GS MP FG FGA FG% 3P 3PA 3P% 2P 2PA 2P% eFG% FT FTA FT% ORB DRB TRB AST STL BLK TOV PF PTS 1996-97 22 PHO NBA PG 4 0 3.8 0.5 2.3 .222 0.3 1.0 .250 0.3 1.3 .200 .278 0.0 0.0 0.3 0.0 0.3 0.3 0.3 0.3 0.5 1.3 1.3 1997-98 23 PHO NBA PG 4 1 12.8 2.0 4.5 .444 0.3 1.3 .200 1.8 3.3 .538 .472 1.3 2.0 .625 0.5 2.0 2.5 1.8 0.5 0.0 0.8 1.8 5.5 2000-01 26 DAL NBA PG 10 10 37.0 4.5 10.8 .417 1.6 3.9 .410 2.9 6.9 .420 .491 3.0 3.4 .882 0.6 2.6 3.2 6.4 0.6 0.1 2.5 1.9 13.6 2001-02 27 DAL NBA PG 8 8 40.4 6.4 14.8 .432 2.5 5.6 .444 3.9 9.1 .425 .517 4.3 4.4 .971 0.9 3.1 4.0 8.8 0.5 0.0 3.8 2.5 19.5 2002-03 28 DAL NBA PG 20 20 36.5 5.8 12.9 .447 1.9 3.8 .487 3.9 9.1 .431 .519 2.8 3.2 .873 0.8 2.8 3.5 7.3 0.9 0.1 2.6 2.4 16.1 2003-04 29 DAL NBA PG 5 5 39.4 5.4 14.0 .386 1.2 3.2 .375 4.2 10.8 .389 .429 1.6 1.8 .889 1.2 4.0 5.2 9.0 0.8 0.0 2.4 2.8 13.6 2004-05 30 PHO NBA PG 15 15 40.7 9.3 17.9 .520 1.4 3.6 .389 7.9 14.3 .553 .559 3.8 4.1 .919 0.6 4.2 4.8 11.3 0.9 0.2 4.7 2.4 23.9 2005-06 31 PHO NBA PG 20 20 39.9 7.3 14.6 .502 1.6 4.4 .368 5.7 10.2 .559 .557 4.2 4.6 .912 0.5 3.2 3.7 10.2 0.4 0.3 3.4 1.9 20.4 2006-07 32 PHO NBA PG 11 11 37.5 6.7 14.5 .463 1.7 3.5 .487 5.0 11.0 .455 .522 3.7 4.2 .891 0.4 2.8 3.2 13.3 0.4 0.1 4.4 1.9 18.9 2007-08 33 PHO NBA PG 5 5 36.6 6.4 14.0 .457 1.2 4.0 .300 5.2 10.0 .520 .500 2.2 2.4 .917 0.2 2.6 2.8 7.8 0.4 0.2 2.4 2.0 16.2 2009-10 35 PHO NBA PG 16 16 33.7 6.2 11.9 .518 1.2 3.1 .380 5.0 8.8 .567 .568 4.2 4.7 .893 0.4 2.9 3.3 10.1 0.3 0.1 3.8 1.3 17.8 2012-13 38 LAL NBA PG 2 2 30.5 5.0 11.5 .435 0.0 1.5 .000 5.0 10.0 .500 .435 2.5 2.5 1.000 1.0 1.5 2.5 4.5 0.0 0.0 1.5 1.0 12.5 Career NBA 120 113 35.7 6.2 13.2 .473 1.5 3.7 .406 4.8 9.6 .498 .529 3.3 3.7 .900 0.6 3.0 3.5 8.8 0.6 0.1 3.2 2.0 17.3 7 seasons PHO NBA 75 68 34.8 6.7 13.4 .497 1.3 3.5 .382 5.4 10.0 .537 .546 3.5 3.9 .898 0.4 3.0 3.4 9.7 0.5 0.2 3.5 1.8 18.2 4 seasons DAL NBA 43 43 37.7 5.5 12.9 .430 1.8 4.1 .449 3.7 8.8 .422 .502 3.0 3.3 .901 0.8 2.9 3.7 7.5 0.7 0.0 2.7 2.3 15.9 1 season LAL NBA 2 2 30.5 5.0 11.5 .435 0.0 1.5 .000 5.0 10.0 .500 .435 2.5 2.5 1.000 1.0 1.5 2.5 4.5 0.0 0.0 1.5 1.0 12.5 POWERED BY Steve Nash is 1 of 4 players 6'3\" or under with 10,000 career points and a True Shooting Percentage of .600 or greater. Can you name the other 3? Subscribe to Stathead , your all-access pass to the Basketball Reference database, to answer more questions like this. Become¬†a¬†Stathead View on stats.nba.com Player Front Shooting Advanced Tracking: Shots Rebounds Passes Defense Insights Table Highlight In Stathead Career high, Points 42 View full stats from top 20 games Career high, Rebounds 13 View full stats from top 20 games Career high, Assists 22 View full stats from top 20 games Career high, Steals 5 View full stats from top 20 games Career high, Blocks 2 View full stats from top 20 games Career high, Game Score 38.6 View full stats from top 20 games Triple-Doubles 3 View all Use Stathead to compare Steve Nash to: Javascript is required for the selection of a player. Choice is: Ex: LeBron James , Michael Jordan Career Player News Add Your Blog Posts Here Player News Archive Player News RSS Feed Steve Nash has every individual accolade an NBA player could dream of, but his teams never won a championship, never even made the Finals. Those teams include some of the best, most innovative teams ever to play the game. So ... why? What obstacles stood between Nash's teams and glory? Watch Untitled ‚Ä¢ Secret Base on Youtube 2/25 thePeachBasket: How the Boston Celtics Win the Finals in 2024 : The Boston Celtics already look like the best team in the NBA. ... 2/20 Upside Hoops: Speed Demon: Quickest Players in NBA History : When it comes to the world of basketball, speed and agility are ... 2/20 NBA Analysis Network: Nets Reveal Interim Coach Choice After Firing Jacque Vaughn : On Monday, the Brooklyn Nets announced the dismissal of head coach ... 2/19 HoopsWire: East Notes: Pacers, Tyrese Haliburton, Bucks, Nets : Pacers Star point guard Tyrese Haliburton admitted he was angry ... 2/19 Last Word on Sports: NBA News: Brooklyn Nets Fire Jacque Vaughn : The Brooklyn Nets have fired head coach Jacque Vaughn, per the ... 1/27 Basketball Reference: The Greatest Point Guards of all time : The point guard (PG), often called as the ‚Äòfloor general‚Äô ... Totals Bold indicates league leader Totals Bold indicates league leader Regular Season Playoffs Totals Table Season Age Tm Lg Pos G GS MP FG FGA FG% 3P 3PA 3P% 2P 2PA 2P% eFG% FT FTA FT% ORB DRB TRB AST STL BLK TOV PF PTS Trp-Dbl 1996-97 22 PHO NBA PG 65 2 684 74 175 .423 23 55 .418 51 120 .425 .489 42 51 .824 16 47 63 138 20 0 63 92 213 0 1997-98 23 PHO NBA PG 76 9 1664 268 584 .459 81 195 .415 187 389 .481 .528 74 86 .860 32 128 160 262 63 4 98 145 691 0 1998-99 24 DAL NBA PG 40 40 1269 114 314 .363 49 131 .374 65 183 .355 .441 38 46 .826 32 82 114 219 37 2 83 98 315 0 1999-00 25 DAL NBA PG 56 27 1532 173 363 .477 60 149 .403 113 214 .528 .559 75 85 .882 34 87 121 272 37 3 102 122 481 0 2000-01 26 DAL NBA PG 70 70 2387 386 792 .487 89 219 .406 297 573 .518 .544 231 258 .895 46 177 223 509 72 5 205 158 1092 0 2001-02 27 DAL NBA PG 82 82 2837 525 1088 .483 156 343 .455 369 745 .495 .554 260 293 .887 50 204 254 634 53 4 229 164 1466 0 2002-03 28 DAL NBA PG 82 82 2711 518 1114 .465 111 269 .413 407 845 .482 .515 308 339 .909 63 171 234 598 85 6 192 134 1455 0 2003-04 29 DAL NBA PG 78 78 2612 397 845 .470 104 257 .405 293 588 .498 .531 230 251 .916 59 173 232 687 67 8 209 139 1128 1 2004-05 30 PHO NBA PG 75 75 2573 430 857 .502 94 218 .431 336 639 .526 .557 211 238 .887 57 192 249 861 74 6 245 136 1165 1 2005-06 31 PHO NBA PG 79 79 2796 541 1056 .512 150 342 .439 391 714 .548 .583 257 279 .921 47 286 333 826 61 12 276 120 1489 1 2006-07 32 PHO NBA PG 76 76 2682 517 971 .532 156 343 .455 361 628 .575 .613 222 247 .899 30 239 269 884 57 6 287 117 1412 0 2007-08 33 PHO NBA PG 81 81 2780 485 962 .504 179 381 .470 306 581 .527 .597 222 245 .906 28 254 282 898 53 5 295 113 1371 0 2008-09 34 PHO NBA PG 74 74 2484 428 851 .503 108 246 .439 320 605 .529 .566 196 210 .933 19 204 223 717 55 10 248 108 1160 0 2009-10 35 PHO NBA PG 81 81 2660 499 985 .507 124 291 .426 375 694 .540 .570 211 225 .938 34 234 268 892 42 12 295 108 1333 0 2010-11 36 PHO NBA PG 75 75 2497 399 811 .492 81 205 .395 318 606 .525 .542 227 249 .912 40 220 260 855 48 4 265 87 1106 0 2011-12 37 PHO NBA PG 62 62 1961 295 555 .532 55 141 .390 240 414 .580 .581 127 142 .894 26 160 186 664 38 8 229 53 772 0 2012-13 38 LAL NBA PG 50 50 1627 236 475 .497 57 130 .438 179 345 .519 .557 107 116 .922 26 116 142 333 30 5 126 70 636 0 2013-14 39 LAL NBA PG 15 10 313 36 94 .383 8 24 .333 28 70 .400 .426 22 24 .917 4 25 29 86 7 2 31 18 102 0 Career NBA 1217 1053 38069 6321 12892 .490 1685 3939 .428 4636 8953 .518 .556 3060 3384 .904 643 2999 3642 10335 899 102 3478 1982 17387 3 10 seasons PHO NBA 744 614 22781 3936 7807 .504 1051 2417 .435 2885 5390 .535 .571 1789 1972 .907 329 1964 2293 6997 511 67 2301 1079 10712 2 6 seasons DAL NBA 408 379 13348 2113 4516 .468 569 1368 .416 1544 3148 .490 .531 1142 1272 .898 284 894 1178 2919 351 28 1020 815 5937 1 2 seasons LAL NBA 65 60 1940 272 569 .478 65 154 .422 207 415 .499 .535 129 140 .921 30 141 171 419 37 7 157 88 738 0 Totals Table Season Age Tm Lg Pos G GS MP FG FGA FG% 3P 3PA 3P% 2P 2PA 2P% eFG% FT FTA FT% ORB DRB TRB AST STL BLK TOV PF PTS Trp-Dbl 1996-97 22 PHO NBA PG 4 0 15 2 9 .222 1 4 .250 1 5 .200 .278 0 0 1 0 1 1 1 1 2 5 5 0 1997-98 23 PHO NBA PG 4 1 51 8 18 .444 1 5 .200 7 13 .538 .472 5 8 .625 2 8 10 7 2 0 3 7 22 0 2000-01 26 DAL NBA PG 10 10 370 45 108 .417 16 39 .410 29 69 .420 .491 30 34 .882 6 26 32 64 6 1 25 19 136 0 2001-02 27 DAL NBA PG 8 8 323 51 118 .432 20 45 .444 31 73 .425 .517 34 35 .971 7 25 32 70 4 0 30 20 156 0 2002-03 28 DAL NBA PG 20 20 729 115 257 .447 37 76 .487 78 181 .431 .519 55 63 .873 15 55 70 145 17 1 51 47 322 0 2003-04 29 DAL NBA PG 5 5 197 27 70 .386 6 16 .375 21 54 .389 .429 8 9 .889 6 20 26 45 4 0 12 14 68 0 2004-05 30 PHO NBA PG 15 15 610 140 269 .520 21 54 .389 119 215 .553 .559 57 62 .919 9 63 72 170 14 3 70 36 358 1 2005-06 31 PHO NBA PG 20 20 798 146 291 .502 32 87 .368 114 204 .559 .557 83 91 .912 9 64 73 204 8 5 67 38 407 0 2006-07 32 PHO NBA PG 11 11 413 74 160 .463 19 39 .487 55 121 .455 .522 41 46 .891 4 31 35 146 4 1 48 21 208 0 2007-08 33 PHO NBA PG 5 5 183 32 70 .457 6 20 .300 26 50 .520 .500 11 12 .917 1 13 14 39 2 1 12 10 81 0 2009-10 35 PHO NBA PG 16 16 539 99 191 .518 19 50 .380 80 141 .567 .568 67 75 .893 6 46 52 161 4 1 60 20 284 0 2012-13 38 LAL NBA PG 2 2 61 10 23 .435 0 3 .000 10 20 .500 .435 5 5 1.000 2 3 5 9 0 0 3 2 25 0 Career NBA 120 113 4289 749 1584 .473 178 438 .406 571 1146 .498 .529 396 440 .900 68 354 422 1061 66 14 383 239 2072 1 7 seasons PHO NBA 75 68 2609 501 1008 .497 99 259 .382 402 749 .537 .546 264 294 .898 32 225 257 728 35 12 262 137 1365 1 4 seasons DAL NBA 43 43 1619 238 553 .430 79 176 .449 159 377 .422 .502 127 141 .901 34 126 160 324 31 2 118 100 682 0 1 season LAL NBA 2 2 61 10 23 .435 0 3 .000 10 20 .500 .435 5 5 1.000 2 3 5 9 0 0 3 2 25 0 Advanced Bold indicates league leader Advanced Bold indicates league leader Regular Season Playoffs Advanced Table Season Age Tm Lg Pos G MP PER TS% 3PAr FTr ORB% DRB% TRB% AST% STL% BLK% TOV% USG% OWS DWS WS WS/48 OBPM DBPM BPM VORP 1996-97 22 PHO NBA PG 65 684 10.8 .539 .314 .291 2.8 8.0 5.4 29.5 1.5 0.0 24.2 17.1 0.4 0.3 0.7 .047 -2.0 -1.1 -3.1 -0.2 1997-98 23 PHO NBA PG 76 1664 15.6 .556 .334 .147 2.3 8.8 5.6 25.2 2.0 0.2 13.6 19.6 2.9 1.8 4.8 .137 1.1 0.3 1.4 1.4 1998-99 24 DAL NBA PG 40 1269 10.9 .471 .417 .146 2.8 7.3 5.0 27.2 1.6 0.1 19.9 15.1 0.6 0.3 1.0 .037 -1.0 -1.4 -2.4 -0.1 1999-00 25 DAL NBA PG 56 1532 13.5 .601 .410 .234 2.4 6.1 4.2 25.5 1.2 0.1 20.3 14.5 2.8 0.2 3.0 .094 0.6 -1.5 -0.9 0.4 2000-01 26 DAL NBA PG 70 2387 19.6 .603 .277 .326 2.2 8.1 5.2 34.6 1.6 0.1 18.5 21.1 6.8 1.6 8.4 .169 3.8 -1.1 2.8 2.9 2001-02 27 DAL NBA PG 82 2837 20.7 .602 .315 .269 2.0 7.9 5.0 36.1 1.0 0.1 15.8 23.0 9.5 0.3 9.9 .167 5.1 -2.1 3.1 3.6 2002-03 28 DAL NBA PG 82 2711 22.6 .576 .241 .304 2.6 7.0 4.8 36.3 1.6 0.2 13.2 24.4 9.7 1.9 11.6 .206 5.4 -0.8 4.6 4.5 2003-04 29 DAL NBA PG 78 2612 20.5 .590 .304 .297 2.4 7.4 4.8 38.3 1.3 0.2 17.9 19.5 8.9 -0.1 8.8 .162 4.7 -2.2 2.5 3.0 2004-05 30 PHO NBA PG 75 2573 22.0 .606 .254 .278 2.5 7.6 5.2 49.2 1.4 0.2 20.3 20.5 9.7 1.2 10.9 .203 5.5 -0.8 4.7 4.4 2005-06 31 PHO NBA PG 79 2796 23.3 .632 .324 .264 1.9 11.1 6.6 44.4 1.1 0.3 19.0 23.3 10.3 2.1 12.4 .212 5.7 -0.7 5.0 4.9 2006-07 32 PHO NBA PG 76 2682 23.8 .654 .353 .254 1.4 9.9 5.8 50.1 1.1 0.2 21.0 22.9 10.8 1.7 12.6 .225 6.7 -0.8 5.9 5.3 2007-08 33 PHO NBA PG 81 2780 21.1 .641 .396 .255 1.2 9.6 5.7 47.3 0.9 0.1 21.6 22.0 9.0 1.4 10.5 .181 5.8 -1.8 3.9 4.2 2008-09 34 PHO NBA PG 74 2484 19.5 .615 .289 .247 0.9 9.2 5.2 42.4 1.1 0.3 20.8 21.1 6.7 0.5 7.3 .140 4.5 -2.1 2.4 2.8 2009-10 35 PHO NBA PG 81 2660 21.6 .615 .295 .228 1.5 9.4 5.7 50.9 0.8 0.3 21.4 22.9 9.1 0.8 9.9 .178 6.1 -1.8 4.3 4.2 2010-11 36 PHO NBA PG 75 2497 20.8 .601 .253 .307 1.8 10.2 6.0 53.1 1.0 0.1 22.4 21.4 7.3 0.7 7.9 .153 4.7 -1.5 3.2 3.3 2011-12 37 PHO NBA PG 62 1961 20.3 .625 .254 .256 1.5 9.1 5.3 53.1 1.0 0.3 27.1 19.6 5.2 0.6 5.9 .144 4.7 -1.2 3.4 2.7 2012-13 38 LAL NBA PG 50 1627 16.0 .605 .274 .244 1.8 7.7 4.8 32.8 0.9 0.2 19.3 17.8 3.7 0.6 4.3 .127 1.8 -1.4 0.4 1.0 2013-14 39 LAL NBA PG 15 313 12.2 .488 .255 .255 1.4 8.5 4.9 40.3 1.1 0.5 22.9 18.9 0.1 0.0 0.1 .017 -1.3 -2.3 -3.6 -0.1 Career NBA 1217 38069 20.0 .605 .306 .262 1.9 8.6 5.4 41.5 1.2 0.2 19.5 21.0 113.7 16.1 129.7 .164 4.4 -1.3 3.0 48.2 10 seasons PHO NBA 744 22781 20.8 .617 .310 .253 1.7 9.4 5.7 46.4 1.1 0.2 21.0 21.5 71.5 11.2 82.7 .174 5.0 -1.2 3.7 33.1 6 seasons DAL NBA 408 13348 19.1 .585 .303 .282 2.3 7.4 4.9 34.2 1.4 0.2 16.7 20.5 38.4 4.2 42.7 .153 3.8 -1.5 2.2 14.3 2 seasons LAL NBA 65 1940 15.4 .585 .271 .246 1.7 7.8 4.8 34.0 1.0 0.3 19.9 18.0 3.8 0.7 4.4 .109 1.3 -1.5 -0.3 0.9 Advanced Table Season Age Tm Lg Pos G MP PER TS% 3PAr FTr ORB% DRB% TRB% AST% STL% BLK% TOV% USG% OWS DWS WS WS/48 OBPM DBPM BPM VORP 1996-97 22 PHO NBA PG 4 15 -5.1 .278 .444 .000 7.7 0.0 3.8 11.7 3.4 5.4 18.2 32.4 -0.1 0.0 -0.1 -0.381 -10.0 -2.4 -12.4 0.0 1997-98 23 PHO NBA PG 4 51 16.5 .511 .278 .444 4.1 19.4 11.1 22.4 2.1 0.0 12.2 21.9 0.1 0.0 0.1 .081 0.6 1.8 2.4 0.1 2000-01 26 DAL NBA PG 10 370 15.1 .553 .361 .315 1.7 8.2 4.8 31.5 0.9 0.2 16.9 18.5 0.6 0.0 0.6 .082 2.9 -1.2 1.7 0.3 2001-02 27 DAL NBA PG 8 323 18.7 .585 .381 .297 2.3 8.2 5.3 32.9 0.6 0.0 18.4 22.0 0.9 -0.2 0.7 .110 4.7 -2.4 2.3 0.3 2002-03 28 DAL NBA PG 20 729 18.1 .565 .296 .245 2.2 8.3 5.3 31.1 1.2 0.1 15.2 20.7 1.8 -0.1 1.7 .113 3.9 -1.7 2.2 0.7 2003-04 29 DAL NBA PG 5 197 15.5 .460 .229 .129 2.6 11.0 6.3 34.7 1.0 0.0 14.0 17.4 0.4 0.0 0.4 .092 1.1 -1.1 0.0 0.1 2004-05 30 PHO NBA PG 15 610 23.4 .604 .201 .230 1.7 11.0 6.6 43.9 1.2 0.3 19.1 26.6 2.1 0.0 2.1 .164 5.9 -1.2 4.7 1.0 2005-06 31 PHO NBA PG 20 798 21.3 .615 .299 .313 1.4 9.4 5.5 40.3 0.5 0.4 16.8 23.5 2.5 0.1 2.6 .153 5.2 -1.5 3.7 1.1 2006-07 32 PHO NBA PG 11 413 21.9 .577 .244 .288 1.2 8.3 4.9 55.8 0.5 0.2 21.0 24.9 1.3 0.1 1.4 .165 6.4 -1.2 5.2 0.8 2007-08 33 PHO NBA PG 5 183 16.8 .538 .286 .171 0.6 8.4 4.5 36.4 0.6 0.4 13.7 22.0 0.3 0.1 0.4 .103 3.6 -1.5 2.1 0.2 2009-10 35 PHO NBA PG 16 539 22.4 .634 .262 .393 1.4 10.1 5.8 49.1 0.4 0.1 21.1 24.2 2.0 0.0 2.0 .175 6.6 -2.0 4.6 0.9 2012-13 38 LAL NBA PG 2 61 14.5 .496 .130 .217 3.7 5.9 4.8 26.6 0.0 0.0 10.6 21.6 0.1 0.0 0.0 .014 2.0 -3.0 -1.0 0.0 Career NBA 120 4289 19.8 .583 .277 .278 1.8 9.3 5.6 39.6 0.8 0.2 17.7 22.8 11.7 0.1 11.9 .133 4.7 -1.5 3.2 5.6 7 seasons PHO NBA 75 2609 21.6 .600 .257 .292 1.5 9.8 5.7 44.6 0.7 0.3 18.7 24.5 8.0 0.4 8.4 .154 5.5 -1.4 4.1 4.0 4 seasons DAL NBA 43 1619 17.2 .554 .318 .255 2.2 8.6 5.3 32.0 1.0 0.1 16.1 20.1 3.7 -0.2 3.5 .103 3.5 -1.7 1.8 1.5 1 season LAL NBA 2 61 14.5 .496 .130 .217 3.7 5.9 4.8 26.6 0.0 0.0 10.6 21.6 0.1 0.0 0.0 .014 2.0 -3.0 -1.0 0.0 Playoffs Series College Stats More College Stats on SR/CBB ¬∑ underline indicates incomplete record Appearances on Leaderboards, Awards, and Honors Transactions Salaries Frequently Asked Questions How old is Steve Nash? Steve Nash is 50 years old. Where was Steve Nash born? Steve Nash was born in Johannesburg, South Africa. When was Steve Nash born? Steve Nash was born on February 7, 1974. How tall is Steve Nash? Steve Nash is 6-3 (190 cm) tall. How much did Steve Nash weigh when playing? Steve Nash weighed 195 lbs (88 kg) when playing. Is Steve Nash in the Hall of Fame? Steve Nash was inducted to the Hall of Fame as a Player in 2018 ( Full List )\n",
      ". When was Steve Nash drafted? Steve Nash was drafted by Phoenix Suns , 1st round (15th pick, 15th overall), 1996 NBA Draft . What position did Steve Nash play? Point Guard. When did Steve Nash retire? Steve Nash last played in 2014. What is Steve Nash's net worth? Steve Nash made at least $146,936,620 playing professional basketball. How much did Steve Nash make? Steve Nash made $13,125,000 in 2010. What did Steve Nash average? Steve Nash averaged 14.3 points, 3.0 rebounds, and 8.5 assists per game. How many rings does Steve Nash have? Steve Nash won 0 championships. What is Steve Nash's Twitter account? Steve Nash is on Twitter at SteveNash . What schools did Steve Nash attend? Steve Nash attended Saint Michaels University School in Victoria, Canada and Santa Clara . What are Steve Nash's nicknames? MVSteve, Two Time, Nashty are nicknames for Steve Nash. Name + \"Statistics\" Translations Note, this is done in an automated way, so we apologize for any errors, & please report any suggested corrections. \"Statistics\" is included to allow non-English speakers to find our pages. More Nash Pages Steve Nash Overview Game Logs 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Career Playoffs Splits 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Career Shooting 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Lineups 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 On/Off 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Other Steve Nash Pages Game Finder Streak Finder Span Finder Shot Finder Event Finder Quarter Finder Teammates & Opponents Compare Steve Nash to other players More Steve Nash pages at Sports Reference International Stats at Basketball-Reference.com College Basketball at Sports-Reference.com Welcome ¬∑ Your Account Logout Ad-Free Login Create Account You are here: BBR Home Page > Players > N > Steve Nash Full Site Menu Return to Top Players In the News : V. Wembanyama , L. James , K. Durant , J. Embiid , J. Harden , S. Curry , L. Donƒçiƒá ... All-Time Greats : E. Hayes , J. Stockton , H. Olajuwon , W. Chamberlain , D. Schayes , J. Havlicek ... Active Greats : L. James , G. Antetokounmpo , C. Paul , J. Harden , S. Curry , K. Durant ... Teams Atlantic : Toronto , Boston , New York , Brooklyn , Philadelphia Central : Cleveland , Indiana , Detroit , Chicago , Milwaukee Southeast : Miami , Atlanta , Charlotte , Washington , Orlando Northwest : Oklahoma City , Portland , Utah , Denver , Minnesota Pacific : Golden State , Los Angeles Clippers , Sacramento , Phoenix , Los Angeles Lakers Southwest : San Antonio , Dallas , Memphis , Houston , New Orleans Seasons 2023-24 , 2022-23 , 2021-22 , 2020-21 , 2019-20 , 2018-19 , 2017-18 ... Leaders Season Points , Career Rebounds , Active Assists , Yearly Steals , Progressive Blocks ... Or, view \"Trailers\" for Season Field Goal Pct , or Career Blocks Per Game NBA Scores Yesterday's Games and Scores from any date in BAA/NBA or ABA history NBA Schedules Team Schedules and League Schedules NBA Standings Today's Standings and Standings for any date in history Stathead Player Finders : Season Finder , Game Finder , Streak Finder , Span Finder Team Finders : Season Finder , Game Finder , Streak Finder , Span Finder Other Finders : Versus Finder (NEW) , Shot Finder Coaches Richie Guerin , Rudy Tomjanovich , Jim O'Brien , Mike Fratello , Alvin Gentry ... Awards NBA MVP , All-NBA , Defensive Player of the Year , Rookie of the Year , All-Rookie , Hall of Fame ... NBA Contracts Main Index , Team Payrolls , Player Contracts , Glossary ... Playoffs 2023 NBA Playoffs , 2022 NBA Playoffs , 2021 NBA Playoffs , 2020 NBA Playoffs , 2019 NBA Playoffs , 2018 NBA Playoffs , 2017 NBA Playoffs , Playoffs Series History ... All-Star Games 2023 All-Star Game , 2022 All-Star Game , 2021 All-Star Game , 2020 All-Star Game , 2019 All-Star Game , 2018 All-Star Game ... NBA Draft 2023 Draft , 2022 Draft , 2021 Draft , 2020 Draft , 2019 Draft , 2018 Draft , 2017 Draft ... Frivolities Players who played for multiple teams (WNBA) , Birthdays , Colleges , High Schools , Milestone Watch ... Executives R.C. Buford , Wayne Embry , Stan Kasten , Danny Ainge , Don Nelson ... Referees Joe Forte , Tony Brothers , Dan Crawford , Ron Olesiak , David Jones ... G League Stats Players , Teams , Seasons , Leaders , Awards ... International Basketball Stats Players , Teams , Seasons , Leaders , Awards ... WNBA Players , Teams , Seasons , Leaders , Awards , All-Star Games , Executives ... NBL Players , Teams , Seasons , Leaders , Awards ... About Glossary , Contact and Media Information , Frequently Asked Questions about the NBA, WNBA and Basketball ... Immaculate Grid (Men's) and Immaculate Grid (Women's) Put your basketball knowledge to the test with our daily basketball trivia games. Can you complete the grids? Basketball-Reference.com Blog and Articles We're Social...for Statheads Every Sports Reference Social Media Account Site Last Updated: Sunday, March 10,  7:58PM Question, Comment, Feedback, or Correction? Subscribe to our Free Email Newsletter Subscribe to Stathead Basketball: Get your first month FREE Your All-Access Ticket to the Basketball Reference Database Do you have a sports website? Or write about sports? We have tools and resources that can help you use sports data.  Find out more. FAQs, Tip & Tricks Tips and Tricks from our Blog. Do you have a blog? Join our linker program. Watch our How-To Videos to Become a Stathead Subscribe to Stathead and get access to more data than you can imagine All logos are the trademark & property of their owners and not Sports Reference LLC.  We present them here for purely educational purposes. Our reasoning for presenting offensive logos. Logos were compiled by the amazing SportsLogos.net. Data Provided By the official stats partner of the NBA, NHL and MLB. Copyright ¬© 2000-2024 Sports Reference LLC . All rights reserved. The SPORTS REFERENCE and STATHEAD trademarks are owned exclusively by Sports Reference LLC. Use without license or authorization is expressly prohibited. Sports¬†Reference‚ÄØ¬Æ Baseball Football (college) Basketball (college) Hockey F√∫tbol Blog Stathead‚ÄØ¬Æ Immaculate Grid About ‚Ä¢ Conditions & Terms of Service ‚Ä¢ Advertise With Us ‚Ä¢ Jobs at SR Sports Reference Purpose: We will be the trusted source of information and tools that inspire and empower users to enjoy, understand, and share the sports they love. Privacy Policy ‚Ä¢ Gambling Revenue Policy ‚Ä¢ Accessibility Policy ‚Ä¢ Use of Data\n",
      "\n",
      "=========== Chunk ===========\n",
      "# of Chunk Characters:  229\n",
      "\n",
      "Steve Nash Stats, Height, Weight, Position, Draft Status and more | Basketball-Reference.com Sports¬†Reference‚ÄØ¬Æ Baseball Football (college) Basketball (college) Hockey Calcio Blog Stathead‚ÄØ¬Æ Immaculate Grid Questions or Comments?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"./crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "\n",
    "        # Get documents\n",
    "        all_documents = parse_htmls(item[\"search_results\"])\n",
    "\n",
    "        # Get chunks\n",
    "        all_chunks = extract_chunks(all_documents)\n",
    "\n",
    "        print(\"=========== Document ===========\")\n",
    "        print(\"# of Document Characters: \", len(all_documents[0]))\n",
    "        print()\n",
    "        print(all_documents[0])\n",
    "        print()\n",
    "        print(\"=========== Chunk ===========\")\n",
    "        print(\"# of Chunk Characters: \", len(all_chunks[0]))\n",
    "        print()\n",
    "        print(all_chunks[0])\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98LPBiYQ-NKu"
   },
   "source": [
    "As a result of the test, we observed that the length of the text was reduced by nearly **100 times** after splitting it into chunks compared to using the full search results. This suggests that chunks can effectively extract only the relevant parts from the `search_results` and pass them to the LLM efficiently.\n",
    "\n",
    "Of course, we cannot guarantee that the retriever will always retrieve relevant results. However, if we use entire documents as the retrieval unit, it will take a long time to compute embeddings, and information loss may occur during that process.\n",
    "\n",
    "Additionally, the length of the chunk characters may not exactly match the value of `MAX_CONTEXT_SENTENCE_LENGTH`. This is because we used the text_to_sentences_and_offsets function to ensure that chunks are formed without splitting sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZNPYuw0K9KC"
   },
   "source": [
    "### 3. Implementing a Retriever\n",
    "\n",
    "This time, we will implement a **Retriever** using the Chunk Extractor we defined earlier, without relying on AI frameworks like LlamaIndex.  \n",
    "\n",
    "For this implementation, the following components are required:\n",
    "\n",
    "<br/>\n",
    "\n",
    "1.\t**Chunk extractor**: Used to split the input search_results into chunks.\n",
    "2.\t**Embedding model**: Used to generate embeddings for the chunks and the query.\n",
    "3.\t**Similarity metric**: Measures the similarity between embeddings. We will use cosine similarity here.\n",
    "\n",
    "Using these components, let‚Äôs implement the `BaseRetriever` with the following code:\n",
    "\n",
    "```Python\n",
    "class BaseRetriever:\n",
    "    def __init__(self,):\n",
    "        self.client = openai.OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    def embed_text(self, texts):\n",
    "        \"\"\"Generate embeddings using OpenAI's embedding model.\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=texts\n",
    "        )\n",
    "\n",
    "        # Extract embeddings correctly from the response object\n",
    "        embeddings = [np.array(item.embedding) for item in response.data]  # Adjust based on actual attributes\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def retrieve(self, query, search_results, topk):\n",
    "        # Get documents\n",
    "        all_documents = parse_htmls(search_results)\n",
    "\n",
    "        # Get chunks\n",
    "        all_chunks = extract_chunks(all_documents)\n",
    "\n",
    "        # Generate embeddings for all chunks and the query.\n",
    "        all_embeddings = self.embed_text(all_chunks)\n",
    "        query_embedding = self.embed_text(query)[0]  # Single query embedding\n",
    "\n",
    "        # Calculate cosine similarity between query and sentence embeddings, and select the top sentences.\n",
    "        cosine_scores = np.dot(all_embeddings, query_embedding) / (\n",
    "            np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "        )\n",
    "        top_k_indices = (-cosine_scores).argsort()[:topk]\n",
    "        top_k_chunks = np.array(all_chunks)[top_k_indices]\n",
    "\n",
    "        return top_k_chunks\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D3sX7arBYmKy"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "class BaseRetriever:\n",
    "    def __init__(self,):\n",
    "        self.client = openai.OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    def embed_text(self, texts):\n",
    "        \"\"\"Generate embeddings using OpenAI's embedding model.\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=texts\n",
    "        )\n",
    "\n",
    "        # Extract embeddings correctly from the response object\n",
    "        embeddings = [np.array(item.embedding) for item in response.data]  # Adjust based on actual attributes\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def retrieve(self, query, search_results, topk):\n",
    "        # Get documents\n",
    "        all_documents = parse_htmls(search_results)\n",
    "\n",
    "        # Get chunks\n",
    "        all_chunks = extract_chunks(all_documents)\n",
    "\n",
    "        # Generate embeddings for all chunks and the query.\n",
    "        all_embeddings = self.embed_text(all_chunks)\n",
    "        query_embedding = self.embed_text(query)[0]  # Single query embedding\n",
    "\n",
    "        # Calculate cosine similarity between query and sentence embeddings, and select the top sentences.\n",
    "        cosine_scores = np.dot(all_embeddings, query_embedding) / (\n",
    "            np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "        )\n",
    "        top_k_indices = (-cosine_scores).argsort()[:topk]\n",
    "        top_k_chunks = np.array(all_chunks)[top_k_indices]\n",
    "\n",
    "        return top_k_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R9KzDF7DPtm"
   },
   "source": [
    "The retriever determines the final chunks to return through three main steps.  \n",
    "\n",
    "1.\t**First Step**: The retriever takes the query, the search_results, and a variable topk (which determines how many chunks to return) as inputs. It then extracts chunks from the `search_results`.\n",
    "2.\t**Second Step**: The extracted chunks are converted into embeddings using an embedding model. Since the chunks are in a list format, the embedding results will also be returned as a list. At the same time, the query is also converted into an embedding.\n",
    "3. **Third Step**: **Cosine similarity** between the query‚Äôs embedding and the chunks‚Äô embeddings is calculated to determine which chunks have the highest similarity to the query.\n",
    "\n",
    "Through this process, our `BaseRetriever` retrieves and returns the `topk` chunks with the highest similarity.  \n",
    "\n",
    "Here is the code to verify the process:\n",
    "\n",
    "```\n",
    "retriever = BaseRetriever()\n",
    "topk = 5\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        retrieved_results = retriever.retrieve(item['query'], item['search_results'], topk)\n",
    "        break\n",
    "\n",
    "print(\"retrieved results:\")\n",
    "print()\n",
    "for rank, retrieved_result in enumerate(retrieved_results):\n",
    "    print(f\"rank {rank+1}: {retrieved_result}\")\n",
    "    print()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5685qgMxnQPs",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: how many 3-point attempts did steve nash average per game in seasons he made the 50-40-90 club?\n",
      "\n",
      "retrieved results:\n",
      "\n",
      "rank 1: What did Steve Nash average?\n",
      "\n",
      "rank 2: In the 2005‚Äì06 season, Nash became the fourth player in NBA history to shoot 50% or better from the field, 40% from three-point range (43.9), and 90% from the line, joining Larry Bird , Reggie Miller , and Mark Price in the 50‚Äì40‚Äì90 club .\n",
      "\n",
      "rank 3: How much did Steve Nash make?\n",
      "\n",
      "rank 4: Steve Nash averaged 14.3 points, 3.0 rebounds, and 8.5 assists per game.\n",
      "\n",
      "rank 5: The catalyst of this turnaround, Nash averaged 11.5 assists per game while making 50.2% of his field goals and 43.1% of his three-pointers in the regular season.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "retriever = BaseRetriever()\n",
    "topk = 5\n",
    "dataset_path = \"./crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        retrieved_results = retriever.retrieve(item['query'], item['search_results'], topk)\n",
    "        break\n",
    "\n",
    "print(\"retrieved results:\")\n",
    "print()\n",
    "for rank, retrieved_result in enumerate(retrieved_results):\n",
    "    print(f\"rank {rank+1}: {retrieved_result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFogBOkgfTkl"
   },
   "source": [
    "### 3. Implementing a Retriever with Llama Index\n",
    "\n",
    "You may recall that in Day 1 practice, we defined a retriever using `LlamaIndex`.\n",
    "\n",
    "In this exercise, we will again define a retriever using LlamaIndex. To create a retriever with `LlamaIndex`, we must first build an index. To build the index, we need to decide which data to use ‚Äì in this case, we will use the `search_results`.\n",
    "\n",
    "Follow the code below to declare the retriever:\n",
    "\n",
    "```Python\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "class LlamaIndexRetriever:\n",
    "  def __init__(self):\n",
    "      self.parser = SentenceSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "  def retrieve(self, query, search_results, topk):\n",
    "      documents = []\n",
    "\n",
    "      for document in parse_htmls(search_results):\n",
    "        if not document:\n",
    "            # If no text is extracted, add an empty string as a placeholder.\n",
    "            documents.append(Document(text=\"\"))\n",
    "        else:\n",
    "            documents.append(Document(text=document))\n",
    "\n",
    "      # Split documents into chunks & Create vector index\n",
    "      base_index = VectorStoreIndex.from_documents(documents = documents, transformations=[self.parser])\n",
    "\n",
    "      # Execute query\n",
    "      base_retriever = base_index.as_retriever(similarity_top_k=topk)\n",
    "\n",
    "      retrieved_nodes = base_retriever.retrieve(query)\n",
    "\n",
    "      retrieved_results = [retrieved_node.node.get_content().strip() for retrieved_node in retrieved_nodes]\n",
    "\n",
    "      return retrieved_results\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ypqI8b9gmcP5"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "class LlamaIndexRetriever:\n",
    "  def __init__(self):\n",
    "      self.parser = SentenceSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "  def retrieve(self, query, search_results, topk):\n",
    "      documents = []\n",
    "\n",
    "      for document in parse_htmls(search_results):\n",
    "        if not document:\n",
    "            # If no text is extracted, add an empty string as a placeholder.\n",
    "            documents.append(Document(text=\"\"))\n",
    "        else:\n",
    "            documents.append(Document(text=document))\n",
    "\n",
    "      # Split documents into chunks & Create vector index\n",
    "      base_index = VectorStoreIndex.from_documents(documents = documents, transformations=[self.parser])\n",
    "\n",
    "      # Execute query\n",
    "      base_retriever = base_index.as_retriever(similarity_top_k=topk)\n",
    "\n",
    "      retrieved_nodes = base_retriever.retrieve(query)\n",
    "\n",
    "      retrieved_results = [retrieved_node.node.get_content().strip() for retrieved_node in retrieved_nodes]\n",
    "\n",
    "      return retrieved_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-Phoua0EeGk"
   },
   "source": [
    "By leveraging an external AI framework like LlamaIndex, we can see that the code has become significantly more concise and streamlined.\n",
    "\n",
    "Now, let‚Äôs practice using the same approach with an example to verify how it works in action!\n",
    "\n",
    "```\n",
    "retriever = LlamaIndexRetriever()\n",
    "topk = 5\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        retrieved_results = retriever.retrieve(item['query'], item['search_results'], topk)\n",
    "        break\n",
    "\n",
    "print(\"retrieved results:\")\n",
    "print()\n",
    "for rank, retrieved_result in enumerate(retrieved_results):\n",
    "    print(f\"rank {rank}: {retrieved_result}\")\n",
    "    print()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9jDeBe4XpAca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: how many 3-point attempts did steve nash average per game in seasons he made the 50-40-90 club?\n",
      "\n",
      "retrieved results:\n",
      "\n",
      "rank 0: 900 0.6 3.0 3.5 8.8 0.6 0.1 3.2 2.0 17.3 7 seasons PHO NBA 75 68 34.8 6.7 13.4 .497 1.3 3.5 .382 5.4 10.0 .537 .546 3.5 3.9 .898 0.4 3.0 3.4 9.7 0.5 0.2 3.5 1.8 18.2 4 seasons DAL NBA 43 43 37.7 5.5 12.9 .430 1.8 4.1 .449 3.7 8.8 .422 .502 3.0 3.3 .901 0.8 2.9 3.7 7.5 0.7 0.0 2.7 2.3 15.9 1 season LAL NBA 2 2 30.5 5.0 11.5 .435 0.0 1.5 .000 5.0 10.0 .500 .435 2.5 2.5 1.000 1.0 1.5 2.5 4.5 0.0 0.0 1.5 1.0 12.5 POWERED BY Steve Nash is 1 of 4 players 6'3\" or under with 10,000 career points and a True Shooting Percentage of .600 or greater. Can you name the other 3? Subscribe to Stathead , your all-access pass to the Basketball Reference database, to answer more questions like this.\n",
      "\n",
      "rank 1: [120] When Nash returned to Phoenix in 2004, he helped the Suns improve from a 29‚Äì53 record in 2003‚Äì04 to 62‚Äì20 in 2004‚Äì05, reaching the conference finals for the first time in 11 years, and earning his first MVP award. The next season, he again led the Suns to the conference finals, despite the injuries of all three big men (Stoudemire, Kurt Thomas , and Brian Grant ). Further, Nash was responsible for seven of his teammates attaining career-highs in season scoring. [33] With Nash operating at the point between the 2005‚Äì06 and 2009‚Äì10 seasons, the Suns led the league in field goal percentage.\n",
      "In 2021, to commemorate the NBA's 75th Anniversary The Athletic ranked their top 75 players of all time, and named Nash as the 38th greatest player in NBA history. [121] Career statistics Legend GP Games played GS Games started MPG Minutes per game FG% Field goal percentage 3P% 3-point field goal percentage FT% Free throw percentage RPG Rebounds per game APG Assists per game SPG Steals per game BPG Blocks per game PPG Points per game Bold Career high * Led the league NBA Regular season Year Team GP GS MPG FG% 3P% FT% RPG APG SPG BPG PPG 1996‚Äì97 Phoenix 65 2 10.5 .423 .418 .824 1.0 2.1 .3 .0 3.3 1997‚Äì98 Phoenix 76 9 21.9 .459 .415 .860 2.1 3.4 .8 .1 9.1 1998‚Äì99 Dallas 40 40 31.7 .363 .374 .826 2.9 5.5 .9 .1 7.9 1999‚Äì00 Dallas 56 27 27.4 .477 .403 .882 2.2 4.9 .7 .1 8.6 2000‚Äì01 Dallas 70 70 34.1 .487 .406 .895 3.2 7.3 1.0 .1 15.6 2001‚Äì02 Dallas 82 82 34.6 .483 .455 .887 3.1 7.7 .6 .0 17.\n",
      "\n",
      "rank 2: More Nash Pages Steve Nash Overview Game Logs 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Career Playoffs Splits 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Career Shooting 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Lineups 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 On/Off 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Other Steve Nash Pages Game Finder Streak Finder Span Finder Shot Finder Event Finder Quarter Finder Teammates & Opponents Compare Steve Nash to other players More Steve Nash pages at Sports Reference International Stats at Basketball-Reference.\n",
      "\n",
      "rank 3: 175 6.6 -2.0 4.6 0.9 2012-13 38 LAL NBA PG 2 61 14.5 .496 .130 .217 3.7 5.9 4.8 26.6 0.0 0.0 10.6 21.6 0.1 0.0 0.0 .014 2.0 -3.0 -1.0 0.0 Career NBA 120 4289 19.8 .583 .277 .278 1.8 9.3 5.6 39.6 0.8 0.2 17.7 22.8 11.7 0.1 11.9 .133 4.7 -1.5 3.2 5.6 7 seasons PHO NBA 75 2609 21.6 .600 .257 .292 1.5 9.8 5.7 44.6 0.7 0.3 18.7 24.5 8.0 0.4 8.4 .154 5.5 -1.4 4.1 4.0 4 seasons DAL NBA 43 1619 17.2 .554 .318 .255 2.2 8.6 5.3 32.0 1.0 0.1 16.1 20.1 3.7 -0.2 3.5 .103 3.5 -1.7 1.8 1.5 1 season LAL NBA 2 61 14.5 .496 .130 .217 3.7 5.9 4.8 26.6 0.0 0.0 10.6 21.6 0.1 0.0 0.0 .014 2.0 -3.0 -1.0 0.0 Playoffs Series College Stats More College Stats on SR/CBB ¬∑ underline indicates incomplete record Appearances on Leaderboards, Awards, and Honors Transactions Salaries Frequently Asked Questions How old is Steve Nash? Steve Nash is 50 years old. Where was Steve Nash born? Steve Nash was born in Johannesburg, South Africa. When was Steve Nash born?\n",
      "\n",
      "rank 4: Steve Nash Stats, Height, Weight, Position, Draft Status and more | Basketball-Reference.com Sports¬†Reference‚ÄØ¬Æ Baseball Football (college) Basketball (college) Hockey Calcio Blog Stathead‚ÄØ¬Æ Immaculate Grid Questions or Comments? Welcome ¬∑ Your Account Logout Ad-Free Login Create Account MENU Players Teams Seasons Leaders Scores WNBA Draft Stathead Newsletter Full Site Menu Below You are here: BBR Home Page > Players > N > Steve Nash Welcome ¬∑ Your Account Logout Ad-Free Login Create Account Steve Nash Stephen John Nash ‚ñ™ Twitter : SteveNash (MVSteve, Two Time, Nashty) Position: Point Guard\n",
      "\n",
      "\n",
      "  \n",
      "  ‚ñ™ Shoots: Right 6-3 , 195lb (190cm,¬†88kg) Born: February 7 , 1974 in¬†Johannesburg, South Africa za College: Santa Clara High School: Saint Michaels University School in Victoria, Canada Draft: Phoenix Suns , 1st round (15th pick, 15th overall), 1996 NBA Draft NBA Debut: November 1, 1996 Hall of Fame: Inducted as Player in 2018 ( Full List ) Career Length: 18 years More bio, uniform, draft, salary info Hall of Fame 8x All Star 5x AST Champ 7x All-NBA 2x MVP NBA 75th Anniv. Team 13 13 13 13 10 +4 SUMMARY Career G 1217 PTS 14.3 TRB 3.0 AST 8.5 FG% 49.0 FG3% 42.8 FT% 90.4 eFG% 55.6 PER 20.0 WS 129.7 Steve Nash Overview More Nash Pages Game Logs 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06 2006-07 2007-08 2008-09 2009-10 2010-11 2011-12 2012-13 2013-14 Career Playoffs Splits 1996-97 1997-98 1998-99 1999-00 2000-01 2001-02 2002-03 2003-04 2004-05 2005-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "retriever = LlamaIndexRetriever()\n",
    "topk = 5\n",
    "dataset_path = \"./crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        retrieved_results = retriever.retrieve(item['query'], item['search_results'], topk)\n",
    "        break\n",
    "\n",
    "print(\"retrieved results:\")\n",
    "print()\n",
    "for rank, retrieved_result in enumerate(retrieved_results):\n",
    "    print(f\"rank {rank}: {retrieved_result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbeB2pmznh3X"
   },
   "source": [
    "## II. Implementing a Reader\n",
    "\n",
    "In this section, we will design the **Reader**.\n",
    "\n",
    "What are the most important considerations when creating a Reader? The most crucial factor is likely the choice of LLM. Factors such as model size, performance on reasoning benchmarks, cost, and other considerations are typically part of the configuration.\n",
    "\n",
    "However, since we have limited options for the LLMs we can use in this practice session, this will not be a consideration for us here.\n",
    "\n",
    "So, what‚Äôs the next most important factor? **Prompt design**. It is well known that well-designed prompts lead to better results from the LLM.\n",
    "\n",
    "Moreover, setting an appropriate prompt becomes even more critical for the CRAG dataset. In this task, the LLM must be able to answer ‚ÄúI don‚Äôt know‚Äù if it encounters something it is unsure about or cannot answer confidently. To achieve this, the prompt must be specifically designed to guide the LLM to behave in this manner.\n",
    "\n",
    "Therefore, this exercise will be conducted in the following three main stages:\n",
    "\n",
    "1. Design a Prompt Template\n",
    "2. Implement a Prompt Generator\n",
    "3. Implement a Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AV1ZcVEiVaJ2"
   },
   "source": [
    "### 1. Design a Prompt Template\n",
    "\n",
    "To design an effective prompt template, we need to carefully consider certain factors.\n",
    "\n",
    "1.\tThe response must be generated based on the given question and references.\n",
    "2.\tIn the CRAG benchmark, answers should not be too long or verbose. During evaluation, only the first 75 tokens are used for scoring, so the response needs to be concise.\n",
    "3.\tThe LLM must be able to recognize questions it cannot answer and respond with ‚ÄúI don‚Äôt know‚Äù.\n",
    "\n",
    "Taking these factors into account, we can draft the following `system_prompt`:\n",
    "\n",
    "```Python\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are provided with a question and various references.\n",
    "Your task is to answer the question succinctly, using the fewest words possible.\n",
    "If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n",
    "There is no need to explain the reasoning behind your answers.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ObtbXbQBpFIp"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are provided with a question and various references.\n",
    "Your task is to answer the question succinctly, using the fewest words possible.\n",
    "If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n",
    "There is no need to explain the reasoning behind your answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbP_U8iXWgNK"
   },
   "source": [
    "### 2. Implement a Prompt Generator\n",
    "\n",
    "Above, we created a system prompt. Now, we need to build a prompt generator that takes the question and reference, combines them into one, and formats it so it can be passed to the LLM.\n",
    "\n",
    "Below is an example of a `prompt_generator` that takes a question and reference, combines them for delivery to the LLM:\n",
    "\n",
    "```Python\n",
    "def prompt_generator(query, top_k_chunks, system_prompt):\n",
    "    user_message = \"\"\n",
    "    references = \"\"\n",
    "\n",
    "    if len(top_k_chunks) > 0:\n",
    "        references += \"# References \\n\"\n",
    "        # Format the top sentences as references in the model's prompt template.\n",
    "        for chunk_id, chunk in enumerate(top_k_chunks):\n",
    "            references += f\"- {chunk.strip()}\\n\"\n",
    "\n",
    "    references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n",
    "    # Limit the length of references to fit the model's input size.\n",
    "\n",
    "    user_message += f\"{references}\\n------\\n\\n\"\n",
    "    user_message += f\"Using only the references listed above, answer the following question: \\n\"\n",
    "    user_message += f\"Question: {query}\\n\"\n",
    "\n",
    "    llm_input = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    return llm_input\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9uniz5hrXrGI"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "def prompt_generator(query, top_k_chunks, system_prompt):\n",
    "    user_message = \"\"\n",
    "    references = \"\"\n",
    "\n",
    "    if len(top_k_chunks) > 0:\n",
    "        references += \"# References \\n\"\n",
    "        # Format the top sentences as references in the model's prompt template.\n",
    "        for chunk_id, chunk in enumerate(top_k_chunks):\n",
    "            references += f\"- {chunk.strip()}\\n\"\n",
    "\n",
    "    references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n",
    "    # Limit the length of references to fit the model's input size.\n",
    "\n",
    "    user_message += f\"{references}\\n------\\n\\n\"\n",
    "    user_message += f\"Using only the references listed above, answer the following question: \\n\"\n",
    "    user_message += f\"Question: {query}\\n\"\n",
    "\n",
    "    llm_input = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    return llm_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDbMms2ApXZm"
   },
   "source": [
    "### 3. Implement a Reader\n",
    "\n",
    "Now that we have created a function to generate the necessary prompts for the Reader, we will proceed to define the Reader itself and set up the components needed for RAG creation.\n",
    "\n",
    "Follow the code below to implement it.\n",
    "\n",
    "```Python\n",
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "class Reader:\n",
    "  def __init__(self):\n",
    "\n",
    "    self.system_prompt = \"\"\"\n",
    "    You are provided with a question and various references.\n",
    "    Your task is to answer the question succinctly, using the fewest words possible.\n",
    "    If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n",
    "    There is no need to explain the reasoning behind your answers.\n",
    "    \"\"\"\n",
    "\n",
    "  def generate_response(self, query: str, top_k_chunks: list) -> str:\n",
    "      \"\"\"\n",
    "      Generate answer from context.\n",
    "      \"\"\"\n",
    "      llm_input = self.prompt_generator(query, top_k_chunks)\n",
    "      completion = oai_client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      temperature=0,\n",
    "      messages=\n",
    "      llm_input\n",
    "      ).choices[0].message.content\n",
    "      return completion\n",
    "\n",
    "  def prompt_generator(self, query, top_k_chunks):\n",
    "      user_message = \"\"\n",
    "      references = \"\"\n",
    "\n",
    "      if len(top_k_chunks) > 0:\n",
    "          references += \"# References \\n\"\n",
    "          # Format the top sentences as references in the model's prompt template.\n",
    "          for chunk_id, chunk in enumerate(top_k_chunks):\n",
    "              references += f\"- {chunk.strip()}\\n\"\n",
    "      \n",
    "      references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n",
    "      # Limit the length of references to fit the model's input size.\n",
    "\n",
    "      user_message += f\"{references}\\n------\\n\\n\"\n",
    "      user_message\n",
    "      user_message += f\"Using only the references listed above, answer the following question: \\n\"\n",
    "      user_message += f\"Question: {query}\\n\"\n",
    "\n",
    "      llm_input = [\n",
    "        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "      ]\n",
    "\n",
    "      return llm_input\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "J59VqE5-al91"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "class Reader:\n",
    "  def __init__(self):\n",
    "\n",
    "    self.system_prompt = \"\"\"\n",
    "    You are provided with a question and various references.\n",
    "    Your task is to answer the question succinctly, using the fewest words possible.\n",
    "    If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n",
    "    There is no need to explain the reasoning behind your answers.\n",
    "    \"\"\"\n",
    "\n",
    "  def generate_response(self, query: str, top_k_chunks: list) -> str:\n",
    "      \"\"\"\n",
    "      Generate answer from context.\n",
    "      \"\"\"\n",
    "      llm_input = self.prompt_generator(query, top_k_chunks)\n",
    "      completion = oai_client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      temperature=0,\n",
    "      messages=\n",
    "      llm_input\n",
    "      ).choices[0].message.content\n",
    "      return completion\n",
    "\n",
    "  def prompt_generator(self, query, top_k_chunks):\n",
    "      user_message = \"\"\n",
    "      references = \"\"\n",
    "\n",
    "      if len(top_k_chunks) > 0:\n",
    "          references += \"# References \\n\"\n",
    "          # Format the top sentences as references in the model's prompt template.\n",
    "          for chunk_id, chunk in enumerate(top_k_chunks):\n",
    "              references += f\"- {chunk.strip()}\\n\"\n",
    "\n",
    "      references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n",
    "      # Limit the length of references to fit the model's input size.\n",
    "\n",
    "      user_message += f\"{references}\\n------\\n\\n\"\n",
    "      user_message\n",
    "      user_message += f\"Using only the references listed above, answer the following question: \\n\"\n",
    "      user_message += f\"Question: {query}\\n\"\n",
    "\n",
    "      llm_input = [\n",
    "        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "      ]\n",
    "\n",
    "      return llm_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uBdH4d9Sr-S"
   },
   "source": [
    "Now, let‚Äôs check the results through an actual example.\n",
    "\n",
    "```\n",
    "reader = Reader()\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print(f\"ground truth: {item['answer']}\")\n",
    "        print()\n",
    "        answer = reader.generate_response(item['query'], [])\n",
    "        break\n",
    "\n",
    "print(f\"answer: {answer}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xaPEFeZ92hh6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: how many 3-point attempts did steve nash average per game in seasons he made the 50-40-90 club?\n",
      "ground truth: 4 3-points attempts per game\n",
      "\n",
      "answer: 3.0\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "reader = Reader()\n",
    "dataset_path = \"./crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print(f\"ground truth: {item['answer']}\")\n",
    "        print()\n",
    "        answer = reader.generate_response(item['query'], [])\n",
    "        break\n",
    "\n",
    "print(f\"answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVHLibfMGMy_"
   },
   "source": [
    "## III. Implementing a RAG\n",
    "\n",
    "At this point, we have defined both the Reader and the Retriever, and we have verified their inputs and outputs.\n",
    "\n",
    "Now, let‚Äôs combine these two components into a functional RAG system that we can use.\n",
    "\n",
    "```\n",
    "class RAG:\n",
    "    def __init__(self):\n",
    "        self.retriever = LlamaIndexRetriever()\n",
    "        self.reader = Reader()\n",
    "  \n",
    "    def inference(self, query, search_results, topk):\n",
    "        # 1. retrieve relevant chunks\n",
    "        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n",
    "\n",
    "        # 2. answer the question based on the retrieved chunks\n",
    "        answer = self.reader.generate_response(query, retrieved_results)\n",
    "\n",
    "        return answer, retrieved_results\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ikmQ42W1PRUt"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self):\n",
    "        self.retriever = LlamaIndexRetriever()\n",
    "        self.reader = Reader()\n",
    "\n",
    "    def inference(self, query, search_results, topk):\n",
    "        # 1. retrieve relevant chunks\n",
    "        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n",
    "\n",
    "        # 2. answer the question based on the retrieved chunks\n",
    "        answer = self.reader.generate_response(query, retrieved_results)\n",
    "\n",
    "        return answer, retrieved_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zByVLutpTnXX"
   },
   "source": [
    "Let‚Äôs now verify whether the RAG system we defined works as intended or not.\n",
    "\n",
    "Using the code below, we will test the system on a total of 10 data points. You can check each result yourself and evaluate whether the RAG performs well enough.\n",
    "\n",
    "```\n",
    "rag = RAG()\n",
    "topk = 5\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat > 9:\n",
    "          break\n",
    "        \n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer = rag.inference(item['query'], item['search_results'], topk)[0]\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        repeat += 1\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-i6vlXeI3Mmp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: how many 3-point attempts did steve nash average per game in seasons he made the 50-40-90 club?\n",
      "\n",
      "predicted answer: 3.5\n",
      "ground truth answer: 4 3-points attempts per game\n",
      "\n",
      "query: what is a movie to feature a person who can create and control a device that can manipulate the laws of physics?\n",
      "\n",
      "predicted answer: Ambrose Chase (Wildstorm) in the movie.\n",
      "ground truth answer: a movie that features a person who can create and control a device that can manipulate the laws of physics was \"the core\" in 2003, which starred aaron eckhart as a scientist who invents a device that can manipulate the laws of physics, allowing him to control gravity, time, and matter, and he must use this technology to save the earth from destruction after the planet's core suddenly stops rotating.\n",
      "\n",
      "query: where did the ceo of salesforce previously work?\n",
      "\n",
      "predicted answer: I don't know.\n",
      "ground truth answer: marc benioff spent 13 years at oracle, before launching salesforce.\n",
      "\n",
      "query: which movie won the oscar best visual effects in 2021?\n",
      "\n",
      "predicted answer: 'Tenet'\n",
      "ground truth answer: tenet\n",
      "\n",
      "query: what company in the dow jones is the best performer today?\n",
      "\n",
      "predicted answer: Salesforce (CRM).\n",
      "ground truth answer: salesforce\n",
      "\n",
      "query: in 2004, which animated film was recognized with the best animated feature film oscar?\n",
      "\n",
      "predicted answer: Finding Nemo\n",
      "ground truth answer: finding nemo\n",
      "\n",
      "query: on which date did sgml distribute dividends the first time\n",
      "\n",
      "predicted answer: I don't know.\n",
      "ground truth answer: none of the days\n",
      "\n",
      "query: what is the average gross for the top 3 pixar movies?\n",
      "\n",
      "predicted answer: $526.7 million\n",
      "ground truth answer: ~$509,638,437\n",
      "\n",
      "query: what are the countries that are located in southern africa.\n",
      "\n",
      "predicted answer: Angola, Botswana, Lesotho, Mozambique, Namibia, South Africa, Swaziland, Zambia, Zimbabwe.\n",
      "ground truth answer: angola, botswana, the comoros, eswatini, lesotho, madagascar, malawi, mauritius, mozambique, namibia, south africa, zambia, and zimbabwe.\n",
      "\n",
      "query: which company in the s&p 500 index has the highest percentage of green energy usage?\n",
      "\n",
      "predicted answer: I don't know.\n",
      "ground truth answer: the company with the highest percentage of renewable energy usage in the s&p 500 index is the estee lauder companies inc., with over 139% of its total power usage coming from green energy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "rag = RAG()\n",
    "topk = 5\n",
    "dataset_path = \"./crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat > 9:\n",
    "          break\n",
    "\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer = rag.inference(item['query'], item['search_results'], topk)[0]\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        repeat += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNNOCLNt3Ypx"
   },
   "source": [
    "## IV. Error case analysis\n",
    "\n",
    "Some of you may be satisfied with the experimental results above, while others may not. However, few would believe that the RAG system produced the correct answer for all questions.\n",
    "\n",
    "Therefore, before formally evaluating the RAG system, we will check which questions it answered incorrectly and try to understand why those results occurred. To do this, we need to classify the data into two categories:\n",
    "\n",
    "1.\tQuestions the RAG answered correctly.\n",
    "2.\tQuestions the RAG answered incorrectly.\n",
    "\n",
    "Ultimately, before moving on to Task 2 in the next session, we will execute the RAG implemented for Task 1 and analyze which queries the system struggles to answer correctly.\n",
    "\n",
    "To begin, let‚Äôs check how well the Reader alone performs on the following questions, without using search results.\n",
    "<br/>  \n",
    "Question: **In 2004, which animated film was recognized with the best animated feature film oscar?**.   \n",
    "Answer: **Finding Nemo**\n",
    "<br/>\n",
    "\n",
    "\n",
    "```\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat != 5:\n",
    "          repeat += 1\n",
    "          continue\n",
    "        \n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer = reader.generate_response(item['query'], [])\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        repeat += 1\n",
    "        break\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tsvz1mgAfimM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: in 2004, which animated film was recognized with the best animated feature film oscar?\n",
      "\n",
      "predicted answer: \"The Incredibles\"\n",
      "ground truth answer: finding nemo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "dataset_path = \"./crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat != 5:\n",
    "          repeat += 1\n",
    "          continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer = reader.generate_response(item['query'], [])\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        repeat += 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hAUpDSyhtVJ"
   },
   "source": [
    "Although the correct answer is **‚ÄúFinding Nemo‚Äù**, the model generated the incorrect answer, **‚ÄúThe Incredibles‚Äù**.\n",
    "\n",
    "Next, let‚Äôs check the generated result when search results are utilized.\n",
    "\n",
    "\n",
    "```\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "rag = RAG()\n",
    "topk = 5\n",
    "\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat != 5:\n",
    "          repeat += 1\n",
    "          continue\n",
    "        \n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        print(\"retrieved results:\")\n",
    "        for rank, retrieved_result in enumerate(retrieved_results):\n",
    "            print(f\"{rank}: {retrieved_result}\")\n",
    "        print()\n",
    "        repeat += 1\n",
    "        break\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ulSkV4uUiGYI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: in 2004, which animated film was recognized with the best animated feature film oscar?\n",
      "\n",
      "predicted answer: Finding Nemo\n",
      "ground truth answer: finding nemo\n",
      "\n",
      "retrieved results:\n",
      "0: But it wasn‚Äôt until 1991‚Äôs Beauty and the Beast that an animated film was nominated for best picture. Throughout the ‚Äô90s, the Oscars resisted adding a category for animated features, though John Lasseter received a special achievement award in 1996 ‚Äúfor his inspired leadership of the Pixar Toy Story team, resulting in the first feature-length computer-animated film.‚Äù In 2001, the Academy finally added a category for best animated feature. Shrek became the first winner in 2002. This year, we had a close contest for best animated feature film between The Boy and the Heron , which won in this category at the BAFTA Awards in London on Feb. 18, and Spider-Man: Across the Spider-Verse. This year‚Äôs other nominees were Elemental, Nimona and Robot Dreams. Here‚Äôs a year-by-year recap of all the Oscar winners for best animated feature film. The years shown are the years of the awards presentation. 2002: Shrek Image Credit: ¬©DreamWorks/courtesy Everett Studio : PDI/DreamWorks Production;¬†DreamWorks Oscar Went to : Aron Warner Film‚Äôs Other Oscar Wins : none Other Oscar Nods : adapted screenplay (written by Ted Elliott & Terry Rossio and Joe Stillman and Roger S.H. Schulman) Notes : Shrek was inducted into the National Film Registry in 2020. The soundtrack reached No. 28 on the Billboard 200. It contains two hits by Smash Mouth, the 1999 smash ‚ÄúAll Star‚Äù and a remake of The Monkees‚Äô ‚ÄúI‚Äôm a Believer‚Äù recorded specifically for the movie. 2003: Spirited Away Studio : Studio Ghibli Production;¬†Buena Vista [Japan] Oscar Went to : Hayao Miyazaki Film‚Äôs Other Oscar Wins : none Other Oscar Nods : none Notes : Spirited Away was the first Japanese, hand-drawn and non-English language film to win in the category. Miyazaki won an honorary Oscar in 2014.\n",
      "1: It was also nominated alongside \"A Bug's Life\" and \"Mulan\" in the \"Best Original Musical or Comedy Score,\" which existed in the mid-1990s, but lost them all to \"Shakespeare in Love.\" 2000: \"Tarzan\" scored a victory with \"You'll Be in My Heart.\" \"Tarzan.\" Disney Disney notched up another victory as Phil Collins's \"You'll Be in My Heart\" won the best original song award. \"Toy Story 2\" was nominated in the category as well, for Randy Newman's song \"When She Loved Me.\" Advertisement 2002: \"Shrek\" won the first best animated feature Oscar. \"Shrek.\" DreamWorks Winning over \"Jimmy Neutron: Boy Genius\" and Pixar's \"Monsters, Inc.,\" the Dreamworks movie \" Shrek \" won the first Oscar for best animated feature. It was also nominated in the best adapted screenplay award. \"Monsters, Inc.,\" though, won the the original song award for \"If I Didn't Have You\" and received nominations for original score and sound editing. Advertisement 2003: The Japanese film \"Spirited Away\" claimed victory. \"Spirited Away.\" Studio Ghibli Hayao Miyazaki's masterpiece \" Spirited Away ,\" from Studio Ghibli, received the award. The movie's English-language dub and release were supervised by Disney. However, it beat out two Disney features nominated in the category: \"Lilo & Stitch\" and \"Treasure Planet,\" while \"Ice Age\" and \"Spirit: Stallion of the Cimarron\" were also nominated. Advertisement 2004: \"Finding Nemo\" gave Pixar its first win. \"Finding Nemo.\" Pixar The movie won in the best animated film category over \"Brother Bear\" and \"The Triplets of Belleville.\" It also received original screenplay, score, and sound editing nominations. 2005: Pixar won again with \"The Incredibles.\" \"the Incredibles.\" Pixar Disney's \"Home on the Range\" is completely ignored by the Academy while \" The Incredibles \" nabs the animated feature and sound editing categories and racks up nominations for original screenplay and sound mixing. The other animated feature nominees are \"Shrek 2\" and \"Shark Tale.\"\n",
      "2: WATCH NOW Stop-motion Animated Oscar Winners Wallace & Gromit: The Curse of the Were-Rabbit (2005) Wallace & Gromit: The Curse of the Were-Rabbit Directed by Nick Park and Steve Box, Wallace & Gromit follows the adventures of eccentric inventor Wallace, and his ever-loyal dog Gromit as they aim to save the town's annual giant vegetable contest from a mysterious beast. The film's unique stop-motion animation style which took years to produce, combined with its humorous storyline and quirky characters made it an instant classic. Wallace & Gromit had previously won three Oscars for Best Animated Short Film, and audiences and critics warmly received their transition into a feature-length film. STOP-MOTION ANIMATED OSCAR WINNERS Conclusion The imaginative and impactful animation, combined with the heart-warming story, has made The Curse of the Were-Rabbit an iconic stop-motion film. WATCH NOW Academy Award for Best Animated Feature Film Happy Feet (2006) Happy Feet's best scene When those happy little penguins first slid their way into our hearts with their toe-tapping moves, nobody could have predicted the heights they'd reach. Happy Feet danced all the way to the top of the box office charts and then straight into Oscar history when it snagged the trophy for Best Animated Feature in 2006. What was it about these flightless birds that charmed audiences and Academy voters alike? Was it the slick animation? The catchy soundtrack? The sheer joy of watching a bunch of feathered friends cut a rug? Whatever it was, it worked like a charm. ACADEMY AWARD FOR BEST ANIMATED FEATURE FILM Conclusion Happy Feet was an instant hit because of its charming protagonist , stunning animation, and feel-good message about being true to oneself. WATCH NOW Pixar‚Äôs Best Animated Movie Oscar Ratatouille (2007) Brad Bird winning for \"Ratatouille\" Ratatouille was nominated for a whopping five Academy Awards. This included the Ratatouille screenplay for Best Original Screenplay and a win for Best Animated Feature in 2008. The Pixar film follows the chaotic yet heartwarming journey of a rodent-turned-chef on more than just a delightful tale of culinary adventures. It's a nuanced exploration of what it truly means to follow your passions, even when doing so feels risky or even impossible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "dataset_path = \"./crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "rag = RAG()\n",
    "topk = 5\n",
    "\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat != 5:\n",
    "          repeat += 1\n",
    "          continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        print(\"retrieved results:\")\n",
    "        for rank, retrieved_result in enumerate(retrieved_results):\n",
    "            print(f\"{rank}: {retrieved_result}\")\n",
    "        print()\n",
    "        repeat += 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnVsb-k14ZCi"
   },
   "source": [
    "The following queries focus on retrieving information related to finance.\n",
    "\n",
    "Such information is typically stored in structured data formats, such as tables or knowledge graphs. However, unstructured data sources, like web search results, often overlook the structural information inherent in tables or knowledge graphs, making it challenging to extract specific information efficiently.\n",
    "\n",
    "For instance, financial data such as Microsoft's ex-dividend date, P/E ratio, or earnings per share is usually presented in numeric, date, or tabular formats. In contrast, text-based data lacks the structured representation found in tables, making it harder to leverage such information.\n",
    "\n",
    "Let us explore whether RAG (Retrieval-Augmented Generation) can effectively answer the following queries using only web search results.\n",
    "\n",
    "<br/>  \n",
    "Question: **What is the ex-dividend date of microsoft in the 1st qtr of 2024**.   \n",
    "Answer: **The ex-dividend date of microsoft in the 1st qtr of 2024 is feb 14, 2024**\n",
    "<br/>\n",
    "\n",
    "<br/>  \n",
    "Question: **I'm looking for the p/e ratio of dks. would you happen to know what it is?**.   \n",
    "Answer: **13.75**\n",
    "<br/>\n",
    "\n",
    "<br/>  \n",
    "Question: **What's auph's earnings per share?**.   \n",
    "Answer: **0.4**\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "```Python\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "\n",
    "rag = RAG()\n",
    "topk = 5\n",
    "\n",
    "repeat = 0\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat not in [14, 53, 64]:\n",
    "          repeat += 1\n",
    "          continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        print(\"retrieved results:\")\n",
    "        for rank, retrieved_result in enumerate(retrieved_results):\n",
    "            print(f\"{rank}: {retrieved_result}\")\n",
    "        print()\n",
    "        repeat += 1\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Dya6MQwm8H-V",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: what is the ex-dividend date of microsoft in the 1st qtr of 2024\n",
      "\n",
      "predicted answer: May 15, 2024\n",
      "ground truth answer: the ex-dividend date of microsoft in the 1st qtr of 2024 is feb 14, 2024\n",
      "\n",
      "retrieved results:\n",
      "0: 2023 Dec 14, 2023 Aug 16, 2023 $0.68 Quarterly Jun 13, 2023 Aug 17, 2023 Sep 14, 2023 May 17, 2023 $0.68 Quarterly Mar 14, 2023 May 18, 2023 Jun 08, 2023 Feb 15, 2023 $0.68 Quarterly Nov 29, 2022 Feb 16, 2023 Mar 09, 2023 Nov 16, 2022 $0.68 Quarterly Sep 20, 2022 Nov 17, 2022 Dec 08, 2022 Aug 17, 2022 $0.62 Quarterly Jun 14, 2022 Aug 18, 2022 Sep 08, 2022 May 18, 2022 $0.62 Quarterly Mar 14, 2022 May 19, 2022 Jun 09, 2022 Feb 16, 2022 $0.62 Quarterly Dec 07, 2021 Feb 17, 2022 Mar 10, 2022 The table shows Microsoft‚Äôs dividend history, including amount per share, payout frequency, declaration, record, and payment dates. Show More FAQ Does Microsoft pay dividends? Yes, MSFT has paid a dividend within the past 12 months. How much is Microsoft's dividend? MSFT pays a dividend of $0.75 per share. MSFT's annual dividend yield is 0.67%. When is Microsoft ex-dividend date? Microsoft's upcoming ex-dividend date is on May 15, 2024. Microsoft shareholders who own MSFT stock before this date will receive Microsoft's next dividend payment of $0.75 per share on Jun 13, 2024. Add MSFT to your watchlist to be reminded before Microsoft's ex-dividend date. When is Microsoft dividend payment date? Microsoft's next dividend payment date is on Jun 13, 2024, when Microsoft shareholders who own MSFT shares before May 15, 2024 will receive a dividend payment of $0.75 per share. Add MSFT to your watchlist to be reminded of MSFT's next dividend payment. Does Microsoft have sufficient earnings to cover their dividend?\n",
      "1: Microsoft Corporation (MSFT) Dividend Date & History | Koyfin Skip to Content Product Features Data Coverage Data Coverage Use Cases Independent Investors Financial Advisors Traders Research Analyst Sales Students Enterprise Schools Pricing Sign Up for Free Log In Log In Sign Up for Free MSFT Microsoft Corporation Microsoft Corporation MSFT NasdaqGS $416.42 USD -8.8 -2.07% Market Cap $3,094,182.25M Forward P/E 34.1x Volume 45,079,900 Total Return (3M) 12.53% Microsoft Corporation ( MSFT ) Dividend Date & History Sign Up for Free Dividend Data Microsoft Corporation‚Äôs ( MSFT ) dividend yield is 0.72%, which means that for every\n",
      "        $100 invested in the company's stock, investors would receive $0.72 in dividends per year. Microsoft Corporation‚Äôs payout ratio is 25.12% which means that 25.12% of the company's\n",
      "        earnings are paid out as dividends. A low payout ratio may indicate that the company has a strong financial position\n",
      "        and can invest in growth opportunities, while a high payout ratio may indicate that the company is returning most\n",
      "        of its earnings to shareholders. MSFT ‚Äôs annual dividend is $3.00 per share. This is\n",
      "        the total amount of dividends paid out to shareholders in a year. Microsoft Corporation‚Äôs ( MSFT ) ex-dividend date is May 15, 2024 , which means that buyers purchasing shares on or after that date will not be eligible to receive the next dividend payment. Microsoft Corporation ( MSFT ) pays dividends on a quarterly basis.\n",
      "          The next dividend payment is planned on June 13, 2024 .\n",
      "          \n",
      "              Microsoft Corporation ( MSFT ) has increased its dividends for 19 \n",
      "              consecutive years. This is a positive sign of the company's financial stability and its ability to pay\n",
      "              consistent dividends in the future. Dividend Yield 0.72% Annual Dividend $3.00 Ex Dividend Date May 15, 2024 Dividend Growth 10.00% Payout Ratio 25.12% Payout Period Quarterly Next Div Payment Jun 13, 2024 Cons.\n",
      "2: The MSFT stock shareholders received the last dividend payment of $0.75 per share on June 13, 2024 . When is Microsoft Corporation‚Äôs next dividend payment date? Microsoft Corporation‚Äôs next dividend payment will be on June 13, 2024 . Does Microsoft Corporation have sufficient earnings to cover its dividends? Microsoft Corporation ( MSFT ) has a low payout ratio of 25.12%, which is generally considered to be a sign that a company has enough earnings to pay dividends and retain earnings to reinvest in the business. Related Tickers Salesforce, Inc. ( CRM ) Oracle Corporation ( ORCL ) NVIDIA Corporation ( NVDA ) Great investments start with great insight. Save time, gain insight and revolutionize the way you invest. Get started today. Get started Resources Help Center Koyfin Academy Compare Yahoo Finance Morningstar Bloomberg Google Finance Finviz Ycharts Atom Finance Trading View Sentieo Factset Company About Us Careers Resources Help Center Koyfin Academy Compare Yahoo Finance Morningstar Bloomberg Google Finance Finviz Ycharts Atom Finance Trading View Sentieo Factset Company About Us Careers Follow Twitter Youtube Instagram Facebook Twitter Youtube Instagram Facebook ¬© 2024 Koyfin Inc. Privacy Terms of Service\n",
      "3: 3% 5/14/2013 5/16/2013 6/13/2013 (Data available from 1/1/2013 forward) Microsoft Dividend - Frequently Asked Questions What is Microsoft's dividend yield? The current dividend yield for Microsoft is 0.72%. Learn more on MSFT's dividend yield history. How much is Microsoft's annual dividend? The annual dividend for MSFT shares is $3.00. Learn more on MSFT's annual dividend history. How often does Microsoft pay dividends? Microsoft pays quarterly dividends to shareholders. When is Microsoft's next dividend payment? Microsoft's next quarterly dividend payment of $0.75 per share will be made to shareholders on Thursday, June 13, 2024. When was Microsoft's most recent dividend payment? Microsoft's most recent quarterly dividend payment of $0.75 per share was made to shareholders on Thursday, March 14, 2024. When is Microsoft's ex-dividend date? Microsoft's next ex-dividend date is Wednesday, May 15, 2024. When was Microsoft's most recent ex-dividend date? Microsoft's most recent ex-dividend date was Wednesday, February 14, 2024. Is Microsoft's dividend growing? Over the past three years, the company's dividend has grown by an average of 10.11% per year. What track record does Microsoft have of raising its dividend? Microsoft has increased its dividend for the past 22 consecutive years. When did Microsoft last increase or decrease its dividend? The most recent change in the company's dividend was an increase of $0.07 on Monday, September 18, 2023. What is Microsoft's dividend payout ratio?\n",
      "\n",
      "query: i'm looking for the p/e ratio of dks. would you happen to know what it is?\n",
      "\n",
      "predicted answer: I don't know.\n",
      "ground truth answer: 13.75\n",
      "\n",
      "retrieved results:\n",
      "0: Dicks Sporting Price to Sales | (NYSE:DKS) United States Sign In New Account About Macroaxis Solutions Settings Plans & Pricing Night Mode Day Mode Auto Mode Home Sign In Create Account About Macroaxis Add Dicks To Portfolio FinTech Suite AI Portfolio Architect Beta Investing Opportunities Equity Analysis Sign In To Macroaxis Sectors Equity Cryptos Correlation Stories Economic Markets Frontier Volatility Diagnostics Performance Dicks Sporting Goods Stock¬†Filter Stocks by Fundamentals Stocks . USA . Stock . Dicks Sporting Goods Summary Performance Analysis Advice Fundamentals Technicals Indicators Dividends Trends Premiums Profitability Ownership Competition DKS Stock USD 177.10 1.58 0.90% Dicks Sporting Goods fundamentals help investors to digest information that contributes to Dicks Sporting's financial success or failures. It also enables traders to predict the movement of Dicks Stock. The fundamental analysis module provides a way to measure Dicks Sporting's intrinsic value by examining its available economic and financial indicators, including the cash flow records, the balance sheet account changes, the income statement patterns, and various microeconomic indicators and financial ratios related to Dicks Sporting stock. Receivables Inventories Price to Sales Ratio is likely to drop to  0.76 in 2024.\n",
      "1: [9] In business culture [ edit ] The P/E ratio of a company is a major focus for many managers. They are usually paid in company stock or options on their company's stock (a form of payment that is supposed to align the interests of management with the interests of other stock holders). The stock price can increase in one of two ways: either through improved earnings or through an improved multiple that the market assigns to those earnings. In turn, the primary drivers for multiples such as the P/E ratio is through higher and more sustained earnings growth rates. Consequently, managers have strong incentives to boost earnings per share, even in the short term, and/or improve long-term growth rates. This can influence business decisions in several ways: If a company wants to acquire companies with a higher P/E ratio than its own, it usually prefers paying in cash or debt rather than in stock. Though in theory the method of payment makes no difference to value, doing it this way offsets or avoids earnings dilution (see accretion/dilution analysis ). Conversely, companies with higher P/E ratios than their targets are more tempted to use their stock to pay for acquisitions. Companies with high P/E ratios but volatile earnings may be tempted to find ways to smooth earnings and diversify risk‚Äîthis is the theory behind building conglomerates . Conversely, companies with low P/E ratios may be tempted to acquire small high-growth businesses in an effort to \"rebrand\" their portfolio of activities and burnish their image as growth stocks and thus obtain a higher PE rating. Companies try to smooth earnings, for example by \" slush fund accounting\" (hiding excess earnings in good years to cover for losses in lean years). Such measures are designed to create the image that the company always slowly but steadily increases profits, with the goal to increase the P/E ratio. Companies with low P/E ratios are usually more open to leveraging their balance sheet. As seen above, this mechanically lowers the P/E ratio, which means the company looks cheaper than it did before leverage, and also improves earnings growth rates. Both of these factors help drive up the share price. Strictly speaking, the ratio is measured in years, since the price is measured in dollars and earnings are measured in dollars per year. Therefore, the ratio demonstrates how many years it takes to cover the price, if earnings stay the same.\n",
      "2: Decide, based on its value, if they should buy, sell or hold any particular stock. ‚ÄúPE ratio‚Äù may sound technical, but it‚Äôs really just a comparison of how the public feels about a company (its stock price) and how well the company is actually doing (its EPS). The reading (and its inferences) can also be applied to market indexes, such as the S&P 500, Dow Jones Industrial Average and Nasdaq. PE ratio example Here‚Äôs one scenario: A company posts stable profits quarter after quarter, and its projected profits are equally stable. If its stock price jumps but its earnings stay the same (and no earnings increases are expected), the company‚Äôs intrinsic value didn‚Äôt change; the market‚Äôs perception of the company did. In this instance, the earnings in the PE ratio stayed the same, while the price soared, which mathematically sends the overall PE ratio higher. If a company‚Äôs PE ratio is significantly higher than its peers, there‚Äôs a chance the stock is overvalued. Another way to understand PE ratio: It‚Äôs a measure of how much investors are paying for every $1 of a company‚Äôs earnings. Imagine two similar companies in the same sector. One has a share price of $100 and a PE ratio of 15. The other has a share price of $50 and a PE ratio of 30. The first company‚Äôs share price may be higher, but a PE ratio of 15 means you‚Äôre only paying $15 for every $1 of the company‚Äôs earnings. Investors in the company with a PE ratio of 30 are paying $30 for $1 of earnings. PE ratio formula To arrive at a company‚Äôs PE ratio, you‚Äôll need to first know its EPS, which is calculated by dividing the company‚Äôs net profits by the number of shares of common stock it has outstanding. Once you have that, you can divide the company‚Äôs current share price by its EPS. For example, if a company has earnings of $10 billion and has 2 billion shares outstanding, its EPS is $5. If its stock price is currently $120, its PE ratio would be 120 divided by 5, which comes out to 24. One way to put it is that the stock is trading 24 times higher than the company‚Äôs earnings, or 24x.\n",
      "\n",
      "query: what's auph's earnings per share?\n",
      "\n",
      "predicted answer: Data is currently not available.\n",
      "ground truth answer: 0.4\n",
      "\n",
      "retrieved results:\n",
      "0: Please try using other words for your search or explore other sections of the website for relevant information. We‚Äôre sorry, we are currently experiencing some issues, please try again later. Our team is working diligently to resolve the issue. Thank you for your patience and understanding. Show more results -> Find a Symbol Search for Earnings Date When autocomplete results are available use up and down arrows to review and enter to select.  Touch device users, explore by touch or with swipe gestures. Edit my quotes Aurinia Pharmaceuticals Inc Ordinary Shares (AUPH) Nasdaq Listed Nasdaq 100 Data is currently not available Bid: x Ask: x Volume: 0 Add to Watchlist Add to Portfolio Quotes Summary Live Real-Time Live After-Hours Live Pre-Market Live Charts Live NEWS & ANALYSIS News Live Press Releases Live Analyst Research Live Dividend History Historical Quotes Historical NOCP Financials Earnings P/E & PEG Ratios Option Chain Short Interest Institutional Holdings Insider Activity SEC Filings Revenue EPS AUPH AUPH EARNINGS DATE AUPH Earnings Date Data is currently not available Latest Press Release Published Published Earnings Per Share Estimated Reported Data is currently not available Estimated EPS 1.00 Reported EPS: 1.00 Quarterly Earnings Surprise Amount Data is currently not available Yearly Earnings Forecast Data is currently not available Quarterly Earnings Forecast Data is currently not available Change in Consensus Data is currently not available Year End: 1.00 Qrtr End: 1.00 Number of Estimates Changed Data is currently not available Data is currently not available Back to AUPH Overview *The upcoming earnings date is derived from an algorithm based on a company's historical reporting dates. It is possible that this date will be updated in the future, once the company announces the actual date. Data Provider: Zacks Investment Research . Zacks earnings numbers are reported on a BNRI (Before Non Recurring Items) basis and include stock option expenses where possible. New to earnings? Here's a quick guide for how to read an earnings report . Visit the Earnings Calendar to see dates for upcoming earnings announcements. About Earnings Date Nasdaq provides visual representation of analyst expected earnings growth. Read our earnings report guide before you consider the forecast information when making investment decisions. Visit the Earnings Calendar to see dates for upcoming earnings announcements.\n",
      "1: Aurinia Pharmaceuticals Inc (AUPH) Stock Price Today, Quote, Latest Discussions, Interactive Chart and News Stocktwits Join the Conversation! Build your trading network Follow your favorite assets Link Your Portfolio and become verified Sign Up Log In Stocktwits StockTwits Symbol Upgrade Rooms Markets Earnings Newsletters Loading... Trending Loading... More Home Symbol AUPH AUPH Aurinia Pharmaceuticals Inc 43,924 $5.86 $0.16 (2.66%) Today Market Cap $870.6M Volume (M) 507,535.00 52-Wk High $12.43 52-Wk Low $5.35 ST Data Watchers New Watcher Rank About Feed News Sentiment Earnings Fundamentals 3rd Party Ad. Not an offer or recommendation by Stocktwits. See disclosure here. Join the conversation. This is where all the magic happens. Sign Up Log In\n",
      "2: AUPH No significant news for  in the past two years. Key Stock Data ? P/E Ratio (TTM) The Price to Earnings (P/E) ratio, a key valuation measure, is calculated by dividing the stock's most recent closing price by the sum of the diluted earnings per share from continuing operations for the trailing 12 month period. Earnings Per Share (TTM) A company's net income for the trailing twelve month period expressed as a dollar amount per fully diluted shares outstanding. Market Capitalization Reflects the total market value of a company. Market Cap is calculated by multiplying the number of shares outstanding by the stock's price. For companies with multiple common share classes, market capitalization includes both classes. Shares Outstanding Number of shares that are currently held by investors, including restricted shares owned by the company's officers and insiders as well as those held by the public. Public Float The number of shares in the hands of public investors and available to trade. To calculate, start with total shares outstanding and subtract the number of restricted shares. Restricted stock typically is that issued to company insiders with limits on when it may be traded. Dividend Yield A company's dividend expressed as a percentage of its current stock price. Key Stock Data P/E Ratio (TTM) N/A EPS (TTM) $ -0.55 Market Cap $ 870.60 M Shares Outstanding 144.62 M Public Float 127.57 M Yield AUPH is not currently paying a regular dividend. Latest Dividend N/A Ex-Dividend Date N/A ? Shares Sold Short The total number of shares of a security that have been sold short and not yet repurchased. Change from Last Percentage change in short interest from the previous report to the most recent report. Exchanges report short interest twice a month. Percent of Float Total short positions relative to the number of shares available to trade. Short Interest (02/15/24) Shares Sold Short 16.85 M Change from Last -9.05% Percent of Float 13.21% ? Money Flow Uptick/Downtick Ratio Money flow measures the relative buying and selling pressure on a stock, based on the value of trades made on an \"uptick\" in price and the value of trades made on a \"downtick\" in price. The up/down ratio is calculated by dividing the value of uptick trades by the value of downtick trades.\n",
      "3: They Could Keep Climbing 11:18a Viking Therapeutics‚Äô promising weight-loss drug data makes the company a takeover target 10:56a Oil prices rise, buoyed by bets that OPEC+ will decide to extend its voluntary output cuts 10:55a Disney teams with India‚Äôs Reliance Industries to form $8.5 billion streaming joint venture 10:54a Barron's These Stocks Are Moving the Most Today: Beyond Meat, eBay, Boston Beer, Lemonade, UnitedHealth, Progyny, and More 10:51a Barron's UnitedHealth Stock Falls After Report Says the Justice Department Has Launched an Antitrust Investigation 10:47a Warby Parker‚Äôs stock tumbles again after earnings, as gross margins keep falling 10:43a Barron's Boston Beer Stock Drops as Shipments Fall, CEO Retires Home Investing Quotes Stocks United States AUPH Overview Stock Screener Earnings Calendar Sectors Nasdaq Search Ticker | AUPH U.S.: Nasdaq Aurinia Pharmaceuticals Inc. Watch list Alert NEW Set a price target alert OK AUPH US Open Last Updated: Feb 28, 2024 11:32 a.m. EST Real time quote $ 5.85 -0.18 -2.91% Previous Close $6.02 Toggle Chart Options Advanced Charting 1D 5D 1M 3M 6M YTD 1Y 3Y All Range Dropdown $ % Vol Volume: 567.78K 65 Day Avg: 2.52M 23% vs Avg 5.84 Day Range 6.03 5.35 52 Week Range 12.43 Partner Center Your Watchlists Customize MarketWatch Have Watchlists? Log in to see them here or sign up to get started. Create Account ‚Ä¶ or Log In Symbol Company Price Chg/Chg % ( Go to Your Watchlist ) No Items in Watchlist There are currently no items in this Watchlist. Add Tickers No Saved Watchlists Create a list of the investments you want to track. Create Watchlist ‚Ä¶or learn more Uh oh Something went wrong while loading Watchlist. Go to Watchlist Recently Viewed Tickers No Recent Tickers Visit a quote page and your recently viewed tickers will be displayed here.\n",
      "4: Here Are 10 Top Analyst Forecasts For Thursday Feb. 22, 2024 at 7:23 a.m. ET on Benzinga.com Aurinia Pharmaceuticals: A Buy Rating on Strong Lupkynis Sales and Streamlined Operations Feb. 18, 2024 at 11:47 p.m. ET on TipRanks.com New Corporate Activity and Growth Risk for Aurinia Pharmaceuticals ‚Äì What‚Äôs the Latest? Feb. 16, 2024 at 1:02 a.m. ET on TipRanks.com Why Herbalife Shares Are Trading Lower By Around 36%? Here Are Other Stocks Moving In Thursday's Mid-Day Session Feb. 15, 2024 at 1:20 p.m. ET on Benzinga.com Analysts Are Bullish on These Healthcare Stocks: Valneva (VALN), Aurinia Pharmaceuticals (AUPH) Feb. 15, 2024 at 12:20 p.m. ET on TipRanks.com Aurinia Pharmaceuticals Finds No Buyer After Strategic Business Review, Launches Stock Buyback Feb. 15, 2024 at 9:51 a.m. ET on Benzinga.com Aurinia Pharmaceuticals (AUPH) Reports Q4 Loss, Tops Revenue Estimates Feb. 15, 2024 at 7:25 a.m. ET on Zacks.com Aurinia Pharmaceuticals Inc (AUPH) Announces Year-End Financial Results and Strategic Initiatives Aurinia Pharmaceuticals Inc (AUPH) Announces Year-End Financial Results and Strategic Initiatives Feb. 15, 2024 at 6:49 a.m. ET on GuruFocus.com Earnings Scheduled For February 15, 2024 Feb. 15, 2024 at 5:17 a.m. ET on Benzinga.com Earnings Outlook For Aurinia Pharmaceuticals Feb. 14, 2024 at 3:01 p.m. ET on Benzinga.com Esperion Therapeutics (ESPR) Expected to Beat Earnings Estimates: Can the Stock Move Higher? Feb. 13, 2024 at 10:00 a.m. ET on Zacks.com Aurinia Pharmaceuticals (AUPH) Expected to Beat Earnings Estimates: What to Know Ahead of Q4 Release Feb. 8, 2024 at 10:00 a.m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"./crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "\n",
    "rag = RAG()\n",
    "topk = 5\n",
    "\n",
    "repeat = 0\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat not in [14, 53, 64]:\n",
    "          repeat += 1\n",
    "          continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        print(\"retrieved results:\")\n",
    "        for rank, retrieved_result in enumerate(retrieved_results):\n",
    "            print(f\"{rank}: {retrieved_result}\")\n",
    "        print()\n",
    "        repeat += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/truera/trulens/blob/main/trulens_eval/examples/quickstart/quickstart.ipynb",
     "timestamp": 1711609044141
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
