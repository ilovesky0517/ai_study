{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXSKbyvh2QaU"
   },
   "source": [
    "#ðŸ““ TASK #2: KNOWLEDGE GRAPH AND WEB AUGMENTATION\n",
    "\n",
    "This task introduces mock APIs to access information from underlying mock Knowledge Graphs (KGs), with structured data possibly related to the questions. Participants use mock APIs, inputting parameters derived from the questions, to retrieve relevant data for answer formulation. The evaluation focuses on the systems' ability to query structured data and integrate information from various sources into comprehensive answers.\n",
    "\n",
    "<img src=\"https://i.ibb.co/cXqvBZq/2024-12-07-3-09-46.png\">\n",
    "\n",
    "###Steps in RAG with Mock API\n",
    "1. The model receives an input query to which a response is required.\n",
    "2. Retriever retrieves relevant chunks from the recieved web pages that are pertinent to the input query.\n",
    "3. Mock API retrieves relevant information from the Mock KG that are pertinent to the input query.\n",
    "4. The large language model then generates a response, informed by both the original query and the retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duLH-aV7Evhl"
   },
   "source": [
    "This practice class will be comprised of five sections.  \n",
    "  \n",
    "### I. Implementing a Mock KG Query Engine\n",
    "### II. Implementing a Reader\n",
    "### III. Implementing a LLM + Mock KG\n",
    "### IV. Implementing a LLM + Web Search Results +Mock KG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qApBmOWhxLV8"
   },
   "source": [
    "## I. Implementing a Mock KG Query Engine\n",
    "\n",
    "As you observed in the previous session, we can send specific queries to the Mock API connected to the Knowledge Graph (KG). The results obtained from the KG through the Mock API can then be utilized in the LLMâ€™s inference stage.\n",
    "\n",
    "Ultimately, what we will implement in this session is a connection between the existing RAG and the KG. Specifically, we will build the Mock KG query engine that serves as this connection.\n",
    "\n",
    "More specifically, we will create a Mock KG query engine that generates a query from an input question **belonging to the finance domain**, sends it to the KG, and retrieves the relevant information.\n",
    "\n",
    "The process will be carried out in the following five steps.\n",
    "\n",
    "1. Preparing Python Packages\n",
    "2. Preparing Mock APIs\n",
    "3. Implementing a Query Generator\n",
    "4. Implementing a Query Executor\n",
    "5. Implementing a Mock KG Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrqPxwa4OOe8"
   },
   "source": [
    "### 1. Preparing Python Packages\n",
    "\n",
    "As always, we will install and import the necessary python packages for use.  \n",
    "\n",
    "The important point is that the **external IP address** of the KG we will use must be set correctly. If the configuration is incorrect, errors may occur in subsequent code execution.\n",
    "\n",
    "```Python\n",
    "!pip install llama-index --quiet\n",
    "!pip install llama-index-readers-wikipedia wikipedia --quiet\n",
    "!pip install llama-index-llms-openai --quiet\n",
    "!pip install llama-index-embeddings-huggingface --quiet\n",
    "!pip install packaging==23.2 trulens trulens-providers-openai openai --quiet\n",
    "!pip install langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n",
    "\n",
    "!pip install blingfire beautifulsoup4 sentence-transformers ray --quiet\n",
    "!pip install textwrap3 --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip uninstall pandas scipy transformers -y\n",
    "!pip install pandas scipy transformers --quiet\n",
    "```\n",
    "```Python\n",
    "from typing import List\n",
    "import requests\n",
    "import numpy as np\n",
    "import bz2\n",
    "import json\n",
    "import torch\n",
    "from blingfire import text_to_sentences_and_offsets\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" #copy your api key\n",
    "os.environ[\"CRAG_SERVER\"] = \"http://10.2.0.165:8000\"\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, get_response_synthesizer\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "import textwrap\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "```\n",
    "\n",
    "```Python\n",
    "# Define the number of context sentences to consider for generating an answer.\n",
    "NUM_CONTEXT_SENTENCES = 20\n",
    "# Set the maximum length for each context sentence (in characters).\n",
    "MAX_CONTEXT_SENTENCE_LENGTH = 1000\n",
    "# Set the maximum context references length (in characters).\n",
    "MAX_CONTEXT_REFERENCES_LENGTH = 4000\n",
    "# Sentence Transformer Parameters\n",
    "SENTENTENCE_TRANSFORMER_BATCH_SIZE = 128 # TUNE THIS VARIABLE depending on the size of your embedding model and GPU mem available\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fTJAIMKnOfe0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-embeddings-huggingface 0.6.1 requires llama-index-core<0.15,>=0.13.0, but you have llama-index-core 0.12.52.post1 which is incompatible.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index 0.12.2 requires llama-index-core<0.13.0,>=0.12.2, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-llms-openai 0.3.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-multi-modal-llms-openai 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-agent-openai 0.4.8 requires llama-index-core<0.13,>=0.12.18, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-cli 0.4.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-embeddings-openai 0.3.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.8.0 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-program-openai 0.3.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-question-gen-openai 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-readers-file 0.4.11 requires llama-index-core<0.13,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-readers-llama-parse 0.4.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-readers-wikipedia 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "llama-index-tools-mcp 0.2.0 requires llama-index-core<0.13,>=0.12.37, but you have llama-index-core 0.14.13 which is incompatible.\n",
      "WARNING: trulens-core 1.5.3 does not provide the extra 'openai'\n",
      "WARNING: trulens-core 1.5.3 does not provide the extra 'otel'\n",
      "WARNING: trulens-core 1.5.3 does not provide the extra 'tqdm'\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\swsuser-j04\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: The script streamlit.exe is installed in 'C:\\Users\\swsuser-j04\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script streamlit.exe is installed in 'C:\\Users\\swsuser-j04\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-proto 1.36.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pandas 2.2.3\n",
      "Uninstalling pandas-2.2.3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\shutil.py\", line 847, in move\n",
      "    os.rename(src, real_dst)\n",
      "PermissionError: [WinError 5] ì•¡ì„¸ìŠ¤ê°€ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤: 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\env_aias_test\\\\lib\\\\site-packages\\\\pandas-2.2.3.dist-info\\\\' -> 'C:\\\\Users\\\\swsuser-j04\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-n9_bdp07'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\pip\\_internal\\commands\\uninstall.py\", line 106, in run\n",
      "    uninstall_pathset = req.uninstall(\n",
      "                        ^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 723, in uninstall\n",
      "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 370, in remove\n",
      "    moved.stash(path)\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 261, in stash\n",
      "    renames(path, new_path)\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 354, in renames\n",
      "    shutil.move(old, new)\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\shutil.py\", line 865, in move\n",
      "    rmtree(src)\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\shutil.py\", line 781, in rmtree\n",
      "    return _rmtree_unsafe(path, onexc)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\shutil.py\", line 635, in _rmtree_unsafe\n",
      "    onexc(os.unlink, fullname, err)\n",
      "  File \"C:\\ProgramData\\miniconda3\\envs\\env_aias_test\\Lib\\shutil.py\", line 633, in _rmtree_unsafe\n",
      "    os.unlink(fullname)\n",
      "PermissionError: [WinError 5] ì•¡ì„¸ìŠ¤ê°€ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤: 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\env_aias_test\\\\lib\\\\site-packages\\\\pandas-2.2.3.dist-info\\\\direct_url.json'\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "!pip install llama-index --quiet\n",
    "!pip install llama-index-readers-wikipedia wikipedia --quiet\n",
    "!pip install llama-index-llms-openai --quiet\n",
    "!pip install llama-index-embeddings-huggingface --quiet\n",
    "!pip install packaging==23.2 trulens trulens-providers-openai openai --quiet\n",
    "!pip install langchain nltk>=3.8.1 streamlit==1.35.0 watchdog kubernetes==26.1.0 --quiet\n",
    "\n",
    "!pip install blingfire beautifulsoup4 sentence-transformers ray --quiet\n",
    "!pip install textwrap3 --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip uninstall pandas scipy transformers -y\n",
    "!pip install pandas scipy transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S8Ig_PSVGW_G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swsuser-j04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "from typing import List\n",
    "import requests\n",
    "import numpy as np\n",
    "import bz2\n",
    "import json\n",
    "from blingfire import text_to_sentences_and_offsets\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" #copy your api key\n",
    "os.environ[\"CRAG_SERVER\"] = \" http://10.2.0.165:8000\"\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, get_response_synthesizer\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "import textwrap\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OcrU0dgdjLIE"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Define the number of context sentences to consider for generating an answer.\n",
    "NUM_CONTEXT_SENTENCES = 20\n",
    "# Set the maximum length for each context sentence (in characters).\n",
    "MAX_CONTEXT_SENTENCE_LENGTH = 1000\n",
    "# Set the maximum context references length (in characters).\n",
    "MAX_CONTEXT_REFERENCES_LENGTH = 4000\n",
    "# Sentence Transformer Parameters\n",
    "SENTENTENCE_TRANSFORMER_BATCH_SIZE = 128 # TUNE THIS VARIABLE depending on the size of your embedding model and GPU mem available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ouB4AuyGWsk"
   },
   "source": [
    "### 2. Preparing Mock APIs\n",
    "\n",
    "We have previously seen an example of an API that sends a query to the Mock KG and retrieves related information. Here, we will explain in more detail what types of APIs may exist.\n",
    "\n",
    "The example below demonstrates a Mock API used in the KDD Cup. Each method is designed to connect to the KG (represented as self.server) and send the required request using the requests library. Simultaneously, it receives the results and finally returns the data in **JSON format**.\n",
    "\n",
    "Here, the `requests` library is a Python package that simplifies making HTTP requests. Using methods like `GET `or `POST` from the requests library allows us to send HTTP requests, but it requires some understanding of computer networks, so we will skip the detailed explanation. If you want to learn more, I recommend researching it independently.\n",
    "\n",
    "In any case, this is the connection bridge we have been discussing with the Mock KG. Please review the code below and check which methods are available.\n",
    "\n",
    "```Python\n",
    "\n",
    "class CRAG(object):\n",
    "    def __init__(self, server = None):\n",
    "        self.server = os.environ.get('CRAG_SERVER', \" http://10.2.0.165:8000\")\n",
    "\n",
    "    def finance_get_company_name(self, query:str):\n",
    "        url = self.server + '/finance/get_company_name'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': query}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_ticker_by_name(self, query:str):\n",
    "        url = self.server + '/finance/get_ticker_by_name'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': query}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_price_history(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_price_history'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_detailed_price_history(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_detailed_price_history'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_dividends_history(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_dividends_history'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "    \n",
    "    def finance_get_market_capitalization(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_market_capitalization'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_eps(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_eps'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_pe_ratio(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_pe_ratio'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_info(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_info'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lpCFzMHMK9rV"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "class CRAG(object):\n",
    "    def __init__(self, server = None):\n",
    "        self.server = os.environ.get('CRAG_SERVER', \"http://10.2.0.165:8000\")\n",
    "\n",
    "    def finance_get_company_name(self, query:str):\n",
    "        url = self.server + '/finance/get_company_name'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': query}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_ticker_by_name(self, query:str):\n",
    "        url = self.server + '/finance/get_ticker_by_name'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': query}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_price_history(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_price_history'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_detailed_price_history(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_detailed_price_history'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_dividends_history(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_dividends_history'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_market_capitalization(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_market_capitalization'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_eps(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_eps'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_pe_ratio(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_pe_ratio'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_info(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_info'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMXL7xu_a9iX"
   },
   "source": [
    "The code below is a simple example that allows you to test one of the APIs mentioned above.  \n",
    "\n",
    "You can review the available methods and experiment freely as you wish.\n",
    "\n",
    "```\n",
    "def pretty_json_print(data):\n",
    "    json_string = json.dumps(data, indent=4)\n",
    "    lines = json_string.splitlines()\n",
    "    formatted_lines = \"\\n\\n\".join(lines)\n",
    "    print(formatted_lines)\n",
    "\n",
    "api = CRAG()\n",
    "\n",
    "metric = \"price\"\n",
    "\n",
    "result = api.finance_get_company_name(\"microsoft\")\n",
    "pretty_json_print(result)\n",
    "ticker_name = api.finance_get_ticker_by_name(result[\"result\"][0])\n",
    "pretty_json_print(ticker_name)\n",
    "\n",
    "if metric == 'price':\n",
    "    response = api.finance_get_price_history(ticker_name['result'])\n",
    "elif metric == 'dividend':\n",
    "    response = api.finance_get_dividends_history(ticker_name['result'])\n",
    "elif metric == 'p/e ratio':\n",
    "    response = api.finance_get_pe_ratio(ticker_name['result'])\n",
    "elif metric == 'eps':\n",
    "    response = api.finance_get_eps(ticker_name['result'])\n",
    "elif metric == 'marketcap' :\n",
    "    response = api.finance_get_market_capitalization(ticker_name['result'])\n",
    "else:\n",
    "    response = api.finance_get_info(ticker_name['result'])\n",
    "\n",
    "pretty_json_print(response)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s4vdZhiz7R1g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\n",
      "    \"result\": [\n",
      "\n",
      "        \"Microsoft Corporation Common Stock\"\n",
      "\n",
      "    ]\n",
      "\n",
      "}\n",
      "{\n",
      "\n",
      "    \"result\": \"MSFT\"\n",
      "\n",
      "}\n",
      "{\n",
      "\n",
      "    \"result\": 13.36\n",
      "\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "def pretty_json_print(data):\n",
    "    json_string = json.dumps(data, indent=4)\n",
    "    lines = json_string.splitlines()\n",
    "    formatted_lines = \"\\n\\n\".join(lines)\n",
    "    print(formatted_lines)\n",
    "\n",
    "api = CRAG()\n",
    "\n",
    "metric = \"eps\"\n",
    "\n",
    "result = api.finance_get_company_name(\"microsoft\")\n",
    "pretty_json_print(result)\n",
    "ticker_name = api.finance_get_ticker_by_name(result[\"result\"][0])\n",
    "pretty_json_print(ticker_name)\n",
    "\n",
    "if metric == 'price':\n",
    "    response = api.finance_get_price_history(ticker_name['result'])\n",
    "elif metric == 'dividend':\n",
    "    response = api.finance_get_dividends_history(ticker_name['result'])\n",
    "elif metric == 'p/e ratio':\n",
    "    response = api.finance_get_pe_ratio(ticker_name['result'])\n",
    "elif metric == 'eps':\n",
    "    response = api.finance_get_eps(ticker_name['result'])\n",
    "elif metric == 'marketcap' :\n",
    "    response = api.finance_get_market_capitalization(ticker_name['result'])\n",
    "else:\n",
    "    response = api.finance_get_info(ticker_name['result'])\n",
    "\n",
    "pretty_json_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZNPYuw0K9KC"
   },
   "source": [
    "### 3. Implementing a Query Generator\n",
    "\n",
    "Creating the API, as shown above, is a good step. However, the important point is that the API requires a specific input format to function correctly.\n",
    "\n",
    "Unfortunately, the CRAG dataset questions do not explicitly indicate which words or terms can be used as inputs for the API. To use the API effectively, we need to extract or generate the required terms from the question and provide them as inputs to the API.\n",
    "\n",
    "There are various methods for this task. For example, in the field of **Named Entity Recognition (NER)**, techniques are studied to extract important words (entities) from a given sentence. Using a model developed in this field could be an excellent approach.\n",
    "\n",
    "However, this would require loading additional models. Fortunately, we already have an LLM. Therefore, we can instruct the LLM to directly extract elements that can serve as inputs for the API.\n",
    "\n",
    "Below is the prompt provided to the LLM for this purpose. It includes a breakdown of which entities are usable as API inputs for each domain. Reviewing this breakdown would also be a valuable exercise.\n",
    "\n",
    "#### Designing prompts\n",
    "\n",
    "```\n",
    "entity_extract_template = \"\"\"\n",
    "You are given a Query and Query Time. Do the following:\n",
    "\n",
    "1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domain applies, use \"other\". Use \"domain\" as the key in the result json.\n",
    "\n",
    "2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n",
    "\n",
    "For `finance` queries, these are possible keys:\n",
    "- `market_identifier`: stock identifiers including individual company names, stock symbols.\n",
    "- `metric`: financial metrics that the query is asking about. This must be one of the following: `price`, `dividend`, `P/E ratio`, `EPS`, `marketCap`, and `other`.\n",
    "- `datetime`: time frame that query asks about. When datetime is not explicitly mentioned, use `Query Time` as default.\n",
    "\n",
    "\n",
    "Return the results in a FLAT json.\n",
    "\n",
    "*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON*  \n",
    "\"\"\"\n",
    "```\n",
    "```\n",
    "def prompt_generator(query):\n",
    "    user_message = \"\"\n",
    "    user_message += f\"Query: {query}\\n\"\n",
    "        \n",
    "    llm_input = [\n",
    "      {\"role\": \"system\", \"content\": entity_extract_template},\n",
    "      {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    return llm_input\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D3sX7arBYmKy"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "entity_extract_template = \"\"\"\n",
    "You are given a Query and Query Time. Do the following:\n",
    "\n",
    "1) Determine the domain the query is about. The domain should be one of the following: \"finance\", \"sports\", \"music\", \"movie\", \"encyclopedia\". If none of the domain applies, use \"other\". Use \"domain\" as the key in the result json.\n",
    "\n",
    "2) Extract structured information from the query. Include different keys into the result json depending on the domains, and put them DIRECTLY in the result json. Here are the rules:\n",
    "\n",
    "For `finance` queries, these are possible keys:\n",
    "- `market_identifier`: stock identifiers including individual company names, stock symbols.\n",
    "- `metric`: financial metrics that the query is asking about. This must be one of the following: `price`, `dividend`, `P/E ratio`, `EPS`, `marketCap`, and `other`.\n",
    "- `datetime`: time frame that query asks about. When datetime is not explicitly mentioned, use `Query Time` as default.\n",
    "\n",
    "\n",
    "Return the results in a FLAT json.\n",
    "\n",
    "*NEVER include ANY EXPLANATION or NOTE in the output, ONLY OUTPUT JSON*\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5685qgMxnQPs"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "def prompt_generator(query):\n",
    "    user_message = \"\"\n",
    "    user_message += f\"Query: {query}\\n\"\n",
    "\n",
    "    llm_input = [\n",
    "      {\"role\": \"system\", \"content\": entity_extract_template},\n",
    "      {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    return llm_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5gciIY8Agqe"
   },
   "source": [
    "#### Generating Queries\n",
    "\n",
    "Now, letâ€™s actually deliver this prompt to the LLM and generate the query that will be sent to the API.\n",
    "\n",
    "```python\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from json import JSONDecoder\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "def generate_query(query):\n",
    "    llm_input = prompt_generator(query)\n",
    "    completion = oai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    messages=\n",
    "    llm_input\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        completion = json.loads(completion)\n",
    "    except:\n",
    "        completion = extract_json_objects(completion)\n",
    "    \n",
    "    if \"domain\" in completion.keys():\n",
    "        domain = completion[\"domain\"]\n",
    "        is_finance = domain == \"finance\"\n",
    "    else:\n",
    "        is_finance = False\n",
    "\n",
    "    return completion, is_finance\n",
    "\n",
    "def extract_json_objects(text, decoder=JSONDecoder()):\n",
    "    \"\"\"Find JSON objects in text, and yield the decoded JSON data\n",
    "    \"\"\"\n",
    "    pos = 0\n",
    "    results = []\n",
    "    while True:\n",
    "        match = text.find(\"{\", pos)\n",
    "        if match == -1:\n",
    "            break\n",
    "        try:\n",
    "            result, index = decoder.raw_decode(text[match:])\n",
    "            results.append(result)\n",
    "            pos = match + index\n",
    "        except ValueError:\n",
    "            pos = match + 1\n",
    "    return results\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yYBkbTyEAtfu"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from json import JSONDecoder\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "def generate_query(query):\n",
    "    llm_input = prompt_generator(query)\n",
    "    completion = oai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    messages=\n",
    "    llm_input\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        completion = json.loads(completion)\n",
    "    except:\n",
    "        completion = extract_json_objects(completion)\n",
    "\n",
    "    if \"domain\" in completion.keys():\n",
    "        domain = completion[\"domain\"]\n",
    "        is_finance = domain == \"finance\"\n",
    "    else:\n",
    "        is_finance = False\n",
    "\n",
    "    return completion, is_finance\n",
    "\n",
    "def extract_json_objects(text, decoder=JSONDecoder()):\n",
    "    \"\"\"Find JSON objects in text, and yield the decoded JSON data\n",
    "    \"\"\"\n",
    "    pos = 0\n",
    "    results = []\n",
    "    while True:\n",
    "        match = text.find(\"{\", pos)\n",
    "        if match == -1:\n",
    "            break\n",
    "        try:\n",
    "            result, index = decoder.raw_decode(text[match:])\n",
    "            results.append(result)\n",
    "            pos = match + index\n",
    "        except ValueError:\n",
    "            pos = match + 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ijz-ttBDdJfN"
   },
   "source": [
    "Letâ€™s verify whether the above code works correctly in practice.\n",
    "\n",
    "\n",
    "```\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat not in [14, 53, 64]:\n",
    "          repeat += 1\n",
    "          continue\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        generated_query = generate_query(item['query'])\n",
    "        print(f\"generated_query: {generated_query}\")\n",
    "        repeat += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rQAwAyvAvh4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: what is the ex-dividend date of microsoft in the 1st qtr of 2024\n",
      "generated_query: ({'domain': 'finance', 'market_identifier': 'microsoft', 'metric': 'dividend', 'datetime': '1st qtr of 2024'}, True)\n",
      "query: i'm looking for the p/e ratio of dks. would you happen to know what it is?\n",
      "generated_query: ({'domain': 'finance', 'market_identifier': 'dks', 'metric': 'P/E ratio', 'datetime': 'Query Time'}, True)\n",
      "query: what's auph's earnings per share?\n",
      "generated_query: ({'domain': 'finance', 'market_identifier': 'auph', 'metric': 'EPS', 'datetime': 'Query Time'}, True)\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"./crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat not in [14, 53, 64]:\n",
    "          repeat += 1\n",
    "          continue\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        generated_query = generate_query(item['query'])\n",
    "        print(f\"generated_query: {generated_query}\")\n",
    "        repeat += 1\n",
    "\n",
    "# print(f\"generated_query: {generated_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFogBOkgfTkl"
   },
   "source": [
    "### 4. Implementing a Query Executor\n",
    "\n",
    "We are now at the most challenging step. Even if we generate a query to send to the API, we cannot directly pass it as is.\n",
    "\n",
    "Therefore, it is necessary to transform the generated results into a format where only the required elements are sent to the API.\n",
    "\n",
    "Additionally, simply being able to call the API does not solve the problem. Many of the data points in the CRAG dataset require multiple pieces of information. As a result, a single API call may not provide sufficient information to generate an answer.\n",
    "\n",
    "To address this, we need to predefine a **decision tree** outlining how we will use and analyze the API. With this decision tree set, the necessary code can then be written.\n",
    "\n",
    "For now, letâ€™s declare some **utility functions** that may be needed later. These functions will transform sentences into a fixed format using methods like pattern matching or other techniques.\n",
    "\n",
    "```python\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "def normalize_key(key):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', key).lower()\n",
    "\n",
    "def get_metric_from_response(response, metric):\n",
    "    normalized_metric = normalize_key(metric)\n",
    "    if response != None:\n",
    "        for key, value in response.items():\n",
    "            if normalize_key(key) == normalized_metric:\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "\n",
    "def convert_to_standard_format(date_string):\n",
    "    try:\n",
    "        dt = parser.parse(date_string)\n",
    "        \n",
    "        est = pytz.timezone('US/Eastern')\n",
    "        \n",
    "        if dt.tzinfo is None:\n",
    "            dt = est.localize(dt)\n",
    "        else:\n",
    "            dt = dt.astimezone(est)\n",
    "        dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        \n",
    "        formatted_date = dt.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "        return formatted_date\n",
    "    except (ValueError, OverflowError) as e:\n",
    "        return date_string\n",
    "\n",
    "def add_one_day(date_string):\n",
    "    try:\n",
    "        dt = parser.parse(date_string)\n",
    "        \n",
    "        est = pytz.timezone('US/Eastern')\n",
    "\n",
    "        if dt.tzinfo is None:\n",
    "            dt = est.localize(dt)\n",
    "        else:\n",
    "            dt = dt.astimezone(est)\n",
    "        \n",
    "        dt_plus_one = dt + timedelta(days=1)\n",
    "        dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        formatted_date = dt_plus_one.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "        return formatted_date\n",
    "    except (ValueError, OverflowError) as e:\n",
    "        return f\"Invalid date string: {e}\"\n",
    "\n",
    "def subtract_one_day(date_string):\n",
    "    try:\n",
    "        dt = parser.parse(date_string)\n",
    "        \n",
    "        est = pytz.timezone('US/Eastern')\n",
    "        \n",
    "        if dt.tzinfo is None:\n",
    "            dt = est.localize(dt)\n",
    "        else:\n",
    "            dt = dt.astimezone(est)\n",
    "        \n",
    "        dt_minus_one = dt - timedelta(days=1)\n",
    "        dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        formatted_date = dt_minus_one.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "        return formatted_date\n",
    "    except (ValueError, OverflowError) as e:\n",
    "        return f\"Invalid date string: {e}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Se8-iuwzJeZh"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil import parser\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "def normalize_key(key):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', key).lower()\n",
    "\n",
    "def get_metric_from_response(response, metric):\n",
    "    normalized_metric = normalize_key(metric)\n",
    "    if response != None:\n",
    "        for key, value in response.items():\n",
    "            if normalize_key(key) == normalized_metric:\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "\n",
    "def convert_to_standard_format(date_string):\n",
    "    try:\n",
    "        dt = parser.parse(date_string)\n",
    "\n",
    "        est = pytz.timezone('US/Eastern')\n",
    "\n",
    "        if dt.tzinfo is None:\n",
    "            dt = est.localize(dt)\n",
    "        else:\n",
    "            dt = dt.astimezone(est)\n",
    "        dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "        formatted_date = dt.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "        return formatted_date\n",
    "    except (ValueError, OverflowError) as e:\n",
    "        return date_string\n",
    "\n",
    "def add_one_day(date_string):\n",
    "    try:\n",
    "        dt = parser.parse(date_string)\n",
    "\n",
    "        est = pytz.timezone('US/Eastern')\n",
    "\n",
    "        if dt.tzinfo is None:\n",
    "            dt = est.localize(dt)\n",
    "        else:\n",
    "            dt = dt.astimezone(est)\n",
    "\n",
    "        dt_plus_one = dt + timedelta(days=1)\n",
    "        dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        formatted_date = dt_plus_one.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "        return formatted_date\n",
    "    except (ValueError, OverflowError) as e:\n",
    "        return f\"Invalid date string: {e}\"\n",
    "\n",
    "def subtract_one_day(date_string):\n",
    "    try:\n",
    "        dt = parser.parse(date_string)\n",
    "\n",
    "        est = pytz.timezone('US/Eastern')\n",
    "\n",
    "        if dt.tzinfo is None:\n",
    "            dt = est.localize(dt)\n",
    "        else:\n",
    "            dt = dt.astimezone(est)\n",
    "\n",
    "        dt_minus_one = dt - timedelta(days=1)\n",
    "        dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        formatted_date = dt_minus_one.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "        return formatted_date\n",
    "    except (ValueError, OverflowError) as e:\n",
    "        return f\"Invalid date string: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQh7LpvnhCJd"
   },
   "source": [
    "\n",
    "Now, letâ€™s write a function that follows the **decision tree** we established to interact with the API and process its results. A decision tree is a decision support structure that uses a tree-like model of decisions and their possible consequences, including outcomes, costs, and utility. We'll be using our pre-built API in this manner.\n",
    "\n",
    "<img src=\"https://i.imgur.com/F7lfJQq.png\">\n",
    "\n",
    "Our search process will proceed as follows:  \n",
    "\n",
    "1.\tExtract relevant entities from the question that are required to use the API.\n",
    "2.\tBased on the extracted entities, determine the necessary parameters to call finance-related APIs.\n",
    "3.\tUse the identified parameters to call the relevant API and retrieve the results.\n",
    "4.\tFilter the results to extract only the relevant information.\n",
    "\n",
    "By following these steps, we will ultimately obtain the kg_results, which contain the relevant information extracted from the Knowledge Graph (KG).\n",
    "\n",
    "```python\n",
    "import copy\n",
    "\n",
    "def get_finance_kg_results(generated_query):\n",
    "    formatted_time_list = []\n",
    "    if 'datetime' in generated_query:\n",
    "        datetime_list = generated_query['datetime'].split(' - ')\n",
    "        for datetime in datetime_list:\n",
    "            formatted_time_list.append(convert_to_standard_format(datetime.strip()))\n",
    "\n",
    "\n",
    "    kg_results = []\n",
    "    res = \"\"\n",
    "    if \"market_identifier\" in generated_query.keys() and generated_query[\"market_identifier\"] is not None:\n",
    "        if isinstance(generated_query[\"market_identifier\"], str):\n",
    "            company_names = generated_query[\"market_identifier\"].split(\",\")\n",
    "        else:\n",
    "            company_names = generated_query[\"market_identifier\"]\n",
    "\n",
    "        for company_name in company_names:\n",
    "            try:\n",
    "                res = api.finance_get_company_name(company_name)[\"result\"]\n",
    "\n",
    "                if res == []:\n",
    "                    ticker_name = company_name.upper()\n",
    "                else:\n",
    "                    ticker_name = api.finance_get_ticker_by_name(res[0])[\"result\"]\n",
    "\n",
    "                if generated_query['metric'].lower().strip() == 'price':\n",
    "                    response = api.finance_get_price_history(ticker_name)['result']\n",
    "                elif generated_query['metric'].lower().strip() == 'dividend':\n",
    "                    response = api.finance_get_dividends_history(ticker_name)['result']\n",
    "                elif generated_query['metric'].lower().strip() == 'p/e ratio':\n",
    "                    response = api.finance_get_pe_ratio(ticker_name)['result']\n",
    "                elif generated_query['metric'].lower().strip() == 'eps':\n",
    "                    response = api.finance_get_eps(ticker_name)[\"result\"]\n",
    "                elif generated_query['metric'].lower().strip() == 'marketcap' :\n",
    "                    response = api.finance_get_market_capitalization(ticker_name)['result']\n",
    "                else:\n",
    "                    response = api.finance_get_info(ticker_name)['result']\n",
    "                    metric_value = get_metric_from_response(response, generated_query['metric'])\n",
    "                    if metric_value is not None:\n",
    "                        response = metric_value\n",
    "\n",
    "                try:\n",
    "                    for formatted_time in formatted_time_list:\n",
    "                        if formatted_time in response:\n",
    "                            filtered_response = copy.deepcopy(response[formatted_time])\n",
    "                        elif add_one_day(formatted_time) in response:\n",
    "                            filtered_response = copy.deepcopy(response[add_one_day(formatted_time)])\n",
    "                        elif subtract_one_day(formatted_time) in response:\n",
    "                            filtered_response = copy.deepcopy(response[subtract_one_day(formatted_time)])\n",
    "                        else:\n",
    "                            filtered_response = copy.deepcopy(response)\n",
    "                        kg_results.append({company_name + \" \" + generated_query[\"metric\"]: filtered_response, 'time': formatted_time})\n",
    "                except:\n",
    "                    kg_results.append({company_name + \" \" + generated_query[\"metric\"]: response})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Fail to parse the generated query\")\n",
    "                pass\n",
    "\n",
    "    kg_results = \"<DOC>\\n\".join([str(res) for res in kg_results]) if len(kg_results) > 0 else \"\"\n",
    "    return  kg_results\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7JWrpozJegU"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "import copy\n",
    "\n",
    "def get_finance_kg_results(generated_query):\n",
    "    formatted_time_list = []\n",
    "    if 'datetime' in generated_query:\n",
    "        datetime_list = generated_query['datetime'].split(' - ')\n",
    "        for datetime in datetime_list:\n",
    "            formatted_time_list.append(convert_to_standard_format(datetime.strip()))\n",
    "\n",
    "\n",
    "    kg_results = []\n",
    "    res = \"\"\n",
    "    if \"market_identifier\" in generated_query.keys() and generated_query[\"market_identifier\"] is not None:\n",
    "        if isinstance(generated_query[\"market_identifier\"], str):\n",
    "            company_names = generated_query[\"market_identifier\"].split(\",\")\n",
    "        else:\n",
    "            company_names = generated_query[\"market_identifier\"]\n",
    "\n",
    "        for company_name in company_names:\n",
    "            try:\n",
    "              # Microsoft\n",
    "# {\n",
    "\n",
    "#     \"result\": [\n",
    "\n",
    "#         \"Microsoft Corporation Common Stock\"\n",
    "\n",
    "#     ]\n",
    "\n",
    "# }\n",
    "# {\n",
    "\n",
    "#     \"result\": \"MSFT\"\n",
    "\n",
    "# }\n",
    "# {\n",
    "\n",
    "#     \"result\": 13.36\n",
    "\n",
    "# }\n",
    "                res = api.finance_get_company_name(company_name)[\"result\"]\n",
    "\n",
    "                if res == []:\n",
    "                    ticker_name = company_name.upper()\n",
    "                else:\n",
    "                    ticker_name = api.finance_get_ticker_by_name(res[0])[\"result\"]\n",
    "\n",
    "                if generated_query['metric'].lower().strip() == 'price':\n",
    "                    response = api.finance_get_price_history(ticker_name)['result']\n",
    "                elif generated_query['metric'].lower().strip() == 'dividend':\n",
    "                    response = api.finance_get_dividends_history(ticker_name)['result']\n",
    "                elif generated_query['metric'].lower().strip() == 'p/e ratio':\n",
    "                    response = api.finance_get_pe_ratio(ticker_name)['result']\n",
    "                elif generated_query['metric'].lower().strip() == 'eps':\n",
    "                    response = api.finance_get_eps(ticker_name)[\"result\"]\n",
    "                elif generated_query['metric'].lower().strip() == 'marketcap' :\n",
    "                    response = api.finance_get_market_capitalization(ticker_name)['result']\n",
    "                else:\n",
    "                    response = api.finance_get_info(ticker_name)['result']\n",
    "                    metric_value = get_metric_from_response(response, generated_query['metric'])\n",
    "                    if metric_value is not None:\n",
    "                        response = metric_value\n",
    "\n",
    "                try:\n",
    "                    for formatted_time in formatted_time_list:\n",
    "                        if formatted_time in response:\n",
    "                            filtered_response = copy.deepcopy(response[formatted_time])\n",
    "                        elif add_one_day(formatted_time) in response:\n",
    "                            filtered_response = copy.deepcopy(response[add_one_day(formatted_time)])\n",
    "                        elif subtract_one_day(formatted_time) in response:\n",
    "                            filtered_response = copy.deepcopy(response[subtract_one_day(formatted_time)])\n",
    "                        else:\n",
    "                            filtered_response = copy.deepcopy(response)\n",
    "                        kg_results.append({company_name + \" \" + generated_query[\"metric\"]: filtered_response, 'time': formatted_time})\n",
    "                except:\n",
    "                    kg_results.append({company_name + \" \" + generated_query[\"metric\"]: response})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Fail to parse the generated query\")\n",
    "                pass\n",
    "\n",
    "    kg_results = \"<DOC>\\n\".join([str(res) for res in kg_results]) if len(kg_results) > 0 else \"\"\n",
    "    return  kg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyddBzT7iTvi"
   },
   "source": [
    "\n",
    "Letâ€™s now verify whether we can obtain the kg_results using real data by executing the pipeline we just declared.  \n",
    "\n",
    "This will confirm that our process, from entity extraction to API interaction, works correctly.  \n",
    "\n",
    "```\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "\n",
    "        if item[\"domain\"] == \"finance\":\n",
    "          generated_query, is_finance = generate_query(item['query'])\n",
    "          if is_finance:\n",
    "            print(\"generated_query: \", generated_query)\n",
    "            kg_results = get_finance_kg_results(generated_query)\n",
    "            if kg_results not in [\"\", None]:\n",
    "                break\n",
    "\n",
    "print(f\"kg_results: {kg_results}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J32vKTgIiO9z"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "\n",
    "        if item[\"domain\"] == \"finance\":\n",
    "          generated_query, is_finance = generate_query(item['query'])\n",
    "          if is_finance:\n",
    "            print(\"generated_query: \", generated_query)\n",
    "            kg_results = get_finance_kg_results(generated_query)\n",
    "            if kg_results not in [\"\", None]:\n",
    "                break\n",
    "\n",
    "print(f\"kg_results: {kg_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysqxYZQBJmqB"
   },
   "source": [
    "###5. Implementing a Mock KG Query Engine\n",
    "\n",
    "Letâ€™s now define the `KGQueryEngine` that brings together all the components we have implemented.\n",
    "\n",
    "When a query is provided, the `KGQueryEngine` will interact with the Knowledge Graph (KG) to retrieve the relevant information.\n",
    "\n",
    "\n",
    "```\n",
    "class KGQueryEngine:\n",
    "    def query(self, query):\n",
    "        generated_query, is_finance = self.generate_query(query)\n",
    "\n",
    "        if is_finance:\n",
    "            kg_results = self.get_finance_kg_results(generated_query)\n",
    "        else:\n",
    "            kg_results = \"\"\n",
    "\n",
    "        return kg_results\n",
    "\n",
    "    def generate_query(self, query):\n",
    "        llm_input = prompt_generator(query)\n",
    "        completion = oai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        messages=\n",
    "        llm_input\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            completion = json.loads(completion)\n",
    "        except:\n",
    "            completion = extract_json_objects(completion)\n",
    "\n",
    "        if \"domain\" in completion.keys():\n",
    "            domain = completion[\"domain\"]\n",
    "            is_finance = domain == \"finance\"\n",
    "        else:\n",
    "            is_finance = False\n",
    "\n",
    "        return completion, is_finance\n",
    "\n",
    "    def get_finance_kg_results(self, generated_query):\n",
    "        formatted_time_list = []\n",
    "        if 'datetime' in generated_query:\n",
    "            datetime_list = generated_query['datetime'].split(' - ')\n",
    "            for datetime in datetime_list:\n",
    "                formatted_time_list.append(convert_to_standard_format(datetime.strip()))\n",
    "\n",
    "\n",
    "        kg_results = []\n",
    "        res = \"\"\n",
    "        if \"market_identifier\" in generated_query.keys() and generated_query[\"market_identifier\"] is not None:\n",
    "            if isinstance(generated_query[\"market_identifier\"], str):\n",
    "                company_names = generated_query[\"market_identifier\"].split(\",\")\n",
    "            else:\n",
    "                company_names = generated_query[\"market_identifier\"]\n",
    "\n",
    "            for company_name in company_names:\n",
    "                try:\n",
    "                    res = api.finance_get_company_name(company_name)[\"result\"]\n",
    "\n",
    "                    if res == []:\n",
    "                        ticker_name = company_name.upper()\n",
    "                    else:\n",
    "                        ticker_name = api.finance_get_ticker_by_name(res[0])[\"result\"]\n",
    "\n",
    "                    if generated_query['metric'].lower().strip() == 'price':\n",
    "                        response = api.finance_get_price_history(ticker_name)['result']\n",
    "                    elif generated_query['metric'].lower().strip() == 'dividend':\n",
    "                        response = api.finance_get_dividends_history(ticker_name)['result']\n",
    "                    elif generated_query['metric'].lower().strip() == 'p/e ratio':\n",
    "                        response = api.finance_get_pe_ratio(ticker_name)['result']\n",
    "                    elif generated_query['metric'].lower().strip() == 'eps':\n",
    "                        response = api.finance_get_eps(ticker_name)[\"result\"]\n",
    "                    elif generated_query['metric'].lower().strip() == 'marketcap' :\n",
    "                        response = api.finance_get_market_capitalization(ticker_name)['result']\n",
    "                    else:\n",
    "                        response = api.finance_get_info(ticker_name)['result']\n",
    "                        metric_value = get_metric_from_response(response, generated_query['metric'])\n",
    "                        if metric_value is not None:\n",
    "                            response = metric_value\n",
    "\n",
    "                    try:\n",
    "                        for formatted_time in formatted_time_list:\n",
    "                            if formatted_time in response:\n",
    "                                filtered_response = copy.deepcopy(response[formatted_time])\n",
    "                            elif add_one_day(formatted_time) in response:\n",
    "                                filtered_response = copy.deepcopy(response[add_one_day(formatted_time)])\n",
    "                            elif subtract_one_day(formatted_time) in response:\n",
    "                                filtered_response = copy.deepcopy(response[subtract_one_day(formatted_time)])\n",
    "                            else:\n",
    "                                filtered_response = copy.deepcopy(response)\n",
    "                            kg_results.append({company_name + \" \" + generated_query[\"metric\"]: filtered_response, 'time': formatted_time})\n",
    "                    except:\n",
    "                        kg_results.append({company_name + \" \" + generated_query[\"metric\"]: response})\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Fail to parse the generated query\")\n",
    "                    pass\n",
    "\n",
    "        kg_results = \"<DOC>\\n\".join([str(res) for res in kg_results]) if len(kg_results) > 0 else \"\"\n",
    "        return  kg_results\n",
    "\n",
    "    def prompt_generator(self, query):\n",
    "        user_message = \"\"\n",
    "        user_message += f\"Query: {query}\\n\"\n",
    "\n",
    "        llm_input = [\n",
    "          {\"role\": \"system\", \"content\": entity_extract_template},\n",
    "          {\"role\": \"user\", \"content\": user_message},\n",
    "        ]\n",
    "\n",
    "        return llm_input\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FfLcfSsMHYR"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "class KGQueryEngine:\n",
    "    def query(self, query):\n",
    "        generated_query, is_finance = self.generate_query(query)\n",
    "\n",
    "        if is_finance:\n",
    "            kg_results = self.get_finance_kg_results(generated_query)\n",
    "        else:\n",
    "            kg_results = \"\"\n",
    "\n",
    "        return kg_results, is_finance\n",
    "\n",
    "    def generate_query(self, query):\n",
    "        llm_input = prompt_generator(query)\n",
    "        completion = oai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        messages=\n",
    "        llm_input\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            completion = json.loads(completion)\n",
    "        except:\n",
    "            completion = extract_json_objects(completion)\n",
    "\n",
    "        if \"domain\" in completion.keys():\n",
    "            domain = completion[\"domain\"]\n",
    "            is_finance = domain == \"finance\"\n",
    "        else:\n",
    "            is_finance = False\n",
    "\n",
    "        return completion, is_finance\n",
    "\n",
    "    def get_finance_kg_results(self, generated_query):\n",
    "        formatted_time_list = []\n",
    "        if 'datetime' in generated_query:\n",
    "            datetime_list = generated_query['datetime'].split(' - ')\n",
    "            for datetime in datetime_list:\n",
    "                formatted_time_list.append(convert_to_standard_format(datetime.strip()))\n",
    "\n",
    "\n",
    "        kg_results = []\n",
    "        res = \"\"\n",
    "        if \"market_identifier\" in generated_query.keys() and generated_query[\"market_identifier\"] is not None:\n",
    "            if isinstance(generated_query[\"market_identifier\"], str):\n",
    "                company_names = generated_query[\"market_identifier\"].split(\",\")\n",
    "            else:\n",
    "                company_names = generated_query[\"market_identifier\"]\n",
    "\n",
    "            for company_name in company_names:\n",
    "                try:\n",
    "                    res = api.finance_get_company_name(company_name)[\"result\"]\n",
    "\n",
    "                    if res == []:\n",
    "                        ticker_name = company_name.upper()\n",
    "                    else:\n",
    "                        ticker_name = api.finance_get_ticker_by_name(res[0])[\"result\"]\n",
    "\n",
    "                    if generated_query['metric'].lower().strip() == 'price':\n",
    "                        response = api.finance_get_price_history(ticker_name)['result']\n",
    "                    elif generated_query['metric'].lower().strip() == 'dividend':\n",
    "                        response = api.finance_get_dividends_history(ticker_name)['result']\n",
    "                    elif generated_query['metric'].lower().strip() == 'p/e ratio':\n",
    "                        response = api.finance_get_pe_ratio(ticker_name)['result']\n",
    "                    elif generated_query['metric'].lower().strip() == 'eps':\n",
    "                        response = api.finance_get_eps(ticker_name)[\"result\"]\n",
    "                    elif generated_query['metric'].lower().strip() == 'marketcap' :\n",
    "                        response = api.finance_get_market_capitalization(ticker_name)['result']\n",
    "                    else:\n",
    "                        response = api.finance_get_info(ticker_name)['result']\n",
    "                        metric_value = get_metric_from_response(response, generated_query['metric'])\n",
    "                        if metric_value is not None:\n",
    "                            response = metric_value\n",
    "\n",
    "                    try:\n",
    "                        for formatted_time in formatted_time_list:\n",
    "                            if formatted_time in response:\n",
    "                                filtered_response = copy.deepcopy(response[formatted_time])\n",
    "                            elif add_one_day(formatted_time) in response:\n",
    "                                filtered_response = copy.deepcopy(response[add_one_day(formatted_time)])\n",
    "                            elif subtract_one_day(formatted_time) in response:\n",
    "                                filtered_response = copy.deepcopy(response[subtract_one_day(formatted_time)])\n",
    "                            else:\n",
    "                                filtered_response = copy.deepcopy(response)\n",
    "                            kg_results.append({company_name + \" \" + generated_query[\"metric\"]: filtered_response, 'time': formatted_time})\n",
    "                    except:\n",
    "                        kg_results.append({company_name + \" \" + generated_query[\"metric\"]: response})\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Fail to parse the generated query\")\n",
    "                    pass\n",
    "\n",
    "        kg_results = \"<DOC>\\n\".join([str(res) for res in kg_results]) if len(kg_results) > 0 else \"\"\n",
    "        return  kg_results\n",
    "\n",
    "    def prompt_generator(self, query):\n",
    "        user_message = \"\"\n",
    "        user_message += f\"Query: {query}\\n\"\n",
    "\n",
    "        llm_input = [\n",
    "          {\"role\": \"system\", \"content\": entity_extract_template},\n",
    "          {\"role\": \"user\", \"content\": user_message},\n",
    "        ]\n",
    "\n",
    "        return llm_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDbMms2ApXZm"
   },
   "source": [
    "## II. Implementing a Reader\n",
    "\n",
    "Before defining RAG with KG, letâ€™s finalize the `Reader`. This `Reader` will be the same as the one you used in Task 1.\n",
    "\n",
    "Hereâ€™s a reminder of the structure:\n",
    "\n",
    "```Python\n",
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "class Reader:\n",
    "  def __init__(self):\n",
    "\n",
    "    self.system_prompt = \"\"\"\n",
    "    You are provided with a question and various references.\n",
    "    Your task is to answer the question succinctly, using the fewest words possible.\n",
    "    If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n",
    "    There is no need to explain the reasoning behind your answers.\n",
    "    \"\"\"\n",
    "\n",
    "  def generate_response(self, question: str, top_k_chunks: list) -> str:\n",
    "      \"\"\"\n",
    "      Generate answer from context.\n",
    "      \"\"\"\n",
    "      llm_input = self.prompt_generator(question, top_k_chunks)\n",
    "      completion = oai_client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      temperature=0,\n",
    "      messages=\n",
    "      llm_input\n",
    "      ).choices[0].message.content\n",
    "      return completion\n",
    "\n",
    "  def prompt_generator(self, query, top_k_chunks):\n",
    "      user_message = \"\"\n",
    "      references = \"\"\n",
    "\n",
    "      if len(top_k_chunks) > 0:\n",
    "          references += \"# References \\n\"\n",
    "          # Format the top sentences as references in the model's prompt template.\n",
    "          for chunk_id, chunk in enumerate(top_k_chunks):\n",
    "              references += f\"- {chunk.strip()}\\n\"\n",
    "\n",
    "      references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n",
    "      # Limit the length of references to fit the model's input size.\n",
    "\n",
    "      user_message += f\"{references}\\n------\\n\\n\"\n",
    "      user_message\n",
    "      user_message += f\"Using only the references listed above, answer the following question: \\n\"\n",
    "      user_message += f\"Question: {query}\\n\"\n",
    "\n",
    "      llm_input = [\n",
    "        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "      ]\n",
    "\n",
    "      return llm_input\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J59VqE5-al91"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "class Reader:\n",
    "  def __init__(self):\n",
    "\n",
    "    self.system_prompt = \"\"\"\n",
    "    You are provided with a question and various references.\n",
    "    Your task is to answer the question succinctly, using the fewest words possible.\n",
    "    If the references do not contain the necessary information to answer the question, respond with 'I don't know'.\n",
    "    There is no need to explain the reasoning behind your answers.\n",
    "    \"\"\"\n",
    "\n",
    "  def generate_response(self, question: str, top_k_chunks: list) -> str:\n",
    "      \"\"\"\n",
    "      Generate answer from context.\n",
    "      \"\"\"\n",
    "      llm_input = self.prompt_generator(question, top_k_chunks)\n",
    "      completion = oai_client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      temperature=0,\n",
    "      messages=\n",
    "      llm_input\n",
    "      ).choices[0].message.content\n",
    "      return completion\n",
    "\n",
    "  def prompt_generator(self, query, top_k_chunks):\n",
    "      user_message = \"\"\n",
    "      references = \"\"\n",
    "\n",
    "      if len(top_k_chunks) > 0:\n",
    "          references += \"# References \\n\"\n",
    "          # Format the top sentences as references in the model's prompt template.\n",
    "          for chunk_id, chunk in enumerate(top_k_chunks):\n",
    "              references += f\"- {chunk.strip()}\\n\"\n",
    "\n",
    "      references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n",
    "      # Limit the length of references to fit the model's input size.\n",
    "\n",
    "      user_message += f\"{references}\\n------\\n\\n\"\n",
    "      user_message\n",
    "      user_message += f\"Using only the references listed above, answer the following question: \\n\"\n",
    "      user_message += f\"Question: {query}\\n\"\n",
    "\n",
    "      llm_input = [\n",
    "        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "      ]\n",
    "\n",
    "      return llm_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYow2o9_jd_w"
   },
   "source": [
    "Letâ€™s verify once again that the model is functioning correctly.\n",
    "\n",
    "Run the following test to ensure all components are working as expected.\n",
    "\n",
    "```\n",
    "reader = Reader()\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer = reader.generate_response(item['query'], [])\n",
    "        break\n",
    "\n",
    "print(f\"answer: {answer}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaPEFeZ92hh6"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "reader = Reader()\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer = reader.generate_response(item['query'], [])\n",
    "        break\n",
    "\n",
    "print(f\"answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVHLibfMGMy_"
   },
   "source": [
    "## III. Implementing an LLM + Mock KG\n",
    "\n",
    "Finally, letâ€™s define our RAG system, which is the ultimate goal of this session. Fortunately, we have already completed the complex steps above. To define the RAG system, we simply need to combine the previously defined components into a single class.  \n",
    "\n",
    "```\n",
    "class RAGWithKG:\n",
    "    def __init__(self):\n",
    "        self.kg_query_engine = KGQueryEngine()\n",
    "        self.reader = Reader()\n",
    "\n",
    "    def inference(self, query):\n",
    "        # 1. retrieve relevant kg results\n",
    "        kg_results, is_finance = self.kg_query_engine.query(query)\n",
    "\n",
    "        # 2. answer the question based on the retrieved chunks\n",
    "        answer = self.reader.generate_response(query, [kg_results])\n",
    "\n",
    "        return answer, kg_results\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikmQ42W1PRUt"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "class RAGWithKG:\n",
    "    def __init__(self):\n",
    "        self.kg_query_engine = KGQueryEngine()\n",
    "        self.reader = Reader()\n",
    "\n",
    "    def inference(self, query):\n",
    "        # 1. retrieve relevant kg results\n",
    "        kg_results, is_finance = self.kg_query_engine.query(query)\n",
    "\n",
    "        # 2. answer the question based on the retrieved chunks\n",
    "        answer = self.reader.generate_response(query, [kg_results])\n",
    "\n",
    "        return answer, kg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWz9-EVLkhYm"
   },
   "source": [
    "Letâ€™s now verify whether the implemented RAG system operates as intended. This system retrieves relevant information from the Knowledge Graph (KG) and utilizes it to generate a final answer.\n",
    "\n",
    "Additionally, we will check whether the system can correctly handle the following queries, which the RAG system from Task 1 previously failed to answer.\n",
    "\n",
    "<br/>  \n",
    "Question: **What is the ex-dividend date of microsoft in the 1st qtr of 2024**.   \n",
    "Answer: **The ex-dividend date of microsoft in the 1st qtr of 2024 is feb 14, 2024**\n",
    "<br/>\n",
    "\n",
    "<br/>  \n",
    "Question: **I'm looking for the p/e ratio of dks. would you happen to know what it is?**.   \n",
    "Answer: **13.75**\n",
    "<br/>\n",
    "\n",
    "<br/>  \n",
    "Question: **What's auph's earnings per share?**.   \n",
    "Answer: **0.4**\n",
    "<br/>\n",
    "\n",
    "```\n",
    "rag = RAGWithKG()\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "repeat = 0\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        if repeat not in [14, 53, 64]:\n",
    "            repeat += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer, kg_results = rag.inference(item['query'])\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        print(f\"kg results: {kg_results}\")\n",
    "\n",
    "        repeat += 1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-i6vlXeI3Mmp"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "rag = RAGWithKG()\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "repeat = 0\n",
    "\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "        if repeat not in [14, 53, 64]:\n",
    "            repeat += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer, kg_results = rag.inference(item['query'])\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        print(f\"kg results: {kg_results}\")\n",
    "\n",
    "        repeat += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPzcQyXz-ODV"
   },
   "source": [
    "## IV. Implementing an LLM + Web Search Results + Mock KG\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "def parse_htmls(search_results):\n",
    "    all_documents = []\n",
    "\n",
    "    # Process each HTML text from the search results to extract text content.\n",
    "    for html_text in search_results:\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_text[\"page_result\"], features=\"lxml\")\n",
    "        text = soup.get_text(\" \", strip=True)  # Use space as a separator, strip whitespaces\n",
    "        all_documents.append(text)\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "class LlamaIndexRetriever:\n",
    "  def __init__(self):\n",
    "      self.parser = SentenceSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "  def retrieve(self, query, search_results, topk):\n",
    "      documents = []\n",
    "\n",
    "      for document in parse_htmls(search_results):\n",
    "        if not document:\n",
    "            # If no text is extracted, add an empty string as a placeholder.\n",
    "            documents.append(Document(text=\"\"))\n",
    "        else:\n",
    "            documents.append(Document(text=document))\n",
    "\n",
    "      # Split documents into chunks & Create vector index\n",
    "      base_index = VectorStoreIndex.from_documents(documents = documents, transformations=[self.parser])\n",
    "\n",
    "      # Execute query\n",
    "      base_retriever = base_index.as_retriever(similarity_top_k=topk)\n",
    "\n",
    "      retrieved_nodes = base_retriever.retrieve(query)\n",
    "\n",
    "      retrieved_results = [retrieved_node.node.get_content().strip() for retrieved_node in retrieved_nodes]\n",
    "\n",
    "      return retrieved_results\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "class RAGWithSRKG:\n",
    "    def __init__(self):\n",
    "        self.retriever = LlamaIndexRetriever()\n",
    "        self.kg_query_engine = KGQueryEngine()\n",
    "        self.reader = Reader()\n",
    "\n",
    "    def inference(self, query, search_results, topk):\n",
    "        # 1. retrieve relevant chunks\n",
    "        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n",
    "\n",
    "        # 2. retrieve relevant kg results\n",
    "        kg_results, is_finance = self.kg_query_engine.query(query)\n",
    "\n",
    "        # combined_results = [kg_results]\n",
    "        # combined_results.extend(retrieved_results)\n",
    "        if is_finance:\n",
    "          combined_results = [kg_results]\n",
    "        else:\n",
    "          combined_results = retrieved_results\n",
    "\n",
    "        # 3. answer the question based on the retrieved chunks\n",
    "        answer = self.reader.generate_response(query, combined_results)\n",
    "\n",
    "        return answer, combined_results\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1YPL8FE_DHP"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "def parse_htmls(search_results):\n",
    "    all_documents = []\n",
    "\n",
    "    # Process each HTML text from the search results to extract text content.\n",
    "    for html_text in search_results:\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_text[\"page_result\"], features=\"lxml\")\n",
    "        text = soup.get_text(\" \", strip=True)  # Use space as a separator, strip whitespaces\n",
    "        all_documents.append(text)\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "class LlamaIndexRetriever:\n",
    "  def __init__(self):\n",
    "      self.parser = SentenceSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "  def retrieve(self, query, search_results, topk):\n",
    "      documents = []\n",
    "\n",
    "      for document in parse_htmls(search_results):\n",
    "        if not document:\n",
    "            # If no text is extracted, add an empty string as a placeholder.\n",
    "            documents.append(Document(text=\"\"))\n",
    "        else:\n",
    "            documents.append(Document(text=document))\n",
    "\n",
    "      # Split documents into chunks & Create vector index\n",
    "      base_index = VectorStoreIndex.from_documents(documents = documents, transformations=[self.parser])\n",
    "\n",
    "      # Execute query\n",
    "      base_retriever = base_index.as_retriever(similarity_top_k=topk)\n",
    "\n",
    "      retrieved_nodes = base_retriever.retrieve(query)\n",
    "\n",
    "      retrieved_results = [retrieved_node.node.get_content().strip() for retrieved_node in retrieved_nodes]\n",
    "\n",
    "      return retrieved_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SB9uyZXg_DPv"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "class RAGWithSRKG:\n",
    "    def __init__(self):\n",
    "        self.retriever = LlamaIndexRetriever()\n",
    "        self.kg_query_engine = KGQueryEngine()\n",
    "        self.reader = Reader()\n",
    "\n",
    "    def inference(self, query, search_results, topk):\n",
    "        # 1. retrieve relevant chunks\n",
    "        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n",
    "\n",
    "        # 2. retrieve relevant kg results\n",
    "        kg_results, is_finance = self.kg_query_engine.query(query)\n",
    "\n",
    "        # combined_results = [kg_results]\n",
    "        # combined_results.extend(retrieved_results)\n",
    "        if is_finance:\n",
    "          combined_results = [kg_results]\n",
    "        else:\n",
    "          combined_results = retrieved_results\n",
    "\n",
    "        # 3. answer the question based on the retrieved chunks\n",
    "        answer = self.reader.generate_response(query, combined_results)\n",
    "\n",
    "        return answer, combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsxJ1Vse_Ffh"
   },
   "source": [
    "Let us evaluate whether it can successfully answer all the following queries.\n",
    "\n",
    "<br/>  \n",
    "Question: **In 2004, which animated film was recognized with the best animated feature film oscar?**.   \n",
    "Answer: **Finding Nemo**\n",
    "<br/>\n",
    "\n",
    "<br/>  \n",
    "Question: **What is the ex-dividend date of microsoft in the 1st qtr of 2024**.   \n",
    "Answer: **The ex-dividend date of microsoft in the 1st qtr of 2024 is feb 14, 2024**\n",
    "<br/>\n",
    "\n",
    "<br/>  \n",
    "Question: **I'm looking for the p/e ratio of dks. would you happen to know what it is?**.   \n",
    "Answer: **13.75**\n",
    "<br/>\n",
    "\n",
    "<br/>  \n",
    "Question: **What's auph's earnings per share?**.   \n",
    "Answer: **0.4**\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "rag = RAGWithSRKG()\n",
    "topk = 5\n",
    "\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat not in [5, 14, 53, 64]:\n",
    "            repeat += 1\n",
    "            continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        print(\"retrieved results:\")\n",
    "        for rank, retrieved_result in enumerate(retrieved_results):\n",
    "            print(f\"{rank}: {retrieved_result}\")\n",
    "        print()\n",
    "        repeat += 1\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q79hzZ-C_jJk"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "dataset_path = \"/path/to/CRAG dataset/crag_task_1_dev_v4_release.jsonl.bz2\"\n",
    "\n",
    "rag = RAGWithSRKG()\n",
    "topk = 5\n",
    "\n",
    "repeat = 0\n",
    "with bz2.open(dataset_path, \"rt\") as file:\n",
    "    for line in file:\n",
    "        if repeat not in [5, 14, 53, 64]:\n",
    "            repeat += 1\n",
    "            continue\n",
    "\n",
    "        item = json.loads(line)\n",
    "        print(f\"query: {item['query']}\")\n",
    "        print()\n",
    "        answer, retrieved_results = rag.inference(item['query'], item['search_results'], topk)\n",
    "        print(f\"predicted answer: {answer}\")\n",
    "        print(f\"ground truth answer: {item['answer']}\")\n",
    "        print()\n",
    "        print(\"retrieved results:\")\n",
    "        for rank, retrieved_result in enumerate(retrieved_results):\n",
    "            print(f\"{rank}: {retrieved_result}\")\n",
    "        print()\n",
    "        repeat += 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/truera/trulens/blob/main/trulens_eval/examples/quickstart/quickstart.ipynb",
     "timestamp": 1711609044141
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
