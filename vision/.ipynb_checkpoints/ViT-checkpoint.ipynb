{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a40e95-e048-4181-a95e-5b3d8025991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Vision Transformer with CIFAR-10 — 패치 임베딩부터 분류까지**\n",
    "\n",
    "**Vision Transformer(ViT)** 실습을 목표로 합니다.  \n",
    "CNN과 달리 ViT는 이미지를 **패치(patch) 시퀀스**로 바꾼 뒤, NLP의 Transformer Encoder와 거의 같은 방식으로 처리합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 이미지를 **패치 토큰**으로 바꾸는 과정(= Patch Embedding)을 이해한다.\n",
    "- **[CLS] 토큰 + Positional Embedding** 이 왜 필요한지 설명할 수 있다.\n",
    "- Transformer Encoder의 핵심 구성(**Pre-LN / MHSA / FFN / Residual**)을 코드에서 찾아 읽을 수 있다.\n",
    "- 학습 후 **오분류(실패) 샘플**을 통해 모델의 한계를 분석한다.\n",
    "\n",
    "> 권장 흐름: (1) 데이터/전처리 → (2) ViT 구성 → (3) 학습/평가 → (4) 오분류 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4692b8bb-59d7-4e54-8184-7e91b9d00136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def ensure_package(pkg_name: str, import_name=None):\n",
    "    \"\"\"\n",
    "    패키지 설치 여부를 확인하고, 설치되어 있지 않으면 설치합니다.\n",
    "    \"\"\"\n",
    "    name = import_name or pkg_name\n",
    "    # 패키지가 설치되어 있는지 확인\n",
    "    if importlib.util.find_spec(name) is None:\n",
    "        print(f\"[install] {pkg_name} 라이브러리를 설치 중입니다... (import name: {name})\")\n",
    "        try:\n",
    "            # -q 옵션을 추가하여 설치 과정을 간결하게 유지할 수 있습니다.\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg_name])\n",
    "            print(f\"[success] {pkg_name} 설치 완료.\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"[error] {pkg_name} 설치 실패: {e}\")\n",
    "    else:\n",
    "        print(f\"[ok] {pkg_name} 이미 설치되어 있습니다.\")\n",
    "\n",
    "# 설치가 필요한 패키지 리스트 (패키지명, 임포트명)\n",
    "# 임포트명이 패키지명과 다른 경우 튜플로 지정합니다.\n",
    "packages = [\n",
    "    (\"einops\", \"einops\"),\n",
    "    (\"torchinfo\", \"torchinfo\")\n",
    "]\n",
    "\n",
    "# 루프를 돌며 확인 및 설치\n",
    "for pkg, imp in packages:\n",
    "    ensure_package(pkg, imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3687d1-2692-4b7d-a8f1-c9a62b3c59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F  # 함수형 API(F): activation/loss 등\n",
    "from torch import optim\n",
    "\n",
    "from einops import rearrange, repeat  # ViT에서 자주 쓰는 텐서 재배열(rearrange)·복제(repeat)\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "import torchvision  # 데이터/변환(torchvision) 사용 (CIFAR-10 포함)\n",
    "import time\n",
    "from torchinfo import summary\n",
    "\n",
    "print('torch:', torch.__version__)  # torch 버전 출력\n",
    "print('torchvision:', torchvision.__version__)  # torchvision 버전 출력\n",
    "print('cuda available:', torch.cuda.is_available())  # GPU 사용 가능 여부\n",
    "if torch.cuda.is_available():\n",
    "    print('gpu:', torch.cuda.get_device_name(0))  # GPU 이름 출력\n",
    "\n",
    "try:\n",
    "    get_ipython().system('nvidia-smi -L')  # GPU 목록 확인(가능한 경우)\n",
    "except Exception as e:\n",
    "    print('nvidia-smi not available:', e)  # nvidia-smi가 없는 환경이면 무시\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce05be0-6263-4d1f-ba49-bee1721bec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "## 1) 데이터 로딩 & 전처리\n",
    "\n",
    "- **목적:** CIFAR-10(컬러 32×32)를 로드하고, 모델 입력 텐서 형태를 확인합니다.\n",
    "- **관찰 포인트**\n",
    "  - 입력 텐서 shape: `B×C×H×W` (CIFAR-10는 `C=3`)\n",
    "  - 학습/테스트 로더 구성과 배치 단위 학습의 의미\n",
    "\"\"\"\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# CIFAR-10에서 널리 쓰는 normalize 값\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)  # 채널별 평균(R,G,B)\n",
    "CIFAR10_STD  = (0.2023, 0.1994, 0.2010)  # 채널별 표준편차(R,G,B)\n",
    "\n",
    "train_tfms = T.Compose([  # train transform(augmentation 포함)\n",
    "    T.RandomCrop(32, padding=4),      # 32x32를 padding 후 random crop\n",
    "    T.RandomHorizontalFlip(),         # 좌우 반전\n",
    "    T.ToTensor(),                     # PIL -> torch tensor (C,H,W)\n",
    "    T.Normalize(CIFAR10_MEAN, CIFAR10_STD),  # 정규화\n",
    "])\n",
    "\n",
    "test_tfms = T.Compose([  # test transform(augmentation 없음)\n",
    "    T.ToTensor(),                     # 텐서 변환\n",
    "    T.Normalize(CIFAR10_MEAN, CIFAR10_STD),  # 정규화\n",
    "])\n",
    "\n",
    "data_root = './data'  # 데이터 저장 경로\n",
    "train_set = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_tfms)  # train set\n",
    "test_set  = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_tfms)  # test set\n",
    "\n",
    "class_names = train_set.classes  # 클래스 이름 목록\n",
    "num_classes = len(class_names)   # 클래스 개수(10)\n",
    "print('classes:', class_names)   # 클래스 출력\n",
    "\n",
    "# 이미지 사이즈 출력\n",
    "x0, y0 = train_set[0]  # 한 샘플 로드(이미 transform 적용된 텐서)\n",
    "print('Sample image tensor shape (C,H,W):', tuple(x0.shape))  # (3,32,32)\n",
    "\n",
    "batch_size = 256  # 배치 크기(A10이면 보통 여유)\n",
    "num_workers = min(8, os.cpu_count() or 2)  # dataloader worker 수(환경에 맞게)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)  # train loader\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)  # test loader\n",
    "\n",
    "print('Total batch size (train: %d, test: %d)' % (len(train_loader), len(test_loader)))  # 배치 개수 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d5476-2ac4-4675-999a-3285f2150587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def denormalize_cifar10(img_chw: torch.Tensor):\n",
    "    \"\"\"정규화된 CIFAR-10 텐서(C,H,W)를 시각화용(0~1)으로 되돌립니다.\"\"\"\n",
    "    mean = torch.tensor(CIFAR10_MEAN, device=img_chw.device)[:, None, None]\n",
    "    std  = torch.tensor(CIFAR10_STD,  device=img_chw.device)[:, None, None]\n",
    "    x = img_chw * std + mean\n",
    "    return x.clamp(0, 1)\n",
    "\n",
    "def show_batch(data_loader, max_images: int = 16, nrow: int = 4, figsize=(10, 10)):\n",
    "    \"\"\"배치에서 일부 샘플을 보기 좋게 시각화합니다.\n",
    "\n",
    "    - 기본값은 16장(4×4)을 비교적 크게 보여주는 설정입니다.\n",
    "    - max_images/nrow/figsize를 바꾸면 '더 적게/더 많이', '더 크게/더 작게'를 조절할 수 있습니다.\n",
    "\n",
    "    - CIFAR-10은 (C,H,W)=(3,32,32) RGB이므로, 시각화할 때는 (H,W,C)로 바꿔야 합니다.\n",
    "    - make_grid 결과가 figure 안에서 '밀려 보이는' 경우가 있어, subplot 여백을 0으로 맞춰 꽉 차게 표시합니다.\n",
    "\n",
    "    Args:\n",
    "        data_loader: DataLoader\n",
    "        max_images: 한 번에 보여줄 이미지 개수(기본 16장)\n",
    "        nrow: grid 한 줄에 배치할 이미지 개수(기본 4개)\n",
    "        figsize: figure 크기\n",
    "    \"\"\"\n",
    "    batch = next(iter(data_loader))\n",
    "    images, labels = batch  # images: (B,C,H,W)\n",
    "    images = images[:max_images]\n",
    "\n",
    "    # make_grid는 (B,C,H,W) → grid(C,H,W)\n",
    "    grid = torchvision.utils.make_grid(images, nrow=nrow, padding=2)\n",
    "\n",
    "    # 시각화는 정규화 해제 후 HWC로 변환\n",
    "    grid = denormalize_cifar10(grid).detach().cpu().numpy().transpose((1, 2, 0))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(grid, interpolation=\"nearest\")\n",
    "    ax.set_title(f\"Batch Samples (CIFAR-10) | shown={len(images)}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # 여백 제거: grid가 4×4에 꽉 차도록\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    plt.show()\n",
    "\n",
    "# 미리보기: 크게(figure) + 적게(16장) 보기\n",
    "show_batch(train_loader, max_images=16, nrow=4, figsize=(10, 10))\n",
    "show_batch(train_loader, max_images=16, nrow=4, figsize=(10, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e3a7e-ebe0-44d4-abed-4f9c6d271542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "## 2) ViT 모델 구성 개요\n",
    "\n",
    "ViT는 크게 아래 3단계로 이해하면 읽기 쉬워집니다.\n",
    "\n",
    "1. **Patch Embedding**  \n",
    "   이미지(`H×W`)를 `P×P` 패치로 잘라 **토큰 시퀀스**로 바꾸고, 각 패치를 `dim` 차원으로 임베딩합니다.\n",
    "\n",
    "2. **Transformer Encoder (× depth)**  \n",
    "   토큰 시퀀스에 대해 반복적으로  \n",
    "   **(Pre-LN → Multi-Head Self-Attention → Residual) + (Pre-LN → FFN → Residual)** 를 수행합니다.\n",
    "\n",
    "3. **Classification Head**  \n",
    "   보통 `[CLS]` 토큰(또는 평균 풀링)을 사용해 최종 분류 로짓을 출력합니다.\n",
    "\n",
    "> 아래 코드에서 `dim / depth / heads / mlp_dim` 이 무엇을 의미하는지 주석과 함께 확인해 보세요.\n",
    "\"\"\"\n",
    "def pair(t):  # image_size/patch_size를 (H,W) 형태로 통일하는 유틸\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):  # Transformer의 Pre-LN 구조: LayerNorm 후 블록 실행\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)  # 토큰 임베딩 차원(dim) 기준 LayerNorm -> 패치별 정규화\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)  # 정규화된 토큰을 Attention/FFN에 전달\n",
    "\n",
    "class FeedForward(nn.Module):  # Transformer의 FFN(MLP) 블록 -> 패치 자체의 정보를 더 깊게 분석\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(  # FFN: Linear→GELU→Dropout→Linear→Dropout 구성\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):  # 멀티헤드 Self-Attention(MHSA) 구현\n",
    "    def __init__(self, dim, heads = 4, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads  # 전체 헤드 차원 = head 수 × head 차원\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads  # 멀티헤드 개수 저장\n",
    "        self.scale = dim_head ** -0.5  # Scaled dot-product를 위한 스케일(1/√d)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)  # attention score를 확률로 변환(softmax), 마지막 차원 : sequence length N\n",
    "        self.last_attn = None  # (학습/추론 시) 마지막 forward에서의 attention map 저장용(시각화/분석)  # attention score를 확률로 변환(softmax), 마지막 차원 : sequence length N\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)  # 입력 토큰→Q,K,V를 한 번에 선형 변환\n",
    "\n",
    "        self.to_out = nn.Sequential(  # 헤드들을 합친 뒤 출력 투영 + 드롭아웃\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads   # x = [batch Size, tokens, embedding dimension]\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)  # 선형변환 결과를 Q,K,V로 분할 -> (Q, K ,V)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)  # multi-head 연산을 위해 각 head 기준 n x d 로 분리\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale  # 각 토큰 간 유사도(Q·Kᵀ) 계산 후 스케일 적용\n",
    "\n",
    "        attn = self.attend(dots)  # 토큰 간 가중치(attention map) 생성\n",
    "        self.last_attn = attn.detach()  # 그래프에서 분리(detach)해서 저장 (시각화 목적)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)  # attention 가중합으로 새로운 토큰 표현(out) 계산\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')  # 헤드 차원을 다시 합쳐 (batch, tokens, dim)로 복원\n",
    "        return self.to_out(out)  # 최종 투영을 거쳐 Attention 블록 출력 반환\n",
    "\n",
    "class Transformer(nn.Module):  # Encoder block을 여러 층(depth) 쌓는 Transformer\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])  # 각 층(Attention+FFN)을 담을 컨테이너\n",
    "        for _ in range(depth):  # depth 만큼 Encoder block 반복 생성\n",
    "            self.layers.append(nn.ModuleList([  # 한 층 = (PreNorm+Attention) + (PreNorm+FFN)\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x # Residual 연결: Attention 결과를 입력에 더해 정보 보존\n",
    "            x = ff(x) + x # Residual 연결: FFN 결과를 입력에 더해 정보 보존\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326b22e-ef6c-4f61-b6f1-54b3f0795765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "## 2-1) ViTConfig + ViT(모델 본체) 구현\n",
    "\n",
    "- **목적:** ViT의 핵심 구성 요소(패치 임베딩, CLS 토큰, 위치 임베딩, Transformer Encoder 블록, 분류 헤드)를 **코드로 직접 확인**합니다.\n",
    "- **관찰 포인트**\n",
    "  - `image_size`, `patch_size` → 패치 개수(`num_patches`)와 토큰 시퀀스 길이가 어떻게 결정되는지\n",
    "  - `depth`(블록 개수), `heads`(멀티헤드 수), `dim`(토큰 임베딩 차원)이 **연산량/표현력**에 어떤 영향을 주는지\n",
    "  - **Pre-LN + Residual** 구조가 어디에 적용되는지(안정적인 학습을 위한 표준 패턴)\n",
    "\"\"\"\n",
    "from dataclasses import dataclass  # 설정값을 묶어 init 인자를 단순화하기 위한 도구\n",
    "from typing import Tuple, Union  # 파이썬 버전 호환을 위한 타입 힌트\n",
    "\n",
    "@dataclass\n",
    "class ViTConfig:\n",
    "    # 입력/토큰화 관련\n",
    "    image_size: 'Union[int, Tuple[int, int]]'          # 입력 이미지 크기 (H, W) 또는 정수(정수면 정사각형)\n",
    "    patch_size: 'Union[int, Tuple[int, int]]'          # 패치 크기 (Ph, Pw) 또는 정수\n",
    "    channels: int = 3                          # 입력 채널 수 (CIFAR-10=3, MNIST=1)\n",
    "\n",
    "    # 모델 본체(Encoder) 관련\n",
    "    dim: int = 64                              # 토큰 임베딩 차원(Transformer의 hidden size)\n",
    "    depth: int = 6                             # Encoder 블록 개수(= Transformer layer 수)\n",
    "    heads: int = 4                             # Multi-Head Self-Attention의 head 개수\n",
    "    dim_head: int = 64                         # 각 head의 Q/K/V 차원\n",
    "    mlp_dim: int = 128                         # FFN(MLP) 중간 차원\n",
    "\n",
    "    # 분류/출력 관련\n",
    "    num_classes: int = 10                      # 분류 클래스 수\n",
    "    pool: str = \"cls\"                          # \"cls\": CLS 토큰 사용, \"mean\": 토큰 평균 풀링\n",
    "    dropout: float = 0.0                       # Encoder 내부 dropout(Attention/FFN)\n",
    "    emb_dropout: float = 0.0                   # 패치+포지션 임베딩 단계 dropout\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Vision Transformer (ViT)\n",
    "    - 이미지를 패치로 쪼개 토큰 시퀀스를 만들고\n",
    "    - CLS 토큰 + 위치 임베딩을 더한 뒤\n",
    "    - Transformer Encoder로 전역(Self-Attention) 관계를 학습하여\n",
    "    - CLS(또는 mean pool) 표현으로 분류합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: ViTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) 입력 해상도/패치 크기를 (H,W) 튜플로 정규화\n",
    "        image_height, image_width = pair(cfg.image_size)\n",
    "        patch_height, patch_width = pair(cfg.patch_size)\n",
    "\n",
    "        # 2) 패치가 이미지에 딱 나누어 떨어져야 (h, w) 그리드가 정확히 형성됨\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        # 3) 패치 개수(=토큰 개수)와 한 패치의 펼친 차원 계산\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = cfg.channels * patch_height * patch_width\n",
    "\n",
    "        # 4) 풀링 방식 검증: CLS 토큰을 쓸지 mean pool을 쓸지 선택\n",
    "        assert cfg.pool in {\"cls\", \"mean\"}, \"pool must be 'cls' or 'mean'\"\n",
    "\n",
    "        # 5) 패치 토큰화: (B,C,H,W) -> (B, N, patch_dim) -> (B, N, dim)\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            # 패치 그리드로 자른 뒤, 각 패치를 1D 벡터로 펼쳐 토큰 시퀀스를 만듦\n",
    "            Rearrange(\n",
    "           \" b c (h p1)(w p2) -> b (p1 p2) (p1 p2 c)\",\n",
    "                p1=patch_height,\n",
    "                p2=patch_width,\n",
    "            ),\n",
    "            # 펼친 패치 벡터를 Transformer hidden size(dim)로 선형 투영\n",
    "            nn.Linear(patch_dim, cfg.dim),\n",
    "        )\n",
    "\n",
    "        # 6) CLS 토큰 + 위치 임베딩(학습 파라미터)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, cfg.dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, cfg.dim))\n",
    "        self.dropout = nn.Dropout(cfg.emb_dropout)\n",
    "\n",
    "        # 7) Transformer Encoder 스택(Attention + FFN + Residual/PreNorm)\n",
    "        self.transformer = Transformer(\n",
    "            dim=cfg.dim,\n",
    "            depth=cfg.depth,\n",
    "            heads=cfg.heads,\n",
    "            dim_head=cfg.dim_head,\n",
    "            mlp_dim=cfg.mlp_dim,\n",
    "            dropout=cfg.dropout,\n",
    "        )\n",
    "\n",
    "        self.pool = cfg.pool\n",
    "\n",
    "        # 8) 분류 헤드: (CLS/mean) 표현 -> LayerNorm -> Linear(logits)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(cfg.dim),\n",
    "            nn.Linear(cfg.dim, cfg.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # A) 패치 임베딩으로 토큰 시퀀스 생성: (B,C,H,W) -> (B,N,dim)\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # B) CLS 토큰을 배치만큼 복제해 시퀀스 맨 앞에 붙임: (B,1,dim)\n",
    "        cls_tokens = repeat(self.cls_token, \" 1 1 d -> b 1 d\", b=b)\n",
    "        x = torch.cat((cls_token,x), dim=1) # (B, N+1, dim)\n",
    "\n",
    "        # C) 위치 임베딩을 더해 토큰 순서(공간 위치) 정보를 주입\n",
    "        x = x + self.pos_embedding[:, : (n + 1)]   # (B, N+1, dim) + (1, N+1, dim) -> Broadcasting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # D) Encoder를 통과하며 전역 의존성(Self-Attention) 학습\n",
    "        x = self.transformer(x)  # (B, N+1, dim)\n",
    "\n",
    "        # E) 이미지 표현 벡터 선택: CLS 토큰(0번) 또는 mean pooling\n",
    "        x = x[:, 0] if self.pool == \"cls\" else x.mean(dim=1)   # (B, dim)\n",
    "\n",
    "        # F) 최종 logits 출력 (softmax는 CrossEntropyLoss 내부에서 처리)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058808aa-7a21-48a6-ac42-facd0f63e919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
