{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9168d8d-8d02-497b-9c6d-b9d3f1c03ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Vision Transformer with CIFAR-10 — 패치 임베딩부터 분류까지**\n",
    "\n",
    "CNN과 달리 ViT는 이미지를 **패치(patch) 시퀀스**로 바꾼 뒤, NLP의 Transformer Encoder와 거의 같은 방식으로 처리\n",
    "\n",
    "## 학습 목표\n",
    "- 이미지를 **패치 토큰**으로 바꾸는 과정(= Patch Embedding)을 이해.\n",
    "- **[CLS] 토큰 + Positional Embedding** 이 왜 필요한지 설명할 수 있다.\n",
    "- Transformer Encoder의 핵심 구성(**Pre-LN / MHSA / FFN / Residual**)을 코드에서 찾아 읽을 수 있다.\n",
    "- 학습 후 **오분류(실패) 샘플**을 통해 모델의 한계를 분석한다.\n",
    "\n",
    "> 권장 흐름: (1) 데이터/전처리 → (2) ViT 구성 → (3) 학습/평가 → (4) 오분류 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa90e8-ef5c-44cc-a28e-751555fa4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ViT 모델 구성\n",
    "    1. patch embedding - 이미지를 P*P 패치로\n",
    "    2. transformer encoder(x depth) \n",
    "        - (Pre-LN → Multi-Head Self-Attention → Residual) + (Pre-LN → FFN → Residual) 를 수행\n",
    "    3. classification head\n",
    "    아래 코드에서 dim / depth / heads / mlp_dim 무엇을 의미하는지 파악하기\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c2b70-0921-48c1-a60a-4c0436af1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    LLM Transformer랑 역시 거의 동일한데\n",
    "    norm을 표현하는 방식 등이 살짝 다름.\n",
    "    코드 이해 자체는 LLM이 더 쉽긴 한데 \n",
    "    같은 로직을 다르게 표현해봐 라고 나오기 좋은 문제일수도?\n",
    "\"\"\"\n",
    "class Transformer(nn.Module):  # Encoder block을 여러 층(depth) 쌓는 Transformer\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])  # 각 층(Attention+FFN)을 담을 컨테이너\n",
    "        for _ in range(depth):  # depth 만큼 Encoder block 반복 생성\n",
    "            self.layers.append(nn.ModuleList([  # 한 층 = (PreNorm+Attention) + (PreNorm+FFN)\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x  # Residual 연결: Attention 결과를 입력에 더해 정보 보존\n",
    "            x = ff(x) + x  # Residual 연결: FFN 결과를 입력에 더해 정보 보존\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7ebaa-318f-41df-a06f-77fb8695011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"Vision Transformer (ViT)\n",
    "    - 이미지를 패치로 쪼개 토큰 시퀀스를 만들고\n",
    "    - CLS 토큰 + 위치 임베딩을 더한 뒤\n",
    "    - Transformer Encoder로 전역(Self-Attention) 관계를 학습하여\n",
    "    - CLS(또는 mean pool) 표현으로 분류합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: ViTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) 입력 해상도/패치 크기를 (H,W) 튜플로 정규화\n",
    "        image_height, image_width = pair(cfg.image_size)\n",
    "        patch_height, patch_width = pair(cfg.patch_size)\n",
    "\n",
    "        # 2) 패치가 이미지에 딱 나누어 떨어져야 (h, w) 그리드가 정확히 형성됨\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        # 3) 패치 개수(=토큰 개수)와 한 패치의 펼친 차원 계산\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = cfg.channels * patch_height * patch_width\n",
    "\n",
    "        # 4) 풀링 방식 검증: CLS 토큰을 쓸지 mean pool을 쓸지 선택\n",
    "        assert cfg.pool in {\"cls\", \"mean\"}, \"pool must be 'cls' or 'mean'\"\n",
    "\n",
    "        # 5) 패치 토큰화: (B,C,H,W) -> (B, N, patch_dim) -> (B, N, dim)\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            # 패치 그리드로 자른 뒤, 각 패치를 1D 벡터로 펼쳐 토큰 시퀀스를 만듦\n",
    "            Rearrange(\n",
    "                \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n",
    "                p1=patch_height,\n",
    "                p2=patch_width,\n",
    "            ),\n",
    "            # 펼친 패치 벡터를 Transformer hidden size(dim)로 선형 투영\n",
    "            nn.Linear(patch_dim, cfg.dim),\n",
    "        )\n",
    "\n",
    "        # 6) CLS 토큰 + 위치 임베딩(학습 파라미터)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, cfg.dim))\n",
    "        # LLM GPT의 token embedding 대신 cls_token으로 분류하는 방식인듯?\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, cfg.dim))\n",
    "        self.dropout = nn.Dropout(cfg.emb_dropout)\n",
    "\n",
    "        # 7) Transformer Encoder 스택(Attention + FFN + Residual/PreNorm)\n",
    "        self.transformer = Transformer(\n",
    "            dim=cfg.dim,\n",
    "            depth=cfg.depth,\n",
    "            heads=cfg.heads,\n",
    "            dim_head=cfg.dim_head,\n",
    "            mlp_dim=cfg.mlp_dim,\n",
    "            dropout=cfg.dropout,\n",
    "        )\n",
    "\n",
    "        self.pool = cfg.pool\n",
    "\n",
    "        # 8) 분류 헤드: (CLS/mean) 표현 -> LayerNorm -> Linear(logits)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(cfg.dim),\n",
    "            nn.Linear(cfg.dim, cfg.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # A) 패치 임베딩으로 토큰 시퀀스 생성: (B,C,H,W) -> (B,N,dim)\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # B) CLS 토큰을 배치만큼 복제해 시퀀스 맨 앞에 붙임: (B,1,dim)\n",
    "        cls_tokens = repeat(self.cls_token, \"1 1 d -> b 1 d\", b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (B, N+1, dim)\n",
    "\n",
    "        # C) 위치 임베딩을 더해 토큰 순서(공간 위치) 정보를 주입\n",
    "        x = x + self.pos_embedding[:, : (n + 1)]   # (B, N+1, dim) + (1, N+1, dim) -> Broadcasting\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # D) Encoder를 통과하며 전역 의존성(Self-Attention) 학습\n",
    "        x = self.transformer(x)  # (B, N+1, dim)\n",
    "\n",
    "        # E) 이미지 표현 벡터 선택: CLS 토큰(0번) 또는 mean pooling\n",
    "        x = x[:, 0] if self.pool == \"cls\" else x.mean(dim=1)   # (B, dim)\n",
    "\n",
    "        # F) 최종 logits 출력 (softmax는 CrossEntropyLoss 내부에서 처리)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c9424e-1c1b-4bf0-b301-e2c8f7757fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
