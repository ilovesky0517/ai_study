{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ce04d4-e443-473b-91bd-a474605c8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" einsum 부분 숙지하고 있어야 할 듯?\n",
    "    LLM의 attention과 dropout의 위치만 다르지 나머지는 대부분 동일한 듯, 표현 방식의 차이에 대한걸 숙지해야 할 듯.\n",
    "    Attention을 다르게 표현해봐라 뭐 이런???\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f50628-9d0d-473a-aeb3-6b427bbf7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):  # Transformer의 Pre-LN 구조: LayerNorm 후 블록 실행\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)  # 토큰 임베딩 차원(dim) 기준 LayerNorm -> 패치별 정규화\n",
    "        self.fn = fn # 호출할 때 Attention이나 FF가 입력되서 단순히 nor하고 attn/FF 한다는 의미\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)  # 정규화된 토큰을 Attention/FFN에 전달\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55b0d2-0dd6-46b2-b32f-6c91e91c93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):  # Transformer의 FFN(MLP) 블록 -> 패치 자체의 정보를 더 깊게 분석\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(  # FFN: Linear→GELU→Dropout→Linear→Dropout 구성\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47465d-363a-4874-ab4e-428a0ea15163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):  # 멀티헤드 Self-Attention(MHSA) 구현\n",
    "    def __init__(self, dim, heads = 4, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads  # 전체 헤드 차원 = head 수 × head 차원\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads  # 멀티헤드 개수 저장\n",
    "        self.scale = dim_head ** -0.5  # Scaled dot-product를 위한 스케일(1/√d)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)  # attention score를 확률로 변환(softmax), 마지막 차원 : sequence length N\n",
    "        self.last_attn = None  # (학습/추론 시) 마지막 forward에서의 attention map 저장용(시각화/분석)  \n",
    "                            # attention score를 확률로 변환(softmax), 마지막 차원 : sequence length N\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)  # 입력 토큰→Q,K,V를 한 번에 선형 변환\n",
    "\n",
    "        self.to_out = nn.Sequential(  # 헤드들을 합친 뒤 출력 투영 + 드롭아웃\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads   # x = [batch Size, tokens, embedding dimension]\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)  # 선형변환 결과를 Q,K,V로 분할 -> (Q, K ,V)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)  # multi-head 연산을 위해 각 head 기준 n x d 로 분리\n",
    "        # 아래 두 코드를 합친게 위 한줄 인 듯. h만 주면 나머지 d는 계산을 해주는...\n",
    "        #  keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        #  keys = keys.transpose(1, 2)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale  # 각 토큰 간 유사도(Q·Kᵀ) 계산 후 스케일 적용\n",
    "        \n",
    "        attn = self.attend(dots)  # 토큰 간 가중치(attention map) 생성\n",
    "        self.last_attn = attn.detach()  # 그래프에서 분리(detach)해서 저장 (시각화 목적)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)  # attention 가중합으로 새로운 토큰 표현(out) 계산\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')  # 헤드 차원을 다시 합쳐 (batch, tokens, dim)로 복원\n",
    "        return self.to_out(out)  # 최종 투영을 거쳐 Attention 블록 출력 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9642a2-8966-4fad-96d9-c15ee402f963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
