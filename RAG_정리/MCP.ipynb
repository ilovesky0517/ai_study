{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e9e7aa-18d9-4021-901f-16241bfbcc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_templateì— 3ê°€ì§€ ì •ë³´ë¥¼ ë„£ì–´ì„œ evaluationìœ¼ë¡œ ì“´ë‹¤ëŠ”ê²Œ íŠ¹ì´ì . ì•Œì•„ëŠ” ë‘ê¸°\n",
    "def CRAG_evaluation(question, ground_truth, prediction):\n",
    "  context_template = f\"\"\"Question: {question}\n",
    "  List of Ground Truth answers: {ground_truth}\n",
    "  Model Prediction: {prediction}\n",
    "  \"\"\"\n",
    "\n",
    "  evaluation_result = generate_answer(user_prompt=context_template, system_prompt=INSTRUCTIONS)\n",
    "\n",
    "  eval_res = parse_response(evaluation_result)\n",
    "\n",
    "  return eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5c9ac-cc9d-4b2f-b863-56c6167bebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë§¨ ë°‘ì— CRAG_score ì‹ ì •ë„ëŠ” ì™¸ì›Œë„ ? \n",
    "# CRAG_score = n_correct_exact + 0.5*n_correct - n_hallucinate\n",
    "\n",
    "for data in tqdm(dataset):\n",
    "  if data['interaction_id'] not in finance_test_dataset_ids:\n",
    "    continue\n",
    "\n",
    "  question = data['query']\n",
    "  ground_truth_lowercase = str(data['answer']).strip().lower()\n",
    "  web_search_results = data['search_results']\n",
    "\n",
    "  prediction_lowercase = rag.inference(question, web_search_results, 5)['answer'].lower()\n",
    "\n",
    "  if prediction_lowercase == ground_truth_lowercase:\n",
    "      n_correct_exact += 1\n",
    "      continue\n",
    "  elif \"i don't know\" in prediction_lowercase:\n",
    "      n_miss += 1\n",
    "      continue\n",
    "\n",
    "  acceptable = CRAG_evaluation(question, ground_truth_lowercase, prediction_lowercase)\n",
    "\n",
    "  if acceptable == 1:\n",
    "    n_correct += 1\n",
    "\n",
    "n_hallucinate = (len(finance_test_dataset_ids) - n_correct_exact - n_correct - n_miss)\n",
    "\n",
    "CRAG_score = n_correct_exact + 0.5*n_correct - n_hallucinate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd41f47-9c9c-42f6-b84d-58c336102b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCPë‘ llamaì¤‘ì— í•˜ë‚˜ ë‚˜ì˜¨ë‹¤ê³  í•˜ë‹ˆ ê¼­ ì™¸ìš°ê¸°\n",
    "external_mcp_server = \"http://x.x.x.x:port/sse\" #change to correct uri\n",
    "\n",
    "mcp_client = BasicMCPClient(external_mcp_server)\n",
    "mcp_tool = McpToolSpec(client=mcp_client)\n",
    "\n",
    "# awaitê°€ ë“¤ì–´ê°€ëŠ” ê²ƒ ì˜ ì™¸ìš°ê¸°\n",
    "tools = await mcp_tool.to_tool_list_async()\n",
    "print(\"\\n=== Available Tools ===\\n\")\n",
    "for tool in tools:\n",
    "    print(f\"ğŸ”§ Name       : {tool.metadata.name}\")\n",
    "    print(f\"   Description: {tool.metadata.description}\\n\")\n",
    "\n",
    "resources = await mcp_tool.fetch_resources()\n",
    "print(\"\\n=== Available Resources ===\\n\")\n",
    "for resource in resources:\n",
    "    print(f\"ğŸ“¦ URI        : {resource.uri}\")\n",
    "    print(f\"   Name       : {resource.name}\")\n",
    "    print(f\"   Description: {resource.description}\")\n",
    "    print(f\"   MIME Type  : {resource.mimeType}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7a795-7fd0-4b23-b941-e9b43f66f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ì ì¸ ë¬¸ë²•ì´ë‘ async ë‘ awaitê°€ ì–´ë””ì— ë¶™ëŠ”ì§€ ì•Œì•„ë‘¬ì•¼ í• ê±° ê°™ì€?\n",
    "# awaitëŠ” ë¹„ë™ê¸°(async) í•¨ìˆ˜ê°€ ëë‚ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” ê²ƒ.\n",
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "You are an AI assistant for Tool Calling.\n",
    "\n",
    "Before you help a user, you need to work with tools to interact with Our Knowledge Graph\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "```python\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "class KGQueryEngineWithMCP:\n",
    "    def __init__(self, mcp_tool_spec: McpToolSpec, model: str, llm = None):\n",
    "        self.llm = llm or OpenAI(model=model, temperature=0)\n",
    "        self.mcp_tool_spec = mcp_tool_spec\n",
    "        self.agent: Optional[FunctionAgent] = None\n",
    "        self.agent_context: Optional[Context] = None\n",
    "\n",
    "    async def init_agent(self):\n",
    "        tools = await self.mcp_tool_spec.to_tool_list_async()\n",
    "            # self.mcp_tool_specëŠ” MCPì—ì„œ ì •ì˜ëœ íˆ´ ìŠ¤í™(tool specification) ê°ì²´\n",
    "            # to_tool_list_async() ë©”ì„œë“œëŠ” ì´ ìŠ¤í™ì„ ì‹¤ì œ íˆ´ ë¦¬ìŠ¤íŠ¸(list of tools)ë¡œ ë³€í™˜í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜\n",
    "            # await í‚¤ì›Œë“œëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ì˜ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦°ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "            # ì¦‰, to_tool_list_async()ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° í›„, ë°˜í™˜ëœ ê²°ê³¼(íˆ´ ë¦¬ìŠ¤íŠ¸)ë¥¼ tools ë³€ìˆ˜ì— ì €ì¥\n",
    "        self.agent = FunctionAgent(\n",
    "            name=\"Agent\",\n",
    "            description=\"An agent that can work with Our Knowledge Graph api.\",\n",
    "            tools=tools,\n",
    "            llm=self.llm,\n",
    "            system_prompt=SYSTEM_PROMPT,\n",
    "        )\n",
    "        self.agent_context = Context(self.agent)\n",
    "\n",
    "    async def query(self, question: str, verbose: bool = False) -> str:\n",
    "        if self.agent is None or self.agent_context is None:\n",
    "            raise RuntimeError(\"Agent not initialized. Call `await init_agent()` first.\")\n",
    "\n",
    "        handler = self.agent.run(question, ctx=self.agent_context)\n",
    "        async for event in handler.stream_events():\n",
    "            if verbose and type(event) == ToolCall:\n",
    "                print(f\"Calling tool {event.tool_name} with kwargs {event.tool_kwargs}\")\n",
    "            elif verbose and type(event) == ToolCallResult:\n",
    "                print(f\"Tool {event.tool_name} returned {event.tool_output}\")\n",
    "\n",
    "        response = await handler\n",
    "        return str(response)\n",
    "\n",
    "kg_engine = KGQueryEngineWithMCP(mcp_tool, model='gpt-3.5-turbo')\n",
    "await kg_engine.init_agent() # ìœ„ì— asyncë¡œ ì„ ì–¸í•´ë†“ì€ init_agent í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ë•Œ await ë¶™ì´ëƒëŠ” ë¬¸ì œê°€ ë‚˜ì˜¬ìˆ˜ ???\n",
    "response = await kg_engine.query(\n",
    "    question,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb414c-9eb4-401e-8de1-8ad5f27f2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RAGWithKGì™€ ë‹¤ë¥¸ì \n",
    "    1) kg_query_engine ëŒ€ì‹  mcp_applicationìœ¼ë¡œ ì„ ì–¸\n",
    "    2) retrieveì™€ inferenceê°€ asyncë¡œ ì„ ì–¸ ë¨ <- retrieveì˜ init_agent()ì™€ queryë¥¼ awaitë¡œ ë°›ì•„ì™€ì•¼ í•´ì„œ í•¨ìˆ˜ ìì²´ëŠ” async \n",
    "    3) ì‹¤ì œ inferenceë¥¼ í˜¸ì¶œí• ë•Œë„ awaitë¡œ í˜¸ì¶œí•´ì•¼ í•¨.\n",
    "\"\"\"\n",
    "class RAGwithMCP:\n",
    "    def __init__(self, server=None):\n",
    "        self.mcp_application = KGQueryEngineWithMCP(server=server)\n",
    "        self.reader = Reader()\n",
    "\n",
    "    async def retrieve(self, query: str, query_time: str, search_results: list, topk: int):\n",
    "        await self.mcp_application.init_agent()\n",
    "\n",
    "        full_query = f\"\"\"Query: {query} \n",
    "                         Query time: {query_time}\"\"\"\n",
    "\n",
    "        mcp_result = await self.mcp_application.query(full_query, verbose=False)\n",
    "        return mcp_result\n",
    "\n",
    "    def generate_response(self, query: str, query_time: str, retrieved_results: str):\n",
    "        answer = self.reader.generate_response(query, query_time, retrieved_results)\n",
    "        return answer\n",
    "\n",
    "    async def inference(self, query: str, search_results: list, query_time: str, topk: int):\n",
    "        retrieved_results = await self.retrieve(query, query_time, search_results, topk)\n",
    "        answer = self.generate_response(query, query_time, retrieved_results)\n",
    "        return {\n",
    "            \"retrieved_results\": retrieved_results,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "\n",
    "rag_mcp = RAGwithMCP(server=external_mcp_server)\n",
    "result = await rag_mcp.inference(\n",
    "    query=question,\n",
    "    search_results=[],\n",
    "    query_time=query_time,\n",
    "    topk=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5872b12-137e-4bbc-ae0c-351f40fc197c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb6199-bdee-422a-a7cf-cb1a53f253bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
