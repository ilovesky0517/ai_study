{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07c50b-76af-4637-8418-8deb69a54c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "# document 만드는게 나오진 않겠지?\n",
    "parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "index = VectorStoreIndex.from_documents(documents=documents, transformations=[text_splitter])\n",
    "# chunk를 만들때 어떻게 변화가 생기는지 체크하면서 외우기\n",
    "\n",
    "node_id = index.index_struct.nodes_dict\n",
    "# 키(key)는 노드 ID, 값(value)은 해당 노드 객체\n",
    "\n",
    "for key, value in node_id.items():\n",
    "    print(key, value)\n",
    "    node_example_id = value\n",
    "    break\n",
    "    # 이것도 코드 자체는 첫번째것만 찍어내는 예제 케이스\n",
    "    \n",
    "index.vector_store.data.embedding_dict[node_example_id]\n",
    "# 특정 노드의 임베딩 벡터 표현을 조회하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01b807-8651-4293-a915-1028d5300491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 이런식으로 할수도 있구나 정도로 파악해두면 될 듯.\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import tiktoken\n",
    "\n",
    "parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "# you can change chunk_size, chunk_overlap\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "encoded_text = tokenizer.encode(nodes[0].text)\n",
    "\n",
    "decoded_text = tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd9023d-d3bc-46a3-929e-86b969ad3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "with open(path_to_txt, 'r', encoding='utf-8') as file:\n",
    "    for line in file: # 텍스트 파일을 한 줄씩 읽으면서\n",
    "        if 'paul graham' in line.lower(): # 특정 키워드를 찾아서\n",
    "            formatted_text = wrapper.fill(line.strip()) # 그 줄을 80글자 단위로 줄바꿈 처리\n",
    "            # formatted_text = textwrap.fill(line.strip(), width=80) 요런식으로 할 수도\n",
    "            print(formatted_text.split('.')[-1]) # 마지막 문장만 출력\n",
    "\n",
    "# 나오진 않을 듯? 그래도 읽어는 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7ab2c-b8c2-4284-a9e1-48d3c91cd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    index.as_retriever() : 검색만 수행, 결과를 직접 활용하거나 다른 모델에 전달할 때\n",
    "    index.as_query_engine() : 검색 + 답변 생성 / 질문에 대한 완성된 답변을 얻고 싶을 때\n",
    "\"\"\"\n",
    "retriever = index.as_retriever()\n",
    "ret_passages = retriever.retrieve(\"Who is the author?\")\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "context_str = query_engine.query(\"Who is the author?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c54aa1-8795-4993-8807-967f55727d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reader.load_data(city_names, auto_suggest=False)\n",
    "index = VectorStoreIndex.from_documents(documents) # VectorStore 만드는 기본 Flow\n",
    "\n",
    "docu = Document(text=data_text, id_=\"new_doc_id\")\n",
    "# 새 문서를 생성\n",
    "\n",
    "\"\"\" 입력 \"\"\"\n",
    "index.insert(docu)\n",
    "# 기존 인덱스(index)에 새 문서를 삽입.문서가 임베딩 벡터로 변환되어 벡터 저장소에 추가\n",
    "\n",
    "docu.set_content(value=\"Natural diamonds were (and are) formed (thousands of million years ago) in the upper mantle of Earth in metallic melts at temperatures of 2,000–6,000 °C and at pressures of 8–9 GPa.\")\n",
    "# 기존 docu 객체의 내용을 새로운 텍스트로 교체하는 코드\n",
    "# vector_store에는 단순히 벡터만 있는 게 아니라, \n",
    "# docu라는 문서 객체와 그 내용이 임베딩과 함께 연결된 형태로 저장 된단다.\n",
    "\n",
    "\"\"\" 갱신 \"\"\"\n",
    "output = index.update_ref_doc( # 인덱스에 참조된 문서를 갱신하는 메서드\n",
    "    docu,\n",
    "    update_kwargs={\"delete_kwargs\": {\"delete_from_docstore\": True}},\n",
    "    # 기존 문서를 docstore에서 삭제한 뒤 새로운 내용으로 교체하라는 의미.\n",
    ")\n",
    "\n",
    "output = index.refresh_ref_docs([docu])\n",
    "# 전달된 문서 리스트를 기반으로 내용을 다시 임베딩하고 vector store에 반영\n",
    "# 기존 문서 ID는 유지되지만, 텍스트와 임베딩은 새 값으로 교체\n",
    "# 옵션을 따로 주지 않아도 기본적으로 문서 내용을 최신 상태로 갱신합니다.\n",
    "\n",
    "print(index.ref_doc_info.keys())\n",
    "# 인덱스에 등록된 참조 문서(ref docs)들의 ID 목록이 출력됨.\n",
    "\n",
    "\"\"\" 삭제 \"\"\"\n",
    "id = docu.doc_id\n",
    "\n",
    "index.delete_ref_doc(id, delete_from_docstore=True)\n",
    "#벡터 저장소(Vector Store)에서 임베딩 + 원본 객체까지 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac0d3a-cbd3-4b20-b13a-77efba8697e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Synthesizer를 활용하는 예제 \n",
    "    response_mode를 통해 답변 스타일 지정 가능\n",
    "        \"compact\" : 응답을 짧고 간결하게 요약\n",
    "        \"tree_summarize\" : 여러 문서 조각을 트리 구조로 요약하여 응답을 생성.\n",
    "                            긴 문서나 복잡한 질문에 적합.\n",
    "        \"refine\" : 초기 응답을 생성한 뒤, 추가 문서 조각을 순차적으로 반영하여 점진적으로 개선.\n",
    "                    정밀한 답변이 필요할 때 사용.\n",
    "        \"simple\" : 가장 기본적인 모드, 검색된 내용을 단순히 합성.\n",
    "\"\"\"\n",
    "\n",
    "class StandardQueryEngine(CustomQueryEngine):\n",
    "    retriever: BaseRetriever\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "        response_obj = self.response_synthesizer.synthesize(query_str, nodes)\n",
    "        return response_obj\n",
    "\n",
    "retriever = index.as_retriever()\n",
    "synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "query_engine = StandardQueryEngine(\n",
    "    retriever=retriever, response_synthesizer=synthesizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58c01e-cce5-4c0d-ac80-fa82b132136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Synthesizer 대신 llm을 직접 부르는 코드 \"\"\"\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# 내가 직접 포맷을 정의해서 프롬프트를 보낼 수 있다는 장점\n",
    "simple_qa_prompt = PromptTemplate(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "class OurCustomQueryEngine(CustomQueryEngine):\n",
    "\n",
    "    retriever: BaseRetriever\n",
    "    response_synthesizer: BaseSynthesizer # 쓰이지 않음. 불필요한걸 왜 남겼을까?\n",
    "    llm: OpenAI\n",
    "    qa_prompt: PromptTemplate = simple_qa_prompt\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "\n",
    "        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",
    "        response = self.llm.complete(\n",
    "            self.qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        )\n",
    "\n",
    "        return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7db3a5-f3b2-4b91-b5d2-545e04a9fc12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
