{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0c3fc6-1b6f-440c-9f38-83a17ebda6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRAG 구성이해및활용 : RAG의구성요소및주요흐름에대해이해하고,그활용방법을알고있나\\n관련Library 활용 : llama-index, mcp등의RAG 관련library들을적절하게사용할수있나\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "RAG 구성이해및활용 : RAG의구성요소및주요흐름에대해이해하고,그활용방법을알고있나\n",
    "관련Library 활용 : llama-index, mcp등의RAG 관련library들을적절하게사용할수있나\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e7204-6222-4d2f-a014-8a8dfa123575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_htmls(search_results):\n",
    "    all_documents = []\n",
    "\n",
    "    # Process each HTML text from the search results to extract text content.\n",
    "    for html_text in search_results:\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_text[\"page_result\"], features=\"lxml\") \n",
    "        # html_text[\"page_result\"]는 HTML 문자열을 담고 있는 변수\n",
    "        # soup는 HTML 구조를 트리 형태로 변환한 파싱 결과\n",
    "        text = soup.get_text(\" \", strip=True)  # Use space as a separator, strip whitespaces\n",
    "        all_documents.append(text)\n",
    "\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b8d02-7309-482d-87d5-5efbbe146f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks(all_documents):\n",
    "    # Initialize a list to hold all extracted sentences from the search results.\n",
    "    all_chunks = []\n",
    "\n",
    "    for document in all_documents:\n",
    "\n",
    "        if not document:\n",
    "            # If no document is extracted, add an empty string as a placeholder.\n",
    "            all_chunks.append(\"\")\n",
    "        else:\n",
    "\n",
    "            # Extract offsets of sentences from the document\n",
    "            _, offsets = text_to_sentences_and_offsets(document)\n",
    "\n",
    "            # Initialize a list to store sentences\n",
    "            chunks = []\n",
    "\n",
    "            # Iterate through the list of offsets and extract sentences\n",
    "            for start, end in offsets:\n",
    "                # Extract the sentence and limit its length\n",
    "                chunk = document[start:end][:MAX_CONTEXT_SENTENCE_LENGTH]\n",
    "                # 앞에건 문자열 슬라이싱\n",
    "                # 뒤에 붙인 [...]를 통해 잘라낸 문자열의 길이를 최대 MAX_CONTEXT_SENTENCE_LENGTH까지만 제한\n",
    "                all_chunks.append(chunk)\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f626813-bd41-4374-a029-707e228fc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRetriever:\n",
    "    def __init__(self,):\n",
    "        self.client = openai.OpenAI(api_key = os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    def embed_text(self, texts):\n",
    "        \"\"\"Generate embeddings using OpenAI's embedding model.\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=texts\n",
    "        )\n",
    "\n",
    "        # Extract embeddings correctly from the response object\n",
    "        embeddings = [np.array(item.embedding) for item in response.data]  # Adjust based on actual attributes\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def retrieve(self, query, search_results, topk):\n",
    "        # Get documents\n",
    "        all_documents = parse_htmls(search_results)\n",
    "\n",
    "        # Get chunks\n",
    "        all_chunks = extract_chunks(all_documents)\n",
    "\n",
    "        # Generate embeddings for all chunks and the query.\n",
    "        all_embeddings = self.embed_text(all_chunks)\n",
    "        query_embedding = self.embed_text(query)[0]  # Single query embedding\n",
    "\n",
    "        # Calculate cosine similarity between query and sentence embeddings, and select the top sentences.\n",
    "        cosine_scores = np.dot(all_embeddings, query_embedding) / (\n",
    "            np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "        )\n",
    "        top_k_indices = (-cosine_scores).argsort()[:topk]\n",
    "        # argsort는 기본적으로 오름차순 정렬. 따라서 앞에 cosine_scores에 음수를 취해준다. \n",
    "        top_k_chunks = np.array(all_chunks)[top_k_indices]\n",
    "\n",
    "        return top_k_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b9a78-db6b-462f-bc87-73a73c214562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model = \"text-embedding-3-small\")\n",
    "\n",
    "class LlamaIndexRetriever:\n",
    "    def __init__.(self):\n",
    "        self.parser = SentenceSplitter(chunk_size=512, chunk_ovelap=0)\n",
    "    def retrieve(self, query, search_results, topk):\n",
    "        documents = []\n",
    "\n",
    "        for document in parse_htmls(search_results):\n",
    "            if not document:\n",
    "                documents.append(Document(text=\"\"))\n",
    "            else:\n",
    "                documents.append(Document(text=document))\n",
    "        base_index = VectorStoreIndex.from_documents(document=documents, transformation=[self.parser])\n",
    "\n",
    "        base_retriever = base_index.as_retriever(similarity_topk=topk)\n",
    "\n",
    "        retrieved_nodes = base_retriever.retrieve(query)\n",
    "\n",
    "        retrieved_results = [retrieved_node.node.get_count().strip() for retrieved_node in retrieved_nodes]\n",
    "        #  앞뒤 공백을 제거한 깨끗한 문자열 리스트 형태로 얻는걸 주의해야 할 듯 앞 뒤 [] \n",
    "        return retrieved_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6fc3c-05fc-4d51-b12f-900876e15064",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 시험에 나오지는 않을거 같음. 별 내용이 있는건 아님....\n",
    "def prompt_generator(query, top_k_chunks, system_prompt):\n",
    "    user_message = \"\"\n",
    "    references = \"\"\n",
    "\n",
    "    if len(top_k_chunks) > 0:\n",
    "        references += \"# References \\n\"\n",
    "        # Format the top sentences as references in the model's prompt template.\n",
    "        for chunk_id, chunk in enumerate(top_k_chunks):\n",
    "            references += f\"- {chunk.strip()}\\n\"\n",
    "\n",
    "    references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n",
    "    # Limit the length of references to fit the model's input size.\n",
    "\n",
    "    user_message += f\"{references}\\n------\\n\\n\"\n",
    "    user_message += f\"Using only the references listed above, answer the following question: \\n\"\n",
    "    user_message += f\"Question: {query}\\n\"\n",
    "\n",
    "    llm_input = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": user_message},\n",
    "    ]\n",
    "\n",
    "    return llm_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df9009-c9ac-473d-abad-da70db7af933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "class Reader:\n",
    "  def __init__(self):\n",
    "\n",
    "    self.system_prompt = \"\"\"\n",
    "        잘 검색해줘라 뭐 이런 내용을 블라블라 써 놓음. 자체로 문제는 X\n",
    "    \"\"\"\n",
    "\n",
    "  def generate_response(self, query: str, top_k_chunks: list) -> str:\n",
    "      \"\"\"\n",
    "       prompt generator를 통해 llm_input을 만들고, oai_client에 어떤식으로 전달하는지 정도 기억은 해두는게\n",
    "      \"\"\"\n",
    "      llm_input = self.prompt_generator(query, top_k_chunks)\n",
    "      completion = oai_client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      temperature=0,\n",
    "      messages= llm_input\n",
    "      ).choices[0].message.content # completion 뒤에 채우라고 나오면 여기가 제일 중요하지 않을까\n",
    "                                  # [0]이 첫번째로 선택된 답변이라는걸 기억해두기      \n",
    "      return completion\n",
    "\n",
    "  def prompt_generator(self, query, top_k_chunks):\n",
    "      user_message = \"\"\n",
    "      references = \"\"\n",
    "\n",
    "      if len(top_k_chunks) > 0:\n",
    "          references += \"# References \\n\"\n",
    "          # Format the top sentences as references in the model's prompt template.\n",
    "          for chunk_id, chunk in enumerate(top_k_chunks):\n",
    "              references += f\"- {chunk.strip()}\\n\"\n",
    "      \n",
    "      references = references[:MAX_CONTEXT_REFERENCES_LENGTH]\n",
    "      # Limit the length of references to fit the model's input size.\n",
    "\n",
    "      user_message += f\"{references}\\n------\\n\\n\"\n",
    "      user_message\n",
    "      user_message += f\"Using only the references listed above, answer the following question: \\n\"\n",
    "      user_message += f\"Question: {query}\\n\"\n",
    "\n",
    "      llm_input = [\n",
    "        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "      ]\n",
    "\n",
    "      return llm_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262a533-d2a8-4a42-bb8f-8eb1cec7a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    아래 3가지 RAG / RAGWithKG / RAGWithSRKG \n",
    "    3가지 방법에서 뭐가 다른지 비교하기 파악은 해두기\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ca695-2987-48fc-85a9-50704c2d7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    def __init__(self):\n",
    "        self.retriever = LlamaIndexRetriever()\n",
    "        self.reader = Reader()\n",
    "\n",
    "    def inference(self, query, search_results, topk):\n",
    "        # 1. 관련된 chunk를 검색하고, retrieve할 때는 topk까\n",
    "        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n",
    "\n",
    "        # 2. 답변을 생성하는 순서 기억은 해둬야 할 듯.\n",
    "        answer = self.reader.generate_response(query, retrieved_results)\n",
    "\n",
    "        return answer, retrieved_results\n",
    "\n",
    "rag = RAG()\n",
    "answer = rag.inference(item['query'], item['search_results'], topk)[0]\n",
    "# 이런식으로 inference 파라미터로 item['query']처럼 넣어주는 것과 맨 뒤에 [0]이 붙어야 하는거 잊지 않기 (하나만 취할 때)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1185d-ebd5-48a6-be5b-9b14c41fdabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWithKG:\n",
    "    def __init__(self):\n",
    "        self.kg_query_engine = KGQueryEngine() # RAG의 Llama를 KGQuery로 바꾼 차이점만\n",
    "        self.reader = Reader()\n",
    "\n",
    "    def inference(self, query):\n",
    "        # 1. retrieve relevant kg results\n",
    "        kg_results, is_finance = self.kg_query_engine.query(query)\n",
    "\n",
    "        # 2. answer the question based on the retrieved chunks\n",
    "        answer = self.reader.generate_response(query, [kg_results]) # 여기 kg_results는 [] 씌워서\n",
    "\n",
    "        return answer, kg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5457f55-de6f-432f-9c1f-547911fcfbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWithSRKG:\n",
    "    def __init__(self): # 얘는 Llama와 KGQuery다 사용\n",
    "        self.retriever = LlamaIndexRetriever()\n",
    "        self.kg_query_engine = KGQueryEngine(server=server) # server=server는 옵션인듯, \n",
    "        self.reader = Reader()\n",
    "\n",
    "    def inference(self, query, search_results, topk):\n",
    "        #inference를 하나로 통일할수도 있고, \n",
    "        # inference에서 retrieve, generate_response를 따로 호출하게 할 수도 있음. 내용은 동일\n",
    "\n",
    "        # 1. retrieve relevant chunks\n",
    "        retrieved_results = self.retriever.retrieve(query, search_results, topk)\n",
    "\n",
    "        # 2. retrieve relevant kg results\n",
    "        kg_results, is_finance = self.kg_query_engine.query(query)\n",
    "\n",
    "        # combined_results = [kg_results]\n",
    "        # combined_results.extend(retrieved_results)\n",
    "        # is_finance에 따라 [kg_results]를 쓸건지, retrieved_results를 쓸건지 선택하는 차이점만\n",
    "        if is_finance:\n",
    "          combined_results = [kg_results]\n",
    "        else:\n",
    "          combined_results = retrieved_results\n",
    "        ### RAGwithoutSR 에서는 위의 is_finance 관련 코드만 삭제 ###\n",
    "\n",
    "        # 3. answer the question based on the retrieved chunks\n",
    "        answer = self.reader.generate_response(query, combined_results)\n",
    "\n",
    "        return answer, combined_results # 그래서 2번째 return값도 combined~를"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6169f08a-77f6-4d60-9dd2-7ce366e448e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에서는 굳이 문제나올 부분이 있을까??? \n",
    "# 그런데 class KGQueryEngineWithMCP:를 이해하려면 같이 비교해보긴 해야할 듯.\n",
    "class KGQueryEngine:\n",
    "    def query(self, query):\n",
    "        generated_query, is_finance = self.generate_query(query)\n",
    "\n",
    "        if is_finance:\n",
    "            kg_results = self.get_finance_kg_results(generated_query)\n",
    "        else:\n",
    "            kg_results = \"\"\n",
    "\n",
    "        return kg_results\n",
    "\n",
    "    def generate_query(self, query):\n",
    "        llm_input = prompt_generator(query)\n",
    "        completion = oai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        messages=\n",
    "        llm_input\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            completion = json.loads(completion)\n",
    "        except:\n",
    "            completion = extract_json_objects(completion)\n",
    "\n",
    "        if \"domain\" in completion.keys():\n",
    "            domain = completion[\"domain\"]\n",
    "            is_finance = domain == \"finance\"\n",
    "        else:\n",
    "            is_finance = False\n",
    "\n",
    "        return completion, is_finance\n",
    "\n",
    "    def get_finance_kg_results(self, generated_query):\n",
    "        formatted_time_list = []\n",
    "        if 'datetime' in generated_query:\n",
    "            datetime_list = generated_query['datetime'].split(' - ')\n",
    "            for datetime in datetime_list:\n",
    "                formatted_time_list.append(convert_to_standard_format(datetime.strip()))\n",
    "\n",
    "\n",
    "        kg_results = []\n",
    "        res = \"\"\n",
    "        if \"market_identifier\" in generated_query.keys() and generated_query[\"market_identifier\"] is not None:\n",
    "            if isinstance(generated_query[\"market_identifier\"], str):\n",
    "                company_names = generated_query[\"market_identifier\"].split(\",\")\n",
    "            else:\n",
    "                company_names = generated_query[\"market_identifier\"]\n",
    "\n",
    "            for company_name in company_names:\n",
    "                try:\n",
    "                    res = api.finance_get_company_name(company_name)[\"result\"]\n",
    "\n",
    "                    if res == []:\n",
    "                        ticker_name = company_name.upper()\n",
    "                    else:\n",
    "                        ticker_name = api.finance_get_ticker_by_name(res[0])[\"result\"]\n",
    "\n",
    "                    if generated_query['metric'].lower().strip() == 'price':\n",
    "                        response = api.finance_get_price_history(ticker_name)['result']\n",
    "                    elif generated_query['metric'].lower().strip() == 'dividend':\n",
    "                        response = api.finance_get_dividends_history(ticker_name)['result']\n",
    "                    elif generated_query['metric'].lower().strip() == 'p/e ratio':\n",
    "                        response = api.finance_get_pe_ratio(ticker_name)['result']\n",
    "                    elif generated_query['metric'].lower().strip() == 'eps':\n",
    "                        response = api.finance_get_eps(ticker_name)[\"result\"]\n",
    "                    elif generated_query['metric'].lower().strip() == 'marketcap' :\n",
    "                        response = api.finance_get_market_capitalization(ticker_name)['result']\n",
    "                    else:\n",
    "                        response = api.finance_get_info(ticker_name)['result']\n",
    "                        metric_value = get_metric_from_response(response, generated_query['metric'])\n",
    "                        if metric_value is not None:\n",
    "                            response = metric_value\n",
    "\n",
    "                    try:\n",
    "                        for formatted_time in formatted_time_list:\n",
    "                            if formatted_time in response:\n",
    "                                filtered_response = copy.deepcopy(response[formatted_time])\n",
    "                            elif add_one_day(formatted_time) in response:\n",
    "                                filtered_response = copy.deepcopy(response[add_one_day(formatted_time)])\n",
    "                            elif subtract_one_day(formatted_time) in response:\n",
    "                                filtered_response = copy.deepcopy(response[subtract_one_day(formatted_time)])\n",
    "                            else:\n",
    "                                filtered_response = copy.deepcopy(response)\n",
    "                            kg_results.append({company_name + \" \" + generated_query[\"metric\"]: filtered_response, 'time': formatted_time})\n",
    "                    except:\n",
    "                        kg_results.append({company_name + \" \" + generated_query[\"metric\"]: response})\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Fail to parse the generated query\")\n",
    "                    pass\n",
    "\n",
    "        kg_results = \"<DOC>\\n\".join([str(res) for res in kg_results]) if len(kg_results) > 0 else \"\"\n",
    "        return  kg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6272a8e-2cf6-4958-95f6-90dc87621058",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "### 다 볼 필요는 없고 위에 하나 정도만 대충 어떻게 되어 있는지 봐두면 될 듯. 패턴은 동일 함. ###\n",
    "class CRAG(object):\n",
    "    def __init__(self, server = None):\n",
    "        self.server = os.environ.get('CRAG_SERVER', \"http://10.2.0.165:8000\")\n",
    "\n",
    "    def finance_get_company_name(self, query:str):\n",
    "        url = self.server + '/finance/get_company_name'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': query}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_ticker_by_name(self, query:str):\n",
    "        url = self.server + '/finance/get_ticker_by_name'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': query}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_price_history(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_price_history'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_detailed_price_history(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_detailed_price_history'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_dividends_history(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_dividends_history'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_market_capitalization(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_market_capitalization'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_eps(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_eps'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_pe_ratio(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_pe_ratio'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "    def finance_get_info(self, ticker_name:str):\n",
    "        url = self.server + '/finance/get_info'\n",
    "        headers={'accept': \"application/json\"}\n",
    "        data = {'query': ticker_name}\n",
    "        result = requests.post(url, json=data, headers=headers)\n",
    "        return json.loads(result.text)\n",
    "\n",
    "api = CRAG()\n",
    "\n",
    "metric = \"eps\"\n",
    "\n",
    "result = api.finance_get_company_name(\"microsoft\")\n",
    "pretty_json_print(result)\n",
    "ticker_name = api.finance_get_ticker_by_name(result[\"result\"][0])\n",
    "pretty_json_print(ticker_name)\n",
    "\n",
    "if metric == 'price':\n",
    "    response = api.finance_get_price_history(ticker_name['result'])\n",
    "elif metric == 'dividend':\n",
    "    response = api.finance_get_dividends_history(ticker_name['result'])\n",
    "elif metric == 'p/e ratio':\n",
    "    response = api.finance_get_pe_ratio(ticker_name['result'])\n",
    "elif metric == 'eps':\n",
    "    response = api.finance_get_eps(ticker_name['result'])\n",
    "elif metric == 'marketcap' :\n",
    "    response = api.finance_get_market_capitalization(ticker_name['result'])\n",
    "else:\n",
    "    response = api.finance_get_info(ticker_name['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6e032-6428-48e4-9802-3b10615f9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query(query):\n",
    "    llm_input = prompt_generator(query)\n",
    "    completion = oai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    messages=\n",
    "    llm_input\n",
    "    ).choices[0].message.content\n",
    "    ##### 여기까지는 generate_response와 똑같다. 뒤에는 굳이 외워야 할까?\n",
    "\n",
    "    try:\n",
    "        completion = json.loads(completion)\n",
    "    except:\n",
    "        completion = extract_json_objects(completion)\n",
    "    \n",
    "    if \"domain\" in completion.keys():\n",
    "        domain = completion[\"domain\"]\n",
    "        is_finance = domain == \"finance\"\n",
    "    else:\n",
    "        is_finance = False\n",
    "\n",
    "    return completion, is_finance\n",
    "\n",
    "# 아래 함수는 아마도? 안 나올 듯.......\n",
    "def extract_json_objects(text, decoder=JSONDecoder()):\n",
    "    \"\"\"Find JSON objects in text, and yield the decoded JSON data\n",
    "    \"\"\"\n",
    "    pos = 0\n",
    "    results = []\n",
    "    while True:\n",
    "        match = text.find(\"{\", pos)\n",
    "        if match == -1:\n",
    "            break\n",
    "        try:\n",
    "            result, index = decoder.raw_decode(text[match:])\n",
    "            results.append(result)\n",
    "            pos = match + index\n",
    "        except ValueError:\n",
    "            pos = match + 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409f5c9-6983-4fff-ac57-36eeabbdbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이 knowledge graph가 나올까 싶긴 한데....\n",
    "def get_finance_kg_results(generated_query):\n",
    "    formatted_time_list = []\n",
    "    if 'datetime' in generated_query:\n",
    "        datetime_list = generated_query['datetime'].split(' - ')\n",
    "        for datetime in datetime_list:\n",
    "            formatted_time_list.append(convert_to_standard_format(datetime.strip()))\n",
    "\n",
    "\n",
    "    kg_results = []\n",
    "    res = \"\"\n",
    "    if \"market_identifier\" in generated_query.keys() and generated_query[\"market_identifier\"] is not None:\n",
    "        if isinstance(generated_query[\"market_identifier\"], str):\n",
    "            company_names = generated_query[\"market_identifier\"].split(\",\")\n",
    "        else:\n",
    "            company_names = generated_query[\"market_identifier\"]\n",
    "\n",
    "        for company_name in company_names:\n",
    "            try:\n",
    "                res = api.finance_get_company_name(company_name)[\"result\"]\n",
    "\n",
    "                if res == []:\n",
    "                    ticker_name = company_name.upper()\n",
    "                else:\n",
    "                    ticker_name = api.finance_get_ticker_by_name(res[0])[\"result\"]\n",
    "\n",
    "                if generated_query['metric'].lower().strip() == 'price':\n",
    "                    response = api.finance_get_price_history(ticker_name)['result']\n",
    "                elif generated_query['metric'].lower().strip() == 'dividend':\n",
    "                    response = api.finance_get_dividends_history(ticker_name)['result']\n",
    "                elif generated_query['metric'].lower().strip() == 'p/e ratio':\n",
    "                    response = api.finance_get_pe_ratio(ticker_name)['result']\n",
    "                elif generated_query['metric'].lower().strip() == 'eps':\n",
    "                    response = api.finance_get_eps(ticker_name)[\"result\"]\n",
    "                elif generated_query['metric'].lower().strip() == 'marketcap' :\n",
    "                    response = api.finance_get_market_capitalization(ticker_name)['result']\n",
    "                else:\n",
    "                    response = api.finance_get_info(ticker_name)['result']\n",
    "                    metric_value = get_metric_from_response(response, generated_query['metric'])\n",
    "                    if metric_value is not None:\n",
    "                        response = metric_value\n",
    "\n",
    "                try:\n",
    "                    for formatted_time in formatted_time_list:\n",
    "                        if formatted_time in response:\n",
    "                            filtered_response = copy.deepcopy(response[formatted_time])\n",
    "                        elif add_one_day(formatted_time) in response:\n",
    "                            filtered_response = copy.deepcopy(response[add_one_day(formatted_time)])\n",
    "                        elif subtract_one_day(formatted_time) in response:\n",
    "                            filtered_response = copy.deepcopy(response[subtract_one_day(formatted_time)])\n",
    "                        else:\n",
    "                            filtered_response = copy.deepcopy(response)\n",
    "                        kg_results.append({company_name + \" \" + generated_query[\"metric\"]: filtered_response, 'time': formatted_time})\n",
    "                except:\n",
    "                    kg_results.append({company_name + \" \" + generated_query[\"metric\"]: response})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Fail to parse the generated query\")\n",
    "                pass\n",
    "\n",
    "    kg_results = \"<DOC>\\n\".join([str(res) for res in kg_results]) if len(kg_results) > 0 else \"\"\n",
    "    return  kg_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
