{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7b911-ec0e-44ad-89a8-7f5242804d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Building a Prototype RAG.\n",
    "    1. Data Load\n",
    "    2. Check Query Engine\n",
    "    3. Connect Retriever and Generator\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403601c-8116-4bec-92e3-27f503c8a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"   1. Data Load \"\"\"\n",
    "city_name_path = r'city_short.txt' #change this path\n",
    "\n",
    "city_names = []\n",
    "\n",
    "with open(city_name_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        city = line.split(':')[0][:-1]\n",
    "        city_names.append(city)\n",
    "\n",
    "print(city_names) #100 different cities\n",
    "\n",
    "if city_related_to_question not in city_names:\n",
    "    city_names.append(city_related_to_question)\n",
    "\n",
    "reader = WikipediaReader()\n",
    "documents = reader.load_data(city_names, auto_suggest=False)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 아래처럼 chunk size와 overlap에 variation을 줄 수 있음. \n",
    "text_splitter_short = SentenceSplitter(chunk_size=200, chunk_overlap=50)\n",
    "index_short = VectorStoreIndex.from_documents(documents=documents, transformations=[text_splitter_short])\n",
    "\n",
    "text_splitter_long = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "index_long = VectorStoreIndex.from_documents(documents=documents, transformations=[text_splitter_long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f494e-c196-4674-9e35-0937cae5810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 2. Check Query Engine \"\"\"\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What's the arts and culture scene in Berlin?\")\n",
    "\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162f165-b096-402f-8dd2-b27e5b7633d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 3. Connect Retriever and Generator \"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "class RAG_from_scratch:\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        # RAG에서 \"검색(Retrieval)\" 단계에 해당\n",
    "        results = query_engine.query(query)\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, query: str, context_str: list) -> str:\n",
    "        # RAG에서 \"생성(Generation)\" 단계에 해당\n",
    "        completion = oai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0,\n",
    "            messages=\n",
    "            [\n",
    "                {\"role\": \"user\",\n",
    "                \"content\":\n",
    "                f\"We have provided context information below. \\n\"\n",
    "                f\"---------------------\\n\"\n",
    "                f\"{context_str}\"\n",
    "                f\"\\n---------------------\\n\"\n",
    "                f\"Given this information, please answer the question: {query}\"\n",
    "                }\n",
    "            ]\n",
    "        ).choices[0].message.content \n",
    "            # API 호출 결과에서 첫 번째 응답(choices[0])을 가져온다.\n",
    "            # message.content는 모델이 생성한 실제 텍스트 답변        \n",
    "        return completion\n",
    "\n",
    "    def query(self, query: str) -> str: # 전체 파이프라인을 실행하는 메서드.\n",
    "        context_str = self.retrieve(query) # retrieve를 호출해 관련 문맥을 가져오고\n",
    "        completion = self.generate_response(query, context_str) # 최종 답변을 생성시킴\n",
    "        return completion\n",
    "\n",
    "rag = RAG_from_scratch()\n",
    "\n",
    "city_question = 'Which Korean city has a relationship with Dresden?'\n",
    "\n",
    "answer = rag.query(city_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8c357-1436-4c23-8232-1031f2cbe0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever()\n",
    "synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "class Refine_RAG:\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        ret = retriever.retrieve(query)\n",
    "        results = query_engine.query(query)\n",
    "        return ret, results\n",
    "\n",
    "    def generate_response(self, query: str, context_str: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a helpful assistant. Answer as concisely as possible.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\":\n",
    "                    f\"\"\"\n",
    "                        .... \n",
    "                    \"\"\"\n",
    "                # f에도 \"\"\" 이게 먹히는듯?\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = oai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0,\n",
    "            messages=messages,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def query(self, query: str) -> str:\n",
    "        ret, context_str = self.retrieve(query)\n",
    "        # 결국 ret 자체는 여기서 쓰이진 않음.\n",
    "        completion = self.generate_response(query, context_str)\n",
    "        return completion\n",
    "\n",
    "refine_rag = Refine_RAG()\n",
    "\n",
    "sample_question = \"City council of Suwon addressed illegal dumping of household waste in what way?\"\n",
    "\n",
    "answer = refine_rag.query(sample_question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
